"""Standalone Modal worker function for helion autotuner.

This module is designed to be importable on Modal workers. Modal imports
it on the remote side by qualified name, so it must be part of the helion
package (which is mounted into the container image).

Only torch and triton are needed at runtime â€” all imports are deferred
to function call time.
"""

from __future__ import annotations

# Cache deserialized args on the container so we don't re-deserialize
# 67MB+ on every call when the same container handles multiple configs.
_args_cache: dict[str, list[object]] = {}


def benchmark_config(
    triton_code: str,
    fn_name: str,
    args_dict_key: str,
) -> dict[str, object]:
    """Run a single config benchmark on a Modal GPU worker.

    Args:
        triton_code: Complete triton source code generated by to_triton_code().
        fn_name: The name of the compiled function to call in the generated code.
        args_dict_key: Key to fetch serialized args from the shared Modal Dict.

    Returns:
        Dict with 'perf' (float, ms), 'status' ('ok'|'error'), 'error' (str|None).
    """
    import functools
    import importlib.util
    import io
    import os
    import tempfile

    import torch
    from triton.testing import do_bench

    try:
        # Load args from shared Dict (cached per container)
        if args_dict_key not in _args_cache:
            import modal

            d = modal.Dict.from_name("helion-autotuner-data", create_if_missing=True)
            args_bytes = d[args_dict_key]
            buf = io.BytesIO(args_bytes)
            args = torch.load(buf, map_location="cuda", weights_only=False)
            assert isinstance(args, list)
            _args_cache[args_dict_key] = args
        args = _args_cache[args_dict_key]

        # Write triton code to a temp file and import it as a module.
        # Triton's @jit requires the kernel to be defined in a real .py file,
        # not in exec'd code.
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".py", prefix="_helion_bench_", delete=False
        ) as f:
            f.write(triton_code)
            temp_path = f.name

        try:
            spec = importlib.util.spec_from_file_location(
                "_helion_modal_worker", temp_path
            )
            assert spec is not None and spec.loader is not None
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
        finally:
            os.unlink(temp_path)

        fn = getattr(module, fn_name, None)
        if fn is None:
            return {
                "perf": float("inf"),
                "status": "error",
                "error": f"Function '{fn_name}' not found in generated code",
            }

        # Warmup run
        torch.cuda.synchronize()
        fn(*args)
        torch.cuda.synchronize()

        # Benchmark
        perf = do_bench(
            functools.partial(fn, *args),
            return_mode="median",
            warmup=1,
            rep=50,
        )

        return {"perf": float(perf), "status": "ok", "error": None}

    except Exception as e:
        return {
            "perf": float("inf"),
            "status": "error",
            "error": f"{type(e).__qualname__}: {e}",
        }
