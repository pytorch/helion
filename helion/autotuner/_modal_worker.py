"""Standalone Modal worker function for helion autotuner.

This module is designed to be importable on Modal workers. Modal imports
it on the remote side by qualified name, so it must be part of the helion
package (which is mounted into the container image).

Only torch and triton are needed at runtime â€” all imports are deferred
to function call time.
"""

from __future__ import annotations


def benchmark_config(
    triton_code: str,
    fn_name: str,
    args_bytes: bytes,
) -> dict[str, object]:
    """Run a single config benchmark on a Modal GPU worker.

    Args:
        triton_code: Complete triton source code generated by to_triton_code().
        fn_name: The name of the compiled function to call in the generated code.
        args_bytes: Serialized kernel arguments.

    Returns:
        Dict with 'perf' (float, ms), 'status' ('ok'|'error'), 'error' (str|None).
    """
    import functools
    import importlib.util
    import io
    import os
    import tempfile

    import torch
    from triton.testing import do_bench

    try:
        # Deserialize args onto the GPU
        buf = io.BytesIO(args_bytes)
        args = torch.load(buf, map_location="cuda", weights_only=False)
        assert isinstance(args, list)

        # Write triton code to a temp file and import it as a module.
        # Triton's @jit requires the kernel to be defined in a real .py file,
        # not in exec'd code.
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".py", prefix="_helion_bench_", delete=False
        ) as f:
            f.write(triton_code)
            temp_path = f.name

        try:
            spec = importlib.util.spec_from_file_location(
                "_helion_modal_worker", temp_path
            )
            assert spec is not None and spec.loader is not None
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
        finally:
            os.unlink(temp_path)

        fn = getattr(module, fn_name, None)
        if fn is None:
            return {
                "perf": float("inf"),
                "status": "error",
                "error": f"Function '{fn_name}' not found in generated code",
            }

        # Warmup run
        torch.cuda.synchronize()
        fn(*args)
        torch.cuda.synchronize()

        # Benchmark
        perf = do_bench(
            functools.partial(fn, *args),
            return_mode="median",
            warmup=1,
            rep=50,
        )

        return {"perf": float(perf), "status": "ok", "error": None}

    except Exception as e:
        return {
            "perf": float("inf"),
            "status": "error",
            "error": f"{type(e).__qualname__}: {e}",
        }
