This file is automatically generated by assertExpectedJournal calls in test_examples.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestExamples.test_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_add(x, y, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0_1: tl.constexpr):
    offsets_0_1 = tl.program_id(0) * _BLOCK_SIZE_0_1 + tl.arange(0, _BLOCK_SIZE_0_1).to(tl.int32)
    indices_1 = offsets_0_1 % x_size_1
    indices_0 = offsets_0_1 // x_size_1
    mask_0_1 = offsets_0_1 < x_size_0 * x_size_1
    load = tl.load(x + (indices_0 * x_stride_0 + indices_1 * x_stride_1), mask_0_1, other=0)
    load_1 = tl.load(y + (indices_0 * y_stride_0 + indices_1 * y_stride_1), mask_0_1, other=0)
    v_0 = load_1.to(tl.float32)
    v_1 = load + v_0
    tl.store(out + (indices_0 * out_stride_0 + indices_1 * out_stride_1), v_1, mask_0_1)

def add(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add two tensors element-wise with broadcasting support.

    Args:
        x: First input tensor
        y: Second input tensor

    Returns:
        A new tensor containing the element-wise sum of x and y
    """
    x, y = torch.broadcast_tensors(x, y)
    out = torch.empty(x.shape, dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0_1 = 128
    _launcher(_helion_add, (triton.cdiv(x.size(0) * x.size(1), _BLOCK_SIZE_0_1), 1, 1), x, y, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_attention_block_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 64
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(tl.make_block_ptr(q_view, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(tl.make_block_ptr(k_view, [64, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [1, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.float16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.float16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(tl.make_block_ptr(v_view, [64, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [1, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        v_14 = v_8.to(tl.float16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.float16)
    tl.store(tl.make_block_ptr(out, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (64 * triton.cdiv(1024, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_dynamic)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, q_in_size_1, k_view_stride_0, k_view_stride_1, k_view_stride_2, out_stride_0, out_stride_1, out_stride_2, q_view_stride_0, q_view_stride_1, q_view_stride_2, v_view_stride_0, v_view_stride_1, v_view_stride_2, m_dim, n_dim, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = q_in_size_1
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m_dim
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * q_view_stride_0 + indices_1[None, :, None] * q_view_stride_1 + indices_4[None, None, :] * q_view_stride_2), mask_1[None, :, None], other=0)
    for offset_2 in tl.range(0, n_dim.to(tl.int32), _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_2 < n_dim
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * k_view_stride_0 + indices_4[None, :, None] * k_view_stride_1 + indices_2[None, None, :] * k_view_stride_2), mask_3[None, None, :], other=0)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        _mask_to_2 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), qk, float('-inf'))
        amax = tl.max(_mask_to_2, 2)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        v_6 = libdevice.exp2(v_5)
        _mask_to_3 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), v_6, 0)
        l_ij = tl.sum(_mask_to_3, 2)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * v_view_stride_0 + indices_2[None, :, None] * v_view_stride_1 + indices_4[None, None, :] * v_view_stride_2), mask_3[None, :, None], other=0)
        acc = tl.reshape(tl.dot(tl.reshape(_mask_to_3, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_2
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    tl.store(out + (indices_0[:, None, None] * out_stride_0 + indices_1[None, :, None] * out_stride_1 + indices_4[None, None, :] * out_stride_2), v_12, mask_1[None, :, None])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_helion_attention, (q_in.size(1) * triton.cdiv(m_dim, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, q_in.size(1), k_view.stride(0), k_view.stride(1), k_view.stride(2), out.stride(0), out.stride(1), out.stride(2), q_view.stride(0), q_view.stride(1), q_view.stride(2), v_view.stride(0), v_view.stride(1), v_view.stride(2), m_dim, n_dim, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_persistent_interleaved_l2_grouping)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    total_pids = 32 * tl.cdiv(512, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM):
        num_pid_m = 32
        num_pid_n = tl.cdiv(512, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 4 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 4
        group_size_m = min(num_pid_m - first_pid_m, 4)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
        acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q = tl.load(tl.make_block_ptr(q_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
            q_copy = q
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_copy_0 = q_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k = tl.load(tl.make_block_ptr(k_view, [32, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [1, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
            qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
            amax = tl.max(qk, 2)
            v_0 = tl.full([], 0.18033688, tl.float16)
            v_1 = amax * v_0
            v_2 = v_1.to(tl.float32)
            v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
            v_4 = tl.full([], 0.18033688, tl.float16)
            v_5 = qk * v_4
            subscript = v_3[:, :, None]
            v_6 = v_5.to(tl.float32)
            v_7 = v_6 - subscript
            v_8 = libdevice.exp2(v_7)
            l_ij = tl.sum(v_8, 2)
            v_9 = m_i_copy_0 - v_3
            v_10 = libdevice.exp2(v_9)
            v_11 = l_i_copy_0 * v_10
            l_i = v_11 + l_ij
            subscript_1 = v_10[:, :, None]
            v_13 = acc_copy_0 * subscript_1
            v = tl.load(tl.make_block_ptr(v_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [1, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
            v_14 = v_8.to(tl.float16)
            acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
            m_i = v_3
        subscript_2 = l_i[:, :, None]
        v_15 = acc / subscript_2
        v_16 = v_15.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _NUM_SM = helion.runtime.get_num_sm(q_in.device)
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (_NUM_SM,), q_view, k_view, v_view, out, _NUM_SM, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 32
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 32768 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        v_6 = libdevice.exp2(v_5)
        l_ij = tl.sum(v_6, 2)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 32768 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        acc = tl.reshape(tl.dot(tl.reshape(v_6, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_2
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    tl.store(out + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_12, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 64
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (32 * triton.cdiv(512, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_bmm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_bmm(A, B, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(512, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
    for offset_3 in tl.range(0, 768, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(A + (indices_0[:, None, None] * 393216 + indices_1[None, :, None] * 768 + indices_3[None, None, :] * 1), None)
        load_1 = tl.load(B + (indices_0[:, None, None] * 786432 + indices_3[None, :, None] * 1024 + indices_2[None, None, :] * 1), None)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    v_0 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None, None] * 524288 + indices_1[None, :, None] * 1024 + indices_2[None, None, :] * 1), v_0, None)

def bmm(A: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs batch matrix multiplication.

    Args:
        A: Input tensor of shape [B, M, K]
        B: Input tensor of shape [B, K, N]

    Returns:
        Output tensor of shape [B, M, N] containing the result of batch matrix multiplication
    """
    b, m, k = A.size()
    b, k, n = B.size()
    out = torch.empty([b, m, n], device=A.device, dtype=torch.promote_types(A.dtype, B.dtype))
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_3 = 16
    _launcher(_helion_bmm, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(512, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), A, B, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_concat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, out, y, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < out_size_1
    v_0 = x_size_1.to(tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    x_part = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :] & subscript, other=0)
    v_2 = x_size_1.to(tl.int32)
    v_3 = indices_1 - v_2
    v_4 = x_size_1.to(tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    y_part = tl.load(y + (indices_0[:, None] * y_stride_0 + v_3[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :] & subscript_1, other=0)
    v_6 = x_size_1.to(tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_8, mask_0[:, None] & mask_1[None, :])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    assert x.size(0) == y.size(0)
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_concat2d_dim1, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(out.size(1), _BLOCK_SIZE_1),), x, out, y, out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_concat_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, out, y, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < out_size_1
    v_0 = x_size_1.to(tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    x_part = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :] & subscript, other=0)
    v_2 = x_size_1.to(tl.int32)
    v_3 = indices_1 - v_2
    v_4 = x_size_1.to(tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    y_part = tl.load(y + (indices_0[:, None] * y_stride_0 + v_3[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :] & subscript_1, other=0)
    v_6 = x_size_1.to(tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_8, boundary_check=[0, 1])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    assert x.size(0) == y.size(0)
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_concat2d_dim1, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(out.size(1), _BLOCK_SIZE_1),), x, out, y, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_cross_entropy)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_cross_entropy(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < v
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + indices_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), logits_rows, float('-inf'))
    max_logits = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_4 = logits_rows - max_logits
    v_5 = tl_math.exp(v_4)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_5, 0)
    sum_exp = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_6 = tl_math.log(squeeze_1)
    v_7 = squeeze + v_6
    v_8 = v_7 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_8, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes the cross entropy loss between logits and target labels.

    Implements the cross entropy loss function commonly used in classification tasks.
    The function computes the log softmax of the logits and then calculates the negative
    log likelihood of the true labels.

    Args:
        logits: Input logits tensor of shape [N, V] where N is batch size and V is vocabulary size
        labels: Target labels tensor of shape [N] containing class indices

    Returns:
        A scalar tensor containing the mean cross entropy loss
    """
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _RDIM_SIZE_1 = triton.next_power_of_2(v)
    _launcher(_helion_cross_entropy, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return losses.mean()

--- assertExpectedJournal(TestExamples.test_embedding_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, out_size_0, out_size_1, x_flat_size_0, out_stride_0, out_stride_1, weight_stride_0, weight_stride_1, x_flat_stride_0, embedding_dim, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_flat_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < embedding_dim
    load = tl.load(tl.make_block_ptr(x_flat, [x_flat_size_0], [x_flat_stride_0], [offset_0], [_BLOCK_SIZE_0], [0]), boundary_check=[0], padding_option='zero')
    load_1 = tl.load(weight + (load[:, None] * weight_stride_0 + indices_1[None, :] * weight_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), load_1, boundary_check=[0, 1])

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    x_flat = x.reshape(-1)
    _, embedding_dim = weight.size()
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_embedding, (triton.cdiv(x_flat.size(0), _BLOCK_SIZE_0), triton.cdiv(embedding_dim, _BLOCK_SIZE_1)), x_flat, weight, out, out.size(0), out.size(1), x_flat.size(0), out.stride(0), out.stride(1), weight.stride(0), weight.stride(1), x_flat.stride(0), embedding_dim, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_embedding_pointers)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, x_flat_size_0, out_stride_0, out_stride_1, weight_stride_0, weight_stride_1, x_flat_stride_0, embedding_dim, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = x_flat_size_0
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < embedding_dim
    load = tl.load(x_flat + indices_0 * x_flat_stride_0, None)
    load_1 = tl.load(weight + (load[:, None] * weight_stride_0 + indices_1[None, :] * weight_stride_1), mask_1[None, :], other=0)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), load_1, mask_1[None, :])

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    x_flat = x.reshape(-1)
    _, embedding_dim = weight.size()
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    _BLOCK_SIZE_1 = 256
    _launcher(_helion_embedding, (x_flat.size(0) * triton.cdiv(embedding_dim, _BLOCK_SIZE_1),), x_flat, weight, out, x_flat.size(0), out.stride(0), out.stride(1), weight.stride(0), weight.stride(1), x_flat.stride(0), embedding_dim, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_fp8_attention)
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_attention_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 256, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 256, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 16384 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            qk = tl.dot(q_tile_copy_0, k_tile_t, acc=None, input_precision='tf32', out_dtype=tl.float32)
            v_0 = 0.18033688
            v_1 = qk * v_0
            qk_max = tl.max(v_1, 1)
            v_2 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_2[:, None]
            v_3 = v_1 - subscript
            v_4 = libdevice.exp2(v_3)
            l_ij = tl.sum(v_4, 1)
            v_5 = m_i_copy_0 - v_2
            v_6 = libdevice.exp2(v_5)
            v_7 = l_i_copy_0 * v_6
            l_i = v_7 + l_ij
            subscript_1 = v_6[:, None]
            v_9 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 16384 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_10 = v_4.to(tl.float8e4nv)
            v_t = tl.permute(v_tile, [1, 0])
            acc = tl.dot(v_10, v_t, acc=v_9, input_precision='tf32', out_dtype=tl.float32)
            m_i = v_2
        subscript_2 = l_i[:, None]
        v_11 = acc / subscript_2
        v_12 = v_11.to(tl.float8e4nv)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_12, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention using FP8 precision.
    Implements the attention with FP8 tensors for improved performance and memory efficiency.
    Args:
        q: Query tensor of shape [batch*heads, seq, dim] in FP8 format
        k: Key tensor of shape [batch*heads, seq, dim] in FP8 format
        v: Value tensor of shape [batch*heads, dim, seq] (pre-transposed) in FP8 format
        batch: Number of batches
        heads: Number of attention heads
    Returns:
        Output tensor of shape [batch, heads, seq_len, head_dim] in FP8 format
    """
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e4m3fn, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_fp8_attention_kernel, (8,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_fp8_gemm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_gemm(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        acc = tl.dot(x_tile, y_tile, acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    v_0 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_0, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    FP8 General Matrix Multiplication (GEMM).
    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.
    Args:
        x (torch.Tensor): Input tensor of shape [m, k] in FP8 format.
        y (torch.Tensor): Input tensor of shape [k, n] in FP8 format.
    Returns:
        torch.Tensor: Output tensor of shape [m, n] in FP16 format.
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_helion_fp8_gemm, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_dense_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_dense_add_2d(x_offsets, x_data, y, out, y_size_1, out_stride_0, out_stride_1, x_data_stride_0, x_offsets_stride_0, y_stride_0, y_stride_1, num_rows, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    for offset_1 in tl.range(0, max_nnz.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_nnz
        starts_copy = starts
        v_2_copy = v_2
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = starts_copy_0[:, None]
        subscript_1 = indices_1[None, :]
        v_3 = subscript_1.to(tl.int64)
        v_4 = subscript + v_3
        subscript_2 = indices_1[None, :]
        subscript_3 = v_2_copy_0[:, None]
        v_5 = subscript_2.to(tl.int64)
        v_6 = v_5 < subscript_3
        x_slice = tl.load(x_data + v_4 * x_data_stride_0, mask_0[:, None] & mask_1[None, :] & v_6, other=0)
        load_1 = tl.load(y + (indices_0[:, None] * y_stride_0 + indices_1[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_7 = load_1 + x_slice
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_7, mask_0[:, None] & mask_1[None, :])
    for offset_2 in tl.range(max_nnz.to(tl.int32), y_size_1.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < y_size_1
        load = tl.load(y + (indices_0[:, None] * y_stride_0 + indices_2[None, :] * y_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), load, mask_0[:, None] & mask_2[None, :])

def jagged_dense_add_2d(x_data: torch.Tensor, x_offsets: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add a jagged-prefix sparse tensor (x_data, x_offsets) to a dense matrix y
    and return the dense result.

    Args:
        x_data: 1-D tensor holding all non-zero elements row-by-row
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1]] (length K_i)
        y: (num_rows, N) tensor, N >= max(K_i)

    Returns:
        Dense tensor of shape (num_rows, N) containing the sum of the jagged and dense tensors
    """
    num_rows = y.size(0)
    assert x_offsets.size(0) == num_rows + 1
    out = torch.zeros_like(y)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_jagged_dense_add_2d, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_data, y, out, y.size(1), out.stride(0), out.stride(1), x_data.stride(0), x_offsets.stride(0), y.stride(0), y.stride(1), num_rows, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_mean_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args:
        x_data: 2-D tensor of shape (total_elements, max_M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]
        x_feature_counts: (num_rows) tensor. Number of valid features for each row
        max_M_tensor: Dummy tensor whose numel() gives max number of features

    Returns:
        2-D tensor of shape (num_rows, max_M) containing the mean of each row.
        Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_jagged_mean_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_layernorm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(bias, x, weight, out, bias_size_0, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, m, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < bias_size_0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = load.to(tl.float32)
    var_mean_extra = tl.reshape(tl.sum(v_0, 1), [_BLOCK_SIZE_0, 1])
    v_1 = var_mean_extra / bias_size_0.to(tl.float32)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_0[:, None], [_BLOCK_SIZE_0, 1]), v_1, 0)
    v_2 = v_0 - _mask_to_1
    v_3 = v_2 * v_2
    var_mean_extra_2 = tl.reshape(tl.sum(v_3, 1), [_BLOCK_SIZE_0, 1])
    v_4 = var_mean_extra_2 / bias_size_0.to(tl.float32)
    v_5 = v_0 - v_1
    v_6 = v_4 + eps
    v_7 = libdevice.rsqrt(v_6)
    v_8 = v_5 * v_7
    load_1 = tl.load(weight + indices_1 * weight_stride_0, mask_1, other=0)
    v_9 = load_1.to(tl.float32)
    v_10 = v_9[None, :]
    v_11 = v_8 * v_10
    load_2 = tl.load(bias + indices_1 * bias_stride_0, mask_1, other=0)
    v_12 = load_2.to(tl.float32)
    v_13 = v_12[None, :]
    v_14 = v_11 + v_13
    v_15 = v_14.to(tl.float16)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_15, mask_0[:, None] & mask_1[None, :])

def layer_norm_fwd(x: torch.Tensor, nomralized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        nomralized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor): Learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        torch.Tensor: The layer-normalized output tensor of shape [batch_size, dim], in FP16.
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {m}'
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {m}'
    assert len(nomralized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    assert nomralized_shape[0] == n, f'normalized shape mismatch {nomralized_shape[0]} != {n}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(bias.size(0))
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(m, _BLOCK_SIZE_0),), bias, x, weight, out, bias.size(0), bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), m, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(128, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(128, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 128, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 128 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 128 + indices_1[None, :] * 1), None)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    tl.store(out + (indices_0[:, None] * 128 + indices_1[None, :] * 1), acc, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(128, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_dynamic_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(bias, x, y, weight, out, bias_size_0, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, y_stride_0, y_stride_1, m, k, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < bias_size_0
    acc = tl.full([_BLOCK_SIZE_1, _RDIM_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, k.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < k
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_1[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
        load_1 = tl.load(y + (indices_2[:, None] * y_stride_0 + indices_0[None, :] * y_stride_1), mask_2[:, None] & mask_0[None, :], other=0)
        mm = tl.dot(load, load_1, input_precision='tf32')
        acc = acc_copy_0 + mm
    _mask_to = tl.where(mask_1[:, None] & mask_0[None, :], acc, 0)
    var_mean_extra = tl.reshape(tl.sum(_mask_to, 1), [_BLOCK_SIZE_1, 1])
    v_1 = var_mean_extra / bias_size_0.to(tl.float32)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[:, None], [_BLOCK_SIZE_1, 1]), v_1, 0)
    v_2 = _mask_to - _mask_to_1
    v_3 = v_2 * v_2
    var_mean_extra_2 = tl.reshape(tl.sum(v_3, 1), [_BLOCK_SIZE_1, 1])
    v_4 = var_mean_extra_2 / bias_size_0.to(tl.float32)
    v_5 = acc - v_1
    v_6 = 1e-05
    v_7 = v_4 + v_6
    v_8 = libdevice.rsqrt(v_7)
    v_9 = v_5 * v_8
    load_2 = tl.load(weight + indices_0 * weight_stride_0, mask_0, other=0)
    v_10 = load_2[None, :]
    v_11 = v_9 * v_10
    load_3 = tl.load(bias + indices_0 * bias_stride_0, mask_0, other=0)
    v_12 = load_3[None, :]
    v_13 = v_11 + v_12
    tl.store(out + (indices_1[:, None] * out_stride_0 + indices_0[None, :] * out_stride_1), v_13, mask_1[:, None] & mask_0[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    m, k = x.size()
    k2 = y.size(0)
    n = y.size(1)
    assert k == k2, f'size mismatch {k} != {k2}'
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_0 = triton.next_power_of_2(bias.size(0))
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul_layernorm, (triton.cdiv(m, _BLOCK_SIZE_1),), bias, x, y, weight, out, bias.size(0), bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), y.stride(0), y.stride(1), m, k, _BLOCK_SIZE_1, _RDIM_SIZE_0, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_static_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(x, y, weight, bias, out, out_stride_0, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < 400
    acc = tl.full([_BLOCK_SIZE_1, _RDIM_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_1[:, None] * 256 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 400 + indices_0[None, :] * 1), mask_0[None, :], other=0)
        mm = tl.dot(load, load_1, input_precision='tf32')
        acc = acc_copy_0 + mm
    _mask_to = tl.where(tl.broadcast_to(mask_0[None, :], [_BLOCK_SIZE_1, _RDIM_SIZE_0]), acc, 0)
    var_mean_extra = tl.reshape(tl.sum(_mask_to, 1), [_BLOCK_SIZE_1, 1])
    v_1 = 400
    v_2 = var_mean_extra / v_1.to(tl.float32)
    v_3 = _mask_to - v_2
    v_4 = v_3 * v_3
    var_mean_extra_2 = tl.reshape(tl.sum(v_4, 1), [_BLOCK_SIZE_1, 1])
    v_5 = 400
    v_6 = var_mean_extra_2 / v_5.to(tl.float32)
    v_7 = acc - v_2
    v_8 = 1e-05
    v_9 = v_6 + v_8
    v_10 = libdevice.rsqrt(v_9)
    v_11 = v_7 * v_10
    load_2 = tl.load(weight + indices_0 * 1, mask_0, other=0)
    v_12 = load_2[None, :]
    v_13 = v_11 * v_12
    load_3 = tl.load(bias + indices_0 * 1, mask_0, other=0)
    v_14 = load_3[None, :]
    v_15 = v_13 + v_14
    tl.store(out + (indices_1[:, None] * out_stride_0 + indices_0[None, :] * 1), v_15, mask_0[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    m, k = x.size()
    k2 = y.size(0)
    n = y.size(1)
    assert k == k2, f'size mismatch {k} != {k2}'
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_0 = 512
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul_layernorm, (triton.cdiv(128, _BLOCK_SIZE_1),), x, y, weight, bias, out, out.stride(0), _BLOCK_SIZE_1, _RDIM_SIZE_0, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_split_k)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import helion._testing.matmul_split_k as _source_module

@triton.jit
def _helion_matmul_split_k(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(64, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1024)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 64 + indices_1[None, :] * 1), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, tuple[torch.Tensor, ...]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Matrix multiplication kernel using split-K parallelism.
    This kernel splits the reduction (K) dimension into multiple fragments to improve
    parallelism and performance, especially for large K. The results from each split
    are accumulated atomically into the output tensor. An optional epilogue function
    can be applied to the accumulator, e.g., for adding bias.
    Args:
        x (torch.Tensor): Left input matrix of shape [m, k].
        y (torch.Tensor): Right input matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        torch.Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 8
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_3 = 32
    _launcher(_helion_matmul_split_k, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_moe_matmul_ogs)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_moe_matmul_ogs(expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A_stride_0, A_stride_1, C_stride_0, C_stride_1, W_stride_0, W_stride_1, W_stride_2, expert_token_counts_stride_0, expert_token_offsets_stride_0, sorted_to_orig_token_idx_stride_0, max_T_per_expert, N, K, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    start = tl.load(expert_token_offsets + offset_0 * expert_token_offsets_stride_0, None)
    num_tokens = tl.load(expert_token_counts + offset_0 * expert_token_counts_stride_0, None)
    v_0 = tl.full([], 0, tl.int32)
    v_1 = num_tokens != v_0
    if v_1:
        num_tokens_copy = num_tokens
        start_copy = start
        num_tokens_copy_0 = num_tokens_copy
        start_copy_0 = start_copy
        for offset_1 in tl.range(0, max_T_per_expert.to(tl.int32), _BLOCK_SIZE_1):
            indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
            mask_1 = indices_1 < max_T_per_expert
            for offset_2 in tl.range(0, N.to(tl.int32), _BLOCK_SIZE_2):
                indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
                mask_2 = indices_2 < N
                num_tokens_copy_0_copy = num_tokens_copy_0
                start_copy_0_copy = start_copy_0
                num_tokens_copy_0_copy_0 = num_tokens_copy_0_copy
                start_copy_0_copy_0 = start_copy_0_copy
                v_2 = num_tokens_copy_0_copy_0[None]
                v_3 = indices_1 < v_2
                v_4 = tl.full([], 0, tl.int32)
                v_5 = v_4[None]
                v_6 = tl.where(v_3, indices_1, v_5)
                v_7 = start_copy_0_copy_0[None]
                v_8 = v_7 + v_6
                squeeze = tl.reshape(v_8, [_BLOCK_SIZE_1])
                expert_orig_token_indices = tl.load(sorted_to_orig_token_idx + squeeze * sorted_to_orig_token_idx_stride_0, mask_1, other=0)
                acc = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
                for offset_3 in tl.range(0, K.to(tl.int32), _BLOCK_SIZE_3):
                    indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
                    mask_3 = indices_3 < K
                    expert_orig_token_indices_copy = expert_orig_token_indices
                    acc_copy = acc
                    expert_orig_token_indices_copy_0 = expert_orig_token_indices_copy
                    acc_copy_0 = acc_copy
                    A_frag = tl.load(A + (expert_orig_token_indices_copy_0[:, None] * A_stride_0 + indices_3[None, :] * A_stride_1), mask_1[:, None] & mask_3[None, :], other=0)
                    W_frag = tl.load(W + (offset_0 * W_stride_0 + indices_3[:, None] * W_stride_1 + indices_2[None, :] * W_stride_2), mask_3[:, None] & mask_2[None, :], other=0)
                    acc = tl.dot(A_frag, W_frag, acc=acc_copy_0, input_precision='tf32')
                existing_values = tl.load(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
                view = tl.reshape(v_3, [_BLOCK_SIZE_1, 1])
                mask_2d = tl.broadcast_to(view, [_BLOCK_SIZE_1, _BLOCK_SIZE_2])
                v_9 = acc.to(tl.float16)
                v_10 = tl.where(mask_2d, v_9, existing_values)
                tl.store(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), v_10, mask_1[:, None] & mask_2[None, :])

def moe_matmul_ogs(A: torch.Tensor, W: torch.Tensor, expert_token_counts: torch.Tensor, expert_token_offsets: torch.Tensor, sorted_to_orig_token_idx: torch.Tensor, max_T_per_expert_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Helion kernel implementing MoE matmul with Outer-Gather-Scatter.
    Args:
        A (torch.Tensor): Input activations of shape [T, K].
        W (torch.Tensor): Expert weights of shape [E, K, N].
        expert_token_counts (torch.Tensor): Number of tokens per expert [E].
        expert_token_offsets (torch.Tensor): Starting offsets of tokens per expert [E+1].
        sorted_to_orig_token_idx (torch.Tensor): Maps sorted token indices to original token indices [T].
        max_T_per_expert_tensor (torch.Tensor): Dummy tensor to indicate max tokens per expert.
    Returns:
        torch.Tensor: Output activations of shape [T, N].
    """
    T, K = A.shape
    E, _, N = W.shape
    max_T_per_expert = max_T_per_expert_tensor.numel()
    C = torch.zeros(T, N, dtype=torch.promote_types(A.dtype, W.dtype), device=A.device)
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_3 = 16
    _launcher(_helion_moe_matmul_ogs, (E,), expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A.stride(0), A.stride(1), C.stride(0), C.stride(1), W.stride(0), W.stride(1), W.stride(2), expert_token_counts.stride(0), expert_token_offsets.stride(0), sorted_to_orig_token_idx.stride(0), max_T_per_expert, N, K, _BLOCK_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return C

--- assertExpectedJournal(TestExamples.test_rms_norm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm(x, weight, out, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    load = tl.load(x + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    v_0 = load.to(tl.float32)
    v_1 = v_0 * v_0
    mean_x_squared_extra = tl.reshape(tl.sum(v_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = 256
    v_3 = mean_x_squared_extra / v_2.to(tl.float32)
    v_4 = v_3 + eps
    v_5 = libdevice.rsqrt(v_4)
    v_6 = v_0 * v_5
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_7 = load_1.to(tl.float32)
    v_8 = v_7[None, :]
    v_9 = v_6 * v_8
    v_10 = v_9.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_10, None)

def rms_norm(x: torch.Tensor, weight: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs Root Mean Square (RMS) normalization on the input tensor.

    RMS normalization normalizes by the root mean square of the elements:
    output = x / sqrt(mean(x^2) + eps) * weight

    Args:
        x: Input tensor of shape [M, N]
        weight: Scale parameter of shape [N]
        eps: Small constant for numerical stability

    Returns:
        Output tensor of shape [M, N] with RMS normalization applied
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = 256
    _launcher(_helion_rms_norm, (triton.cdiv(128, _BLOCK_SIZE_0),), x, weight, out, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_segment_reduction)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

import helion._testing.segment_reduction as _source_module

@triton.jit
def combine_fn_helion_0(param_0, param_1, param_2, param_3):
    v_0 = param_1 == param_3
    v_1 = param_0 + param_2
    v_2 = tl.where(v_0, v_1, param_2)
    return (v_2, param_3)

@triton.jit
def _helion_segmented_reduction_helion(input_data, indices, output, indices_stride_0, input_data_stride_0, input_data_stride_1, output_stride_0, output_stride_1, num_elements, num_features, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(num_elements, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_elements
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < num_features
    vals = tl.load(input_data + (indices_0[:, None] * input_data_stride_0 + indices_1[None, :] * input_data_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    idxs = tl.load(indices + indices_0 * indices_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    sub = -1 + num_elements
    v_2 = sub.to(tl.int32)
    v_3 = indices_0 < v_2
    idxs_next = tl.load(indices + v_1 * indices_stride_0, mask_0 & v_3, other=0)
    v_4 = idxs.to(tl.float32)
    unsqueeze = v_4[:, None]
    expand = tl.broadcast_to(unsqueeze, [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    out_vals = tl.associative_scan((vals, expand), 0, combine_fn_helion_0)[0]
    v_5 = idxs != idxs_next
    _BLOCK_SIZE_0_ = _BLOCK_SIZE_0
    v_6 = _BLOCK_SIZE_0_.to(tl.int32)
    v_7 = indices_0 % v_6
    v_8 = tl.full([], 0, tl.int32)
    v_9 = v_7 != v_8
    v_10 = libdevice.signbit(v_7) != 0 if v_7.dtype is tl.float32 else v_7 < 0
    v_11 = libdevice.signbit(v_6) != 0 if v_6.dtype is tl.float32 else v_6 < 0
    v_12 = v_10 != v_11
    v_13 = v_9 & v_12
    v_14 = v_7 + v_6
    v_15 = tl.where(v_13, v_14, v_7)
    sub_1 = -1 + _BLOCK_SIZE_0
    v_16 = sub_1.to(tl.int32)
    v_17 = v_15 == v_16
    v_18 = v_5 | v_17
    unsqueeze_1 = v_18[:, None]
    v_19 = 0.0
    v_20 = v_19[None, None]
    v_21 = tl.where(unsqueeze_1, out_vals, v_20)
    tl.atomic_add(output + (idxs[:, None] * output_stride_0 + indices_1[None, :] * output_stride_1), v_21, mask=mask_0[:, None] & mask_1[None, :], sem='relaxed')

def segmented_reduction_helion(indices: torch.Tensor, input_data: torch.Tensor, num_nodes: int, *, _launcher=_default_launcher):
    """
    Performs segmented reduction using Helion.

    Reduces input data by summing values with the same index.

    Args:
        indices: Tensor of segment indices for each element
        input_data: Input tensor of shape [num_elements, num_features]
        num_nodes: Number of output nodes/segments

    Returns:
        Output tensor of shape [num_nodes, num_features] with reduced values
    """
    num_elements, num_features = input_data.shape
    output = torch.zeros((num_nodes, num_features), dtype=input_data.dtype, device=input_data.device)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_segmented_reduction_helion, (triton.cdiv(num_elements, _BLOCK_SIZE_0) * triton.cdiv(num_features, _BLOCK_SIZE_1),), input_data, indices, output, indices.stride(0), input_data.stride(0), input_data.stride(1), output.stride(0), output.stride(1), num_elements, num_features, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return output

--- assertExpectedJournal(TestExamples.test_softmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_decomposed)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_decomposed(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    values = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), values, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = values - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_exp = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_exp
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax_decomposed(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Helion kernel implementing softmax by decomposing into max, exp, and normalization steps.
    This avoids using PyTorch's built-in softmax decomposition.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax_decomposed, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), v_6, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 32
    _launcher(_helion_softmax, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, m, n, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < n
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        values = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], values, float('-inf'))
        local_amax = tl.max(_mask_to, 1)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        v_1 = mi_copy_0 - v_0
        v_2 = tl_math.exp(v_1)
        v_3 = di_copy_0 * v_2
        subscript = v_0[:, None]
        v_4 = values - subscript
        v_5 = tl_math.exp(v_4)
        _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_5, 0)
        sum_1 = tl.sum(_mask_to_1, 1)
        di = v_3 + sum_1
        mi = v_0
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_2 = indices_2 < n
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        values_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = tl_math.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), v_9, mask_0[:, None] & mask_2[None, :])

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    m, n = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_softmax_two_pass, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), m, n, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, m, n, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < n
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        values = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], values, float('-inf'))
        local_amax = tl.max(_mask_to, 1)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        v_1 = mi_copy_0 - v_0
        v_2 = tl_math.exp(v_1)
        v_3 = di_copy_0 * v_2
        subscript = v_0[:, None]
        v_4 = values - subscript
        v_5 = tl_math.exp(v_4)
        _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_5, 0)
        sum_1 = tl.sum(_mask_to_1, 1)
        di = v_3 + sum_1
        mi = v_0
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        values_1 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = tl_math.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_9, boundary_check=[0, 1])

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    m, n = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_softmax_two_pass, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), m, n, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_sum_kernel(x, out, out_stride_0, x_stride_0, x_stride_1, n, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, n, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < n
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_0 = sum_1_acc + load
        sum_1_acc = v_0
    sum_1 = tl.sum(sum_1_acc, 1)
    tl.store(out + indices_0 * out_stride_0, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Sums a 2D tensor along the last dimension.

    Args:
        x: Input tensor of shape [M, N]

    Returns:
        Output tensor of shape [M] containing the sum of each row
    """
    m, n = x.shape
    out = torch.empty([m], dtype=x.dtype, device=x.device)
    _REDUCTION_BLOCK_1 = 32768
    _launcher(_helion_sum_kernel, (m,), x, out, out.stride(0), x.stride(0), x.stride(1), n, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    load_2 = tl.load(epilogue_closure_0 + indices_1[None, :] * 1, None)
    v_0 = load_2.to(tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = v_3.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_4, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure1)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    load_2 = tl.load(tl.make_block_ptr(epilogue_closure_0, [1, 1024], [1024, 1], [0, offset_1], [1, _BLOCK_SIZE_1], [1, 0]), boundary_check=[1], padding_option='zero')
    v_0 = load_2.to(tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = v_3.to(tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_4, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    v_0 = tl.full([], 0, tl.int32)
    v_1 = triton_helpers.maximum(v_0, acc)
    v_2 = v_1.to(tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out
