This file is automatically generated by assertExpectedJournal calls in test_examples.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestExamples.test_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_add(x, y, out, _BLOCK_SIZE_0_1: tl.constexpr):
    # src[add.py:N]: for tile in hl.tile(out.size()):
    offsets_0_1 = tl.program_id(0) * _BLOCK_SIZE_0_1 + tl.arange(0, _BLOCK_SIZE_0_1).to(tl.int32)
    indices_1 = offsets_0_1 % 512
    indices_0 = offsets_0_1 // 512
    # src[add.py:N]: out[tile] = x[tile] + y[tile]
    load = tl.load(x + (indices_0 * 512 + indices_1 * 1), None)
    load_1 = tl.load(y + (indices_0 * 0 + indices_1 * 1), None)
    v_0 = tl.cast(load_1, tl.float32)
    v_1 = load + v_0
    tl.store(out + (indices_0 * 512 + indices_1 * 1), v_1, None)

def add(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add two tensors element-wise with broadcasting support.

    Args:
        x: First input tensor
        y: Second input tensor

    Returns:
        A new tensor containing the element-wise sum of x and y
    """
    # src[add.py:N]: x, y = torch.broadcast_tensors(x, y)
    x, y = torch.broadcast_tensors(x, y)
    # src[add.py:N]: out = torch.empty(
    # src[add.py:N]:     x.shape,
    # src[add.py:N]:     # match type promotion of torch.add
    # src[add.py:N-N]: ...
    out = torch.empty(x.shape, dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[add.py:N]: for tile in hl.tile(out.size()):
    _BLOCK_SIZE_0_1 = 128
    # src[add.py:N]: for tile in hl.tile(out.size()):
    # src[add.py:N]:     out[tile] = x[tile] + y[tile]
    _launcher(_helion_add, (triton.cdiv(262144, _BLOCK_SIZE_0_1), 1, 1), x, y, out, _BLOCK_SIZE_0_1, num_warps=4, num_stages=1)
    # src[add.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_addmm_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_addmm_bwd(grad_out, grad_input, mat2, grad_mat1, mat1, grad_mat2, beta, alpha, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr, _BLOCK_SIZE_5: tl.constexpr, _BLOCK_SIZE_6: tl.constexpr, _BLOCK_SIZE_7: tl.constexpr):
    # src[matmul.py:N]: for tile_m3, tile_n3 in hl.tile([m, n]):
    # src[matmul.py:N]:     grad_input[tile_m3, tile_n3] = beta * grad_out[tile_m3, tile_n3]
    pid_shared = tl.program_id(0)
    if pid_shared < tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1):
        # src[matmul.py:N]: for tile_m3, tile_n3 in hl.tile([m, n]):
        num_blocks_0 = tl.cdiv(128, _BLOCK_SIZE_0)
        pid_0 = pid_shared % num_blocks_0
        pid_1 = pid_shared // num_blocks_0
        offset_0 = pid_0 * _BLOCK_SIZE_0
        indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
        offset_1 = pid_1 * _BLOCK_SIZE_1
        indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
        # src[matmul.py:N]: grad_input[tile_m3, tile_n3] = beta * grad_out[tile_m3, tile_n3]
        load = tl.load(grad_out + (indices_0[:, None] * 128 + indices_1[None, :] * 1), None)
        v_0 = load * beta
        tl.store(grad_input + (indices_0[:, None] * 128 + indices_1[None, :] * 1), v_0, None)
    elif pid_shared < tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1) + tl.cdiv(128, _BLOCK_SIZE_2) * tl.cdiv(128, _BLOCK_SIZE_3):
        # src[matmul.py:N]: for tile_m1, tile_k1 in hl.tile([m, k]):
        pid_shared -= tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1)
        num_blocks_1 = tl.cdiv(128, _BLOCK_SIZE_2)
        pid_2 = pid_shared % num_blocks_1
        pid_3 = pid_shared // num_blocks_1
        offset_2 = pid_2 * _BLOCK_SIZE_2
        indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
        offset_3 = pid_3 * _BLOCK_SIZE_3
        indices_3 = (offset_3 + tl.arange(0, _BLOCK_SIZE_3)).to(tl.int32)
        # src[matmul.py:N]: acc1 = hl.zeros([tile_m1, tile_k1], dtype=torch.float32)
        acc1 = tl.full([_BLOCK_SIZE_2, _BLOCK_SIZE_3], 0.0, tl.float32)
        # src[matmul.py:N]: for tile_n1 in hl.tile(n):
        # src[matmul.py:N]:     acc1 = torch.addmm(
        # src[matmul.py:N]:         acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
        # src[matmul.py:N-N]: ...
        for offset_4 in tl.range(0, 128, _BLOCK_SIZE_4):
            indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
            acc1_copy = acc1
            acc1_copy_0 = acc1_copy
            # src[matmul.py:N]: acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
            load_1 = tl.load(grad_out + (indices_2[:, None] * 128 + indices_4[None, :] * 1), None)
            load_2 = tl.load(mat2 + (indices_3[:, None] * 128 + indices_4[None, :] * 1), None)
            permute = tl.permute(load_2, [1, 0])
            # src[matmul.py:N]: acc1 = torch.addmm(
            # src[matmul.py:N]:     acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
            # src[matmul.py:N]: )
            acc1 = tl.dot(tl.cast(load_1, tl.float32), tl.cast(permute, tl.float32), acc=acc1_copy_0, input_precision='tf32', out_dtype=tl.float32)
        # src[matmul.py:N]: grad_mat1[tile_m1, tile_k1] = (alpha * acc1).to(mat1.dtype)
        v_1 = acc1 * alpha
        tl.store(grad_mat1 + (indices_2[:, None] * 128 + indices_3[None, :] * 1), v_1, None)
    else:
        # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
        pid_shared -= tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1) + tl.cdiv(128, _BLOCK_SIZE_2) * tl.cdiv(128, _BLOCK_SIZE_3)
        num_blocks_2 = tl.cdiv(128, _BLOCK_SIZE_5)
        pid_4 = pid_shared % num_blocks_2
        pid_5 = pid_shared // num_blocks_2
        offset_5 = pid_4 * _BLOCK_SIZE_5
        indices_5 = (offset_5 + tl.arange(0, _BLOCK_SIZE_5)).to(tl.int32)
        offset_6 = pid_5 * _BLOCK_SIZE_6
        indices_6 = (offset_6 + tl.arange(0, _BLOCK_SIZE_6)).to(tl.int32)
        # src[matmul.py:N]: acc2 = hl.zeros([tile_k2, tile_n2], dtype=torch.float32)
        acc2 = tl.full([_BLOCK_SIZE_5, _BLOCK_SIZE_6], 0.0, tl.float32)
        # src[matmul.py:N]: for tile_m2 in hl.tile(m):
        # src[matmul.py:N]:     acc2 = torch.addmm(
        # src[matmul.py:N]:         acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
        # src[matmul.py:N-N]: ...
        for offset_7 in tl.range(0, 128, _BLOCK_SIZE_7):
            indices_7 = offset_7 + tl.arange(0, _BLOCK_SIZE_7).to(tl.int32)
            acc2_copy = acc2
            acc2_copy_0 = acc2_copy
            # src[matmul.py:N]: acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
            load_3 = tl.load(mat1 + (indices_7[:, None] * 128 + indices_5[None, :] * 1), None)
            permute_1 = tl.permute(load_3, [1, 0])
            load_4 = tl.load(grad_out + (indices_7[:, None] * 128 + indices_6[None, :] * 1), None)
            # src[matmul.py:N]: acc2 = torch.addmm(
            # src[matmul.py:N]:     acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
            # src[matmul.py:N]: )
            acc2 = tl.dot(tl.cast(permute_1, tl.float32), tl.cast(load_4, tl.float32), acc=acc2_copy_0, input_precision='tf32', out_dtype=tl.float32)
        # src[matmul.py:N]: grad_mat2[tile_k2, tile_n2] = (alpha * acc2).to(mat2.dtype)
        v_2 = acc2 * alpha
        tl.store(grad_mat2 + (indices_5[:, None] * 128 + indices_6[None, :] * 1), v_2, None)

def addmm_bwd(grad_out: Tensor, bias: Tensor, mat1: Tensor, mat2: Tensor, alpha: float=1.0, beta: float=1.0, *, _launcher=_default_launcher):
    """
    Backward pass for addmm operation following Triton reference pattern.

    Forward: output = beta * bias + alpha * (mat1 @ mat2)

    Based on the Triton kernel analysis:
    - grad_input = beta * grad_out (with proper reduction for broadcasting)
    - grad_mat1 = alpha * (grad_out @ mat2.T)
    - grad_mat2 = alpha * (mat1.T @ grad_out)

    Args:
        grad_out: Gradient w.r.t output [m, n]
        bias: Bias tensor [m, n] (or broadcastable)
        mat1: First matrix [m, k]
        mat2: Second matrix [k, n]
        alpha: Scalar multiplier for matmul
        beta: Scalar multiplier for bias

    Returns:
        tuple[Tensor, Tensor, Tensor]: (grad_input, grad_mat1, grad_mat2)
    """
    # src[matmul.py:N]: m, n = grad_out.size()
    m, n = grad_out.size()
    # src[matmul.py:N]: m2, k = mat1.size()
    m2, k = mat1.size()
    # src[matmul.py:N]: k2, n2 = mat2.size()
    k2, n2 = mat2.size()
    # src[matmul.py:N]: assert m == m2 and n == n2 and k == k2, "Size mismatch in addmm backward"
    assert m == m2 and n == n2 and (k == k2), 'Size mismatch in addmm backward'
    # src[matmul.py:N]: grad_input = torch.empty_like(bias)
    grad_input = torch.empty_like(bias)
    # src[matmul.py:N]: grad_mat1 = torch.empty_like(mat1)
    grad_mat1 = torch.empty_like(mat1)
    # src[matmul.py:N]: grad_mat2 = torch.empty_like(mat2)
    grad_mat2 = torch.empty_like(mat2)
    # src[matmul.py:N]: for tile_m3, tile_n3 in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[matmul.py:N]: for tile_m1, tile_k1 in hl.tile([m, k]):
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_3 = 16
    # src[matmul.py:N]: for tile_n1 in hl.tile(n):
    # src[matmul.py:N]:     acc1 = torch.addmm(
    # src[matmul.py:N]:         acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
    # src[matmul.py:N-N]: ...
    _BLOCK_SIZE_4 = 16
    # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
    _BLOCK_SIZE_5 = 16
    _BLOCK_SIZE_6 = 16
    # src[matmul.py:N]: for tile_m2 in hl.tile(m):
    # src[matmul.py:N]:     acc2 = torch.addmm(
    # src[matmul.py:N]:         acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
    # src[matmul.py:N-N]: ...
    _BLOCK_SIZE_7 = 16
    # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
    # src[matmul.py:N]:     acc2 = hl.zeros([tile_k2, tile_n2], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_m2 in hl.tile(m):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_addmm_bwd, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(128, _BLOCK_SIZE_1) + triton.cdiv(128, _BLOCK_SIZE_2) * triton.cdiv(128, _BLOCK_SIZE_3) + triton.cdiv(128, _BLOCK_SIZE_5) * triton.cdiv(128, _BLOCK_SIZE_6),), grad_out, grad_input, mat2, grad_mat1, mat1, grad_mat2, beta, alpha, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, _BLOCK_SIZE_4, _BLOCK_SIZE_5, _BLOCK_SIZE_6, _BLOCK_SIZE_7, num_warps=4, num_stages=1)
    # src[matmul.py:N]: return grad_input, grad_mat1, grad_mat2
    return (grad_input, grad_mat1, grad_mat2)

--- assertExpectedJournal(TestExamples.test_attention_block_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
    l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
    # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
    q = tl.load(tl.make_block_ptr(q_view, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
        k = tl.load(tl.make_block_ptr(k_view, [64, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [_BLOCK_SIZE_0, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
        # src[attention.py:N]: qk = torch.bmm(q, k)
        qk = tl.cast(tl.dot(tl.cast(q_copy_0, tl.float16), tl.cast(k, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
        # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        amax = tl.cast(tl.max(qk, 2), tl.float16)
        v_0 = 0.18033688
        v_1 = tl.cast(amax * v_0, tl.float16)
        v_2 = tl.cast(v_1, tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
        v_4 = 0.18033688
        v_5 = tl.cast(qk * v_4, tl.float16)
        subscript = v_3[:, :, None]
        v_6 = tl.cast(v_5, tl.float32)
        v_7 = v_6 - subscript
        # src[attention.py:N]: p = torch.exp2(qk)
        v_8 = libdevice.exp2(v_7)
        # src[attention.py:N]: l_ij = torch.sum(p, -1)
        l_ij = tl.cast(tl.sum(v_8, 2), tl.float32)
        # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        # src[attention.py:N]: l_i = l_i * alpha + l_ij
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        # src[attention.py:N]: acc = acc * alpha[:, :, None]
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
        v = tl.load(tl.make_block_ptr(v_view, [64, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        # src[attention.py:N]: p = p.to(v.dtype)
        v_14 = tl.cast(v_8, tl.float16)
        # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
        acc = tl.dot(tl.cast(v_14, tl.float16), tl.cast(v, tl.float16), acc=v_13, input_precision='tf32', out_dtype=tl.float32)
        # src[attention.py:N]: m_i = m_ij
        m_i = v_3
    # src[attention.py:N]: acc = acc / l_i[:, :, None]
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(tl.make_block_ptr(out, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 32
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_dynamic)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, q_in_size_1, k_view_stride_0, k_view_stride_1, k_view_stride_2, out_stride_0, out_stride_1, out_stride_2, q_view_stride_0, q_view_stride_1, q_view_stride_2, v_view_stride_0, v_view_stride_1, v_view_stride_2, m_dim, n_dim, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    num_blocks_0 = q_in_size_1
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m_dim
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
    l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
    # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
    q = tl.load(q_view + (indices_0[:, None, None] * q_view_stride_0 + indices_1[None, :, None] * q_view_stride_1 + indices_4[None, None, :] * q_view_stride_2), mask_1[None, :, None], other=0)
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    for offset_2 in tl.range(0, n_dim.to(tl.int32), _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_2 < n_dim
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
        k = tl.load(k_view + (indices_0[:, None, None] * k_view_stride_0 + indices_4[None, :, None] * k_view_stride_1 + indices_2[None, None, :] * k_view_stride_2), mask_3[None, None, :], other=0)
        # src[attention.py:N]: qk = torch.bmm(q, k)
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float32), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float32), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        _mask_to_2 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), qk, tl.full([], float('-inf'), tl.float32))
        amax = tl.cast(tl.max(_mask_to_2, 2), tl.float32)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        # src[attention.py:N]: p = torch.exp2(qk)
        v_6 = libdevice.exp2(v_5)
        # src[attention.py:N]: l_ij = torch.sum(p, -1)
        _mask_to_3 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), v_6, tl.full([], 0, tl.float32))
        l_ij = tl.cast(tl.sum(_mask_to_3, 2), tl.float32)
        # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        # src[attention.py:N]: l_i = l_i * alpha + l_ij
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        # src[attention.py:N]: acc = acc * alpha[:, :, None]
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
        v = tl.load(v_view + (indices_0[:, None, None] * v_view_stride_0 + indices_2[None, :, None] * v_view_stride_1 + indices_4[None, None, :] * v_view_stride_2), mask_3[None, :, None], other=0)
        # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(_mask_to_3, tl.float32), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float32), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64])
        # src[attention.py:N]: m_i = m_ij
        m_i = v_2
    # src[attention.py:N]: acc = acc / l_i[:, :, None]
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
    tl.store(out + (indices_0[:, None, None] * out_stride_0 + indices_1[None, :, None] * out_stride_1 + indices_4[None, None, :] * out_stride_2), v_12, mask_1[None, :, None])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _BLOCK_SIZE_1 = 64
    _RDIM_SIZE_2 = 64
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 32
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (q_in.size(1) * triton.cdiv(m_dim, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, q_in.size(1), k_view.stride(0), k_view.stride(1), k_view.stride(2), out.stride(0), out.stride(1), out.stride(2), q_view.stride(0), q_view.stride(1), q_view.stride(2), v_view.stride(0), v_view.stride(1), v_view.stride(2), m_dim, n_dim, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_persistent_interleaved_l2_grouping)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    total_pids = tl.cdiv(32, _BLOCK_SIZE_0) * tl.cdiv(512, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM):
        # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
        num_pid_m = tl.cdiv(32, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(512, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 4 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 4
        group_size_m = min(num_pid_m - first_pid_m, 4)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
        m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
        # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
        l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
        # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
        # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
        q = tl.load(tl.make_block_ptr(q_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
        # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
        # src[attention.py:N]:     qk = torch.bmm(q, k)
        # src[attention.py:N-N]: ...
        for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
            q_copy = q
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_copy_0 = q_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
            k = tl.load(tl.make_block_ptr(k_view, [32, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [_BLOCK_SIZE_0, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
            # src[attention.py:N]: qk = torch.bmm(q, k)
            qk = tl.cast(tl.dot(tl.cast(q_copy_0, tl.float16), tl.cast(k, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
            # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
            amax = tl.cast(tl.max(qk, 2), tl.float16)
            v_0 = 0.18033688
            v_1 = tl.cast(amax * v_0, tl.float16)
            v_2 = tl.cast(v_1, tl.float32)
            v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
            # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
            v_4 = 0.18033688
            v_5 = tl.cast(qk * v_4, tl.float16)
            subscript = v_3[:, :, None]
            v_6 = tl.cast(v_5, tl.float32)
            v_7 = v_6 - subscript
            # src[attention.py:N]: p = torch.exp2(qk)
            v_8 = libdevice.exp2(v_7)
            # src[attention.py:N]: l_ij = torch.sum(p, -1)
            l_ij = tl.cast(tl.sum(v_8, 2), tl.float32)
            # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
            v_9 = m_i_copy_0 - v_3
            v_10 = libdevice.exp2(v_9)
            # src[attention.py:N]: l_i = l_i * alpha + l_ij
            v_11 = l_i_copy_0 * v_10
            l_i = v_11 + l_ij
            # src[attention.py:N]: acc = acc * alpha[:, :, None]
            subscript_1 = v_10[:, :, None]
            v_13 = acc_copy_0 * subscript_1
            # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
            v = tl.load(tl.make_block_ptr(v_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
            # src[attention.py:N]: p = p.to(v.dtype)
            v_14 = tl.cast(v_8, tl.float16)
            # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
            acc = tl.dot(tl.cast(v_14, tl.float16), tl.cast(v, tl.float16), acc=v_13, input_precision='tf32', out_dtype=tl.float32)
            # src[attention.py:N]: m_i = m_ij
            m_i = v_3
        # src[attention.py:N]: acc = acc / l_i[:, :, None]
        subscript_2 = l_i[:, :, None]
        v_15 = acc / subscript_2
        # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
        v_16 = tl.cast(v_15, tl.float16)
        tl.store(tl.make_block_ptr(out, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _NUM_SM = helion.runtime.get_num_sm(q_in.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 32
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (_NUM_SM,), q_view, k_view, v_view, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    num_blocks_0 = 32
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
    l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
    # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
    q = tl.load(q_view + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
        k = tl.load(k_view + (indices_0[:, None, None] * 32768 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        # src[attention.py:N]: qk = torch.bmm(q, k)
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float32), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float32), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        amax = tl.cast(tl.max(qk, 2), tl.float32)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        # src[attention.py:N]: p = torch.exp2(qk)
        v_6 = libdevice.exp2(v_5)
        # src[attention.py:N]: l_ij = torch.sum(p, -1)
        l_ij = tl.cast(tl.sum(v_6, 2), tl.float32)
        # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        # src[attention.py:N]: l_i = l_i * alpha + l_ij
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        # src[attention.py:N]: acc = acc * alpha[:, :, None]
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
        v = tl.load(v_view + (indices_0[:, None, None] * 32768 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(v_6, tl.float32), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float32), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64])
        # src[attention.py:N]: m_i = m_ij
        m_i = v_2
    # src[attention.py:N]: acc = acc / l_i[:, :, None]
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
    tl.store(out + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_12, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _BLOCK_SIZE_1 = 64
    _RDIM_SIZE_2 = 64
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 32
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (32 * triton.cdiv(512, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_bf16xint16)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion__bf16xint16_gemm(x, w, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    num_blocks_0 = tl.cdiv(65536, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[bf16xint16_gemm.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[bf16xint16_gemm.py:N]: for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N]:     x_tile = x[tile_m, tile_k]
    # src[bf16xint16_gemm.py:N]:     w_tile = w[tile_k, tile_n].to(torch.bfloat16)
    # src[bf16xint16_gemm.py:N-N]: ...
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[bf16xint16_gemm.py:N]: x_tile = x[tile_m, tile_k]
        x_tile = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        # src[bf16xint16_gemm.py:N]: w_tile = w[tile_k, tile_n].to(torch.bfloat16)
        load_1 = tl.load(w + (indices_2[:, None] * 1280 + indices_1[None, :] * 1), None)
        v_0 = tl.cast(load_1, tl.bfloat16)
        # src[bf16xint16_gemm.py:N]: acc = hl.dot(x_tile, w_tile, acc=acc)
        acc = tl.dot(tl.cast(x_tile, tl.bfloat16), tl.cast(v_0, tl.bfloat16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[bf16xint16_gemm.py:N]: out[tile_m, tile_n] = acc.to(torch.bfloat16)
    v_1 = tl.cast(acc, tl.bfloat16)
    tl.store(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), v_1, None)

def _bf16xint16_gemm(x: Tensor, w: Tensor, *, _launcher=_default_launcher):
    """
    x is bf16, w is int16.
    """
    # src[bf16xint16_gemm.py:N]: M, K = x.shape
    M, K = x.shape
    # src[bf16xint16_gemm.py:N]: K2, N = w.shape
    K2, N = w.shape
    # src[bf16xint16_gemm.py:N]: assert K == K2, f"size mismatch {K} != {K2}"
    assert K == K2, f'size mismatch {K} != {K2}'
    # src[bf16xint16_gemm.py:N]: out = torch.empty([M, N], dtype=torch.bfloat16, device=x.device)
    out = torch.empty([M, N], dtype=torch.bfloat16, device=x.device)
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[bf16xint16_gemm.py:N]: for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N]:     x_tile = x[tile_m, tile_k]
    # src[bf16xint16_gemm.py:N]:     w_tile = w[tile_k, tile_n].to(torch.bfloat16)
    # src[bf16xint16_gemm.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    # src[bf16xint16_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[bf16xint16_gemm.py:N]:     for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N-N]: ...
    _launcher(_helion__bf16xint16_gemm, (triton.cdiv(65536, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, w, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[bf16xint16_gemm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_bf16xint16)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion__int16xbf16_gemm(x, w, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    num_blocks_0 = tl.cdiv(65536, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[bf16xint16_gemm.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[bf16xint16_gemm.py:N]: for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N]:     x_tile = x[tile_m, tile_k].to(torch.bfloat16)
    # src[bf16xint16_gemm.py:N]:     w_tile = w[tile_k, tile_n]
    # src[bf16xint16_gemm.py:N-N]: ...
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[bf16xint16_gemm.py:N]: x_tile = x[tile_m, tile_k].to(torch.bfloat16)
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        v_0 = tl.cast(load, tl.bfloat16)
        # src[bf16xint16_gemm.py:N]: w_tile = w[tile_k, tile_n]
        w_tile = tl.load(w + (indices_2[:, None] * 1280 + indices_1[None, :] * 1), None)
        # src[bf16xint16_gemm.py:N]: acc = hl.dot(x_tile, w_tile, acc=acc)
        acc = tl.dot(tl.cast(v_0, tl.bfloat16), tl.cast(w_tile, tl.bfloat16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[bf16xint16_gemm.py:N]: out[tile_m, tile_n] = acc.to(torch.bfloat16)
    v_1 = tl.cast(acc, tl.bfloat16)
    tl.store(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), v_1, None)

def _int16xbf16_gemm(x: Tensor, w: Tensor, *, _launcher=_default_launcher):
    """
    x is int16, w is bf16.
    """
    # src[bf16xint16_gemm.py:N]: M, K = x.shape
    M, K = x.shape
    # src[bf16xint16_gemm.py:N]: K2, N = w.shape
    K2, N = w.shape
    # src[bf16xint16_gemm.py:N]: assert K == K2, f"size mismatch {K} != {K2}"
    assert K == K2, f'size mismatch {K} != {K2}'
    # src[bf16xint16_gemm.py:N]: out = torch.empty([M, N], dtype=torch.bfloat16, device=x.device)
    out = torch.empty([M, N], dtype=torch.bfloat16, device=x.device)
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[bf16xint16_gemm.py:N]: for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N]:     x_tile = x[tile_m, tile_k].to(torch.bfloat16)
    # src[bf16xint16_gemm.py:N]:     w_tile = w[tile_k, tile_n]
    # src[bf16xint16_gemm.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[bf16xint16_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    # src[bf16xint16_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[bf16xint16_gemm.py:N]:     for tile_k in hl.tile(K):
    # src[bf16xint16_gemm.py:N-N]: ...
    _launcher(_helion__int16xbf16_gemm, (triton.cdiv(65536, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, w, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[bf16xint16_gemm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_bmm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_bmm(A, B, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[bmm.py:N]: for tile_b, tile_m, tile_n in hl.tile([b, m, n]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(512, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    # src[bmm.py:N]: acc = hl.zeros([tile_b, tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
    # src[bmm.py:N]: for tile_k in hl.tile(k):
    # src[bmm.py:N]:     acc = torch.baddbmm(
    # src[bmm.py:N]:         acc, A[tile_b, tile_m, tile_k], B[tile_b, tile_k, tile_n]
    # src[bmm.py:N-N]: ...
    for offset_3 in tl.range(0, 768, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[bmm.py:N]: acc, A[tile_b, tile_m, tile_k], B[tile_b, tile_k, tile_n]
        load = tl.load(A + (indices_0[:, None, None] * 393216 + indices_1[None, :, None] * 768 + indices_3[None, None, :] * 1), None)
        load_1 = tl.load(B + (indices_0[:, None, None] * 786432 + indices_3[None, :, None] * 1024 + indices_2[None, None, :] * 1), None)
        # src[bmm.py:N]: acc = torch.baddbmm(
        # src[bmm.py:N]:     acc, A[tile_b, tile_m, tile_k], B[tile_b, tile_k, tile_n]
        # src[bmm.py:N]: )
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[bmm.py:N]: out[tile_b, tile_m, tile_n] = acc
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None, None] * 524288 + indices_1[None, :, None] * 1024 + indices_2[None, None, :] * 1), v_0, None)

def bmm(A: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs batch matrix multiplication.

    Args:
        A: Input tensor of shape [B, M, K]
        B: Input tensor of shape [B, K, N]

    Returns:
        Output tensor of shape [B, M, N] containing the result of batch matrix multiplication
    """
    # src[bmm.py:N]: b, m, k = A.size()
    b, m, k = A.size()
    # src[bmm.py:N]: b, k, n = B.size()
    b, k, n = B.size()
    # src[bmm.py:N]: out = torch.empty(
    # src[bmm.py:N]:     [b, m, n], device=A.device, dtype=torch.promote_types(A.dtype, B.dtype)
    # src[bmm.py:N]: )
    out = torch.empty([b, m, n], device=A.device, dtype=torch.promote_types(A.dtype, B.dtype))
    # src[bmm.py:N]: for tile_b, tile_m, tile_n in hl.tile([b, m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    # src[bmm.py:N]: for tile_k in hl.tile(k):
    # src[bmm.py:N]:     acc = torch.baddbmm(
    # src[bmm.py:N]:         acc, A[tile_b, tile_m, tile_k], B[tile_b, tile_k, tile_n]
    # src[bmm.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[bmm.py:N]: for tile_b, tile_m, tile_n in hl.tile([b, m, n]):
    # src[bmm.py:N]:     acc = hl.zeros([tile_b, tile_m, tile_n], dtype=torch.float32)
    # src[bmm.py:N]:     for tile_k in hl.tile(k):
    # src[bmm.py:N-N]: ...
    _launcher(_helion_bmm, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(512, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), A, B, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[bmm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_concat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    num_blocks_0 = tl.cdiv(512, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < 1012
    # src[concatenate.py:N]: x, [tile0, tile1], extra_mask=(tile1.index < x.size(1))[None, :]
    v_0 = tl.full([], 500, tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    # src[concatenate.py:N]: x_part = hl.load(
    # src[concatenate.py:N]:     x, [tile0, tile1], extra_mask=(tile1.index < x.size(1))[None, :]
    # src[concatenate.py:N]: )
    x_part = tl.load(x + (indices_0[:, None] * 500 + indices_1[None, :] * 1), mask_1[None, :] & subscript, other=0)
    # src[concatenate.py:N]: [tile0, tile1.index - x.size(1)],
    v_2 = tl.full([], 500, tl.int32)
    v_3 = indices_1 - v_2
    # src[concatenate.py:N]: extra_mask=(tile1.index >= x.size(1))[None, :],
    v_4 = tl.full([], 500, tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    # src[concatenate.py:N]: y_part = hl.load(
    # src[concatenate.py:N]:     y,
    # src[concatenate.py:N]:     [tile0, tile1.index - x.size(1)],
    # src[concatenate.py:N-N]: ...
    y_part = tl.load(y + (indices_0[:, None] * 512 + v_3[None, :] * 1), mask_1[None, :] & subscript_1, other=0)
    # src[concatenate.py:N]: (tile1.index < x.size(1))[None, :], x_part, y_part
    v_6 = tl.full([], 500, tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    # src[concatenate.py:N]: out[tile0, tile1] = torch.where(
    # src[concatenate.py:N]:     (tile1.index < x.size(1))[None, :], x_part, y_part
    # src[concatenate.py:N]: )
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(out + (indices_0[:, None] * 1012 + indices_1[None, :] * 1), v_8, mask_1[None, :])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    # src[concatenate.py:N]: assert x.size(0) == y.size(0)
    assert x.size(0) == y.size(0)
    # src[concatenate.py:N]: out = torch.empty(
    # src[concatenate.py:N]:     [x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device
    # src[concatenate.py:N]: )
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    # src[concatenate.py:N]:     # Most masking is automatic in helion, but tile1 spans both x and y we need to do some manual masking
    # src[concatenate.py:N]:     x_part = hl.load(
    # src[concatenate.py:N-N]: ...
    _launcher(_helion_concat2d_dim1, (triton.cdiv(512, _BLOCK_SIZE_0) * triton.cdiv(1012, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[concatenate.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_concat_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    num_blocks_0 = tl.cdiv(222, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 222
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < 251
    # src[concatenate.py:N]: x, [tile0, tile1], extra_mask=(tile1.index < x.size(1))[None, :]
    v_0 = tl.full([], 100, tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    # src[concatenate.py:N]: x_part = hl.load(
    # src[concatenate.py:N]:     x, [tile0, tile1], extra_mask=(tile1.index < x.size(1))[None, :]
    # src[concatenate.py:N]: )
    x_part = tl.load(x + (indices_0[:, None] * 100 + indices_1[None, :] * 1), mask_0[:, None] & mask_1[None, :] & subscript, other=0)
    # src[concatenate.py:N]: [tile0, tile1.index - x.size(1)],
    v_2 = tl.full([], 100, tl.int32)
    v_3 = indices_1 - v_2
    # src[concatenate.py:N]: extra_mask=(tile1.index >= x.size(1))[None, :],
    v_4 = tl.full([], 100, tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    # src[concatenate.py:N]: y_part = hl.load(
    # src[concatenate.py:N]:     y,
    # src[concatenate.py:N]:     [tile0, tile1.index - x.size(1)],
    # src[concatenate.py:N-N]: ...
    y_part = tl.load(y + (indices_0[:, None] * 151 + v_3[None, :] * 1), mask_0[:, None] & mask_1[None, :] & subscript_1, other=0)
    # src[concatenate.py:N]: (tile1.index < x.size(1))[None, :], x_part, y_part
    v_6 = tl.full([], 100, tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    # src[concatenate.py:N]: out[tile0, tile1] = torch.where(
    # src[concatenate.py:N]:     (tile1.index < x.size(1))[None, :], x_part, y_part
    # src[concatenate.py:N]: )
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(tl.make_block_ptr(out, [222, 251], [251, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_8, boundary_check=[0, 1])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    # src[concatenate.py:N]: assert x.size(0) == y.size(0)
    assert x.size(0) == y.size(0)
    # src[concatenate.py:N]: out = torch.empty(
    # src[concatenate.py:N]:     [x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device
    # src[concatenate.py:N]: )
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    # src[concatenate.py:N]: for tile0, tile1 in hl.tile(out.size()):
    # src[concatenate.py:N]:     # Most masking is automatic in helion, but tile1 spans both x and y we need to do some manual masking
    # src[concatenate.py:N]:     x_part = hl.load(
    # src[concatenate.py:N-N]: ...
    _launcher(_helion_concat2d_dim1, (triton.cdiv(222, _BLOCK_SIZE_0) * triton.cdiv(251, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[concatenate.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_cross_entropy)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_cross_entropy(labels, logits_flat, logits, losses, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[cross_entropy.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 1000
    # src[cross_entropy.py:N]: labels_tile = labels[tile_n]  # [tile_size]
    labels_tile = tl.load(labels + indices_0 * 1, None)
    # src[cross_entropy.py:N]: base_indices_tile = tile_n.index * v  # [tile_size]
    v_0 = tl.full([], 1000, tl.int32)
    v_1 = tl.cast(indices_0 * v_0, tl.int32)
    # src[cross_entropy.py:N]: flat_indices = base_indices_tile + labels_tile
    v_2 = tl.cast(v_1, tl.int64)
    v_3 = v_2 + labels_tile
    # src[cross_entropy.py:N]: logits_at_target = hl.load(logits_flat, [flat_indices])
    logits_at_target = tl.load(logits_flat + v_3 * 1, None)
    # src[cross_entropy.py:N]: logits_rows = logits[tile_n, :]  # [tile_size, V]
    logits_rows = tl.load(logits + (indices_0[:, None] * 1000 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    # src[cross_entropy.py:N]: max_logits = torch.amax(logits_rows, dim=-1, keepdim=True)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), logits_rows, tl.full([], float('-inf'), tl.float32))
    max_logits = tl.cast(tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[cross_entropy.py:N]: shifted = logits_rows - max_logits
    v_4 = logits_rows - max_logits
    # src[cross_entropy.py:N]: exp_shifted = torch.exp(shifted)
    v_5 = libdevice.exp(v_4)
    # src[cross_entropy.py:N]: sum_exp = torch.sum(exp_shifted, dim=-1, keepdim=True)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), v_5, tl.full([], 0, tl.float32))
    sum_exp = tl.cast(tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[cross_entropy.py:N]: log_sum_exp = max_logits.squeeze(-1) + torch.log(sum_exp.squeeze(-1))
    squeeze = tl.reshape(max_logits, [_BLOCK_SIZE_0])
    squeeze_1 = tl.reshape(sum_exp, [_BLOCK_SIZE_0])
    v_6 = tl_math.log(squeeze_1)
    v_7 = squeeze + v_6
    # src[cross_entropy.py:N]: losses[tile_n] = log_sum_exp - logits_at_target
    v_8 = v_7 - logits_at_target
    tl.store(losses + indices_0 * 1, v_8, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes the cross entropy loss between logits and target labels.

    Implements the cross entropy loss function commonly used in classification tasks.
    The function computes the log softmax of the logits and then calculates the negative
    log likelihood of the true labels.

    Args:
        logits: Input logits tensor of shape [N, V] where N is batch size and V is vocabulary size
        labels: Target labels tensor of shape [N] containing class indices

    Returns:
        A scalar tensor containing the mean cross entropy loss
    """
    # src[cross_entropy.py:N]: n, v = logits.shape
    n, v = logits.shape
    # src[cross_entropy.py:N]: losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    # src[cross_entropy.py:N]: logits_flat = logits.view(-1)
    logits_flat = logits.view(-1)
    # src[cross_entropy.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 1024
    # src[cross_entropy.py:N]: labels_tile = labels[tile_n]  # [tile_size]
    _BLOCK_SIZE_0 = 1
    # src[cross_entropy.py:N]: for tile_n in hl.tile(n):
    # src[cross_entropy.py:N]:     # Get data for this tile
    # src[cross_entropy.py:N]:     labels_tile = labels[tile_n]  # [tile_size]
    # src[cross_entropy.py:N-N]: ...
    _launcher(_helion_cross_entropy, (128,), labels, logits_flat, logits, losses, _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[cross_entropy.py:N]: return losses.mean()
    return losses.mean()

--- assertExpectedJournal(TestExamples.test_embedding_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[embedding.py:N]: out[tile_b, tile_e] = weight[x_flat[tile_b], tile_e]
    load = tl.load(tl.make_block_ptr(x_flat, [1024], [1], [offset_0], [_BLOCK_SIZE_0], [0]), boundary_check=[0], padding_option='zero')
    load_1 = tl.load(weight + (load[:, None] * 256 + indices_1[None, :] * 1), None)
    tl.store(tl.make_block_ptr(out, [1024, 256], [256, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), load_1, boundary_check=[0, 1])

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    # src[embedding.py:N]: x_flat = x.reshape(-1)  # collapse x into a single dimension
    x_flat = x.reshape(-1)
    # src[embedding.py:N]: _, embedding_dim = weight.size()
    _, embedding_dim = weight.size()
    # src[embedding.py:N]: out = torch.empty(
    # src[embedding.py:N]:     [x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device
    # src[embedding.py:N]: )
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    # src[embedding.py:N]:     out[tile_b, tile_e] = weight[x_flat[tile_b], tile_e]
    _launcher(_helion_embedding, (triton.cdiv(1024, _BLOCK_SIZE_0), triton.cdiv(256, _BLOCK_SIZE_1)), x_flat, weight, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[embedding.py:N]: return out.view(*x.size(), embedding_dim)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_embedding_pointers)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, _BLOCK_SIZE_1: tl.constexpr):
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    num_blocks_0 = 1024
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[embedding.py:N]: out[tile_b, tile_e] = weight[x_flat[tile_b], tile_e]
    load = tl.load(x_flat + indices_0 * 1, None)
    load_1 = tl.load(weight + (load[:, None] * 256 + indices_1[None, :] * 1), None)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), load_1, None)

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    # src[embedding.py:N]: x_flat = x.reshape(-1)  # collapse x into a single dimension
    x_flat = x.reshape(-1)
    # src[embedding.py:N]: _, embedding_dim = weight.size()
    _, embedding_dim = weight.size()
    # src[embedding.py:N]: out = torch.empty(
    # src[embedding.py:N]:     [x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device
    # src[embedding.py:N]: )
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    _BLOCK_SIZE_1 = 256
    # src[embedding.py:N]: for tile_b, tile_e in hl.tile([x_flat.size(0), embedding_dim]):
    # src[embedding.py:N]:     out[tile_b, tile_e] = weight[x_flat[tile_b], tile_e]
    _launcher(_helion_embedding, (1024 * triton.cdiv(256, _BLOCK_SIZE_1),), x_flat, weight, out, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[embedding.py:N]: return out.view(*x.size(), embedding_dim)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_exp_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_exp_bwd(dy, exp_x, dx, _BLOCK_SIZE_0: tl.constexpr):
    # src[exp.py:N]: for tile in hl.tile(exp_x.size()):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[exp.py:N]: dx[tile] = dy[tile] * exp_x[tile]
    load = tl.load(dy + indices_0 * 1, None)
    load_1 = tl.load(exp_x + indices_0 * 1, None)
    v_0 = load * load_1
    tl.store(dx + indices_0 * 1, v_0, None)

def exp_bwd(dy: torch.Tensor, exp_x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes the gradient of the exponential function with respect to the input tensor.

    Args:
        dy: Gradient of the output tensor
        exp_x: Saved activation from the forward pass

    Returns:
        Gradient of the input tensor
    """
    # src[exp.py:N]: dx = torch.empty_like(exp_x)
    dx = torch.empty_like(exp_x)
    # src[exp.py:N]: for tile in hl.tile(exp_x.size()):
    _BLOCK_SIZE_0 = 16
    # src[exp.py:N]: for tile in hl.tile(exp_x.size()):
    # src[exp.py:N]:     dx[tile] = dy[tile] * exp_x[tile]
    _launcher(_helion_exp_bwd, (triton.cdiv(1024, _BLOCK_SIZE_0),), dy, exp_x, dx, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[exp.py:N]: return dx
    return dx

--- assertExpectedJournal(TestExamples.test_exp_fwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_exp_fwd(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[exp.py:N]: for tile in hl.tile(x.size()):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[exp.py:N]: out[tile] = torch.exp(x[tile])
    load = tl.load(x + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    v_1 = libdevice.exp(v_0)
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(out + indices_0 * 1, v_2, None)

def exp_fwd(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes the exponential of all elements in the input tensor.

    Args:
        x: Input tensor

    Returns:
        Output tensor with the exponential of each element in the input
    """
    # src[exp.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[exp.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    # src[exp.py:N]: for tile in hl.tile(x.size()):
    # src[exp.py:N]:     out[tile] = torch.exp(x[tile])
    _launcher(_helion_exp_fwd, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[exp.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_fp8_attention)
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_attention_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[fp8_attention.py:N]: for bh in hl.grid(batch_heads):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[fp8_attention.py:N]: out[b, h, tile_m, :] = acc.to(torch.float8_e4m3fn)
    symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
    symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
    # src[fp8_attention.py:N]: for tile_m in hl.tile(seq_len):
    # src[fp8_attention.py:N]:     # Initialize for online softmax
    # src[fp8_attention.py:N]:     m_i = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    # src[fp8_attention.py:N-N]: ...
    for offset_4 in tl.range(0, 256, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        # src[fp8_attention.py:N]: m_i = hl.full([tile_m], float("-inf"), dtype=torch.float32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        # src[fp8_attention.py:N]: l_i = hl.full([tile_m], 0.0, dtype=torch.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        # src[fp8_attention.py:N]: acc = hl.zeros([tile_m, head_dim], dtype=torch.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        # src[fp8_attention.py:N]: q_tile = q[bh, tile_m, :]  # [tile_m, dim]
        q_tile = tl.load(q + (offset_0 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        # src[fp8_attention.py:N]: for tile_n in hl.tile(seq_len):
        # src[fp8_attention.py:N]:     # Load key tile and transpose for Q @ K^T
        # src[fp8_attention.py:N]:     k_tile = k[bh, tile_n, :]  # [tile_n, dim] - keep in FP8
        # src[fp8_attention.py:N-N]: ...
        for offset_2 in tl.range(0, 256, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            # src[fp8_attention.py:N]: k_tile = k[bh, tile_n, :]  # [tile_n, dim] - keep in FP8
            k_tile = tl.load(k + (offset_0 * 16384 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            # src[fp8_attention.py:N]: k_tile_t = k_tile.transpose(0, 1)  # [dim, tile_n]
            k_tile_t = tl.permute(k_tile, [1, 0])
            # src[fp8_attention.py:N]: qk = hl.dot(q_tile, k_tile_t)  # [tile_m, tile_n]
            qk = tl.dot(tl.cast(q_tile_copy_0, tl.float8e4nv), tl.cast(k_tile_t, tl.float8e4nv), input_precision='tf32', out_dtype=tl.float32)
            # src[fp8_attention.py:N]: qk_scaled = qk * sm_scale  # [tile_m, tile_n]
            v_0 = 0.18033688
            v_1 = qk * v_0
            # src[fp8_attention.py:N]: qk_max = torch.amax(qk_scaled, dim=-1)  # [tile_m]
            qk_max = tl.cast(tl.max(v_1, 1), tl.float32)
            # src[fp8_attention.py:N]: m_new = torch.maximum(m_i, qk_max)
            v_2 = triton_helpers.maximum(m_i_copy_0, qk_max)
            # src[fp8_attention.py:N]: qk_shifted = qk_scaled - m_new[:, None]
            subscript = v_2[:, None]
            v_3 = v_1 - subscript
            # src[fp8_attention.py:N]: p = torch.exp2(qk_shifted)  # [tile_m, tile_n]
            v_4 = libdevice.exp2(v_3)
            # src[fp8_attention.py:N]: l_ij = torch.sum(p, dim=-1)  # [tile_m]
            l_ij = tl.cast(tl.sum(v_4, 1), tl.float32)
            # src[fp8_attention.py:N]: alpha = torch.exp2(m_i - m_new)
            v_5 = m_i_copy_0 - v_2
            v_6 = libdevice.exp2(v_5)
            # src[fp8_attention.py:N]: l_i = l_i * alpha + l_ij
            v_7 = l_i_copy_0 * v_6
            l_i = v_7 + l_ij
            # src[fp8_attention.py:N]: acc = acc * alpha[:, None]
            subscript_1 = v_6[:, None]
            v_9 = acc_copy_0 * subscript_1
            # src[fp8_attention.py:N]: v_tile = v[bh, :, tile_n]  # [dim, tile_n] - keep in FP8
            v_tile = tl.load(v + (offset_0 * 16384 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            # src[fp8_attention.py:N]: p_fp8 = p.to(v.dtype)  # Convert to same FP8 type as V
            v_10 = tl.cast(v_4, tl.float8e4nv)
            # src[fp8_attention.py:N]: v_t = v_tile.t()  # [tile_n, dim]
            v_t = tl.permute(v_tile, [1, 0])
            # src[fp8_attention.py:N]: acc = hl.dot(p_fp8, v_t, acc=acc)  # [tile_m, dim]
            acc = tl.dot(tl.cast(v_10, tl.float8e4nv), tl.cast(v_t, tl.float8e4nv), acc=v_9, input_precision='tf32', out_dtype=tl.float32)
            # src[fp8_attention.py:N]: m_i = m_new
            m_i = v_2
        # src[fp8_attention.py:N]: acc = acc / l_i[:, None]
        subscript_2 = l_i[:, None]
        v_11 = acc / subscript_2
        # src[fp8_attention.py:N]: out[b, h, tile_m, :] = acc.to(torch.float8_e4m3fn)
        v_12 = tl.cast(v_11, tl.float8e4nv)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_12, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention using FP8 precision.
    Implements the attention with FP8 tensors for improved performance and memory efficiency.
    Args:
        q: Query tensor of shape [batch*heads, seq, dim] in FP8 format
        k: Key tensor of shape [batch*heads, seq, dim] in FP8 format
        v: Value tensor of shape [batch*heads, dim, seq] (pre-transposed) in FP8 format
        batch: Number of batches
        heads: Number of attention heads
    Returns:
        Output tensor of shape [batch, heads, seq_len, head_dim] in FP8 format
    """
    # src[fp8_attention.py:N]: batch_heads = q.size(0)
    batch_heads = q.size(0)
    # src[fp8_attention.py:N]: seq_len = q.size(1)
    seq_len = q.size(1)
    # src[fp8_attention.py:N]: head_dim = q.size(2)
    head_dim = q.size(2)
    # src[fp8_attention.py:N]: out = torch.empty(
    # src[fp8_attention.py:N]:     [batch, heads, seq_len, head_dim], dtype=torch.float8_e4m3fn, device=q.device
    # src[fp8_attention.py:N]: )
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e4m3fn, device=q.device)
    # src[fp8_attention.py:N]: sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    # src[fp8_attention.py:N]: sm_scale = sm_scale * 1.44269504
    sm_scale = sm_scale * 1.44269504
    # src[fp8_attention.py:N]: for bh in hl.grid(batch_heads):
    _RDIM_SIZE_2 = 64
    # src[fp8_attention.py:N]: for tile_m in hl.tile(seq_len):
    # src[fp8_attention.py:N]:     # Initialize for online softmax
    # src[fp8_attention.py:N]:     m_i = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    # src[fp8_attention.py:N-N]: ...
    _BLOCK_SIZE_1 = 64
    # src[fp8_attention.py:N]: for tile_n in hl.tile(seq_len):
    # src[fp8_attention.py:N]:     # Load key tile and transpose for Q @ K^T
    # src[fp8_attention.py:N]:     k_tile = k[bh, tile_n, :]  # [tile_n, dim] - keep in FP8
    # src[fp8_attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 64
    # src[fp8_attention.py:N]: for bh in hl.grid(batch_heads):
    # src[fp8_attention.py:N]:     # Calculate batch and head indices
    # src[fp8_attention.py:N]:     b = bh // heads
    # src[fp8_attention.py:N-N]: ...
    _launcher(_helion_fp8_attention_kernel, (8,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[fp8_attention.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_fp8_gemm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_gemm(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[fp8_gemm.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[fp8_gemm.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[fp8_gemm.py:N]: for tile_k in hl.tile(k):
    # src[fp8_gemm.py:N]:     # Load FP8 tiles directly - no conversion needed
    # src[fp8_gemm.py:N]:     x_tile = x[tile_m, tile_k]
    # src[fp8_gemm.py:N-N]: ...
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[fp8_gemm.py:N]: x_tile = x[tile_m, tile_k]
        x_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None)
        # src[fp8_gemm.py:N]: y_tile = y[tile_k, tile_n]
        y_tile = tl.load(y + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        # src[fp8_gemm.py:N]: acc = hl.dot(x_tile, y_tile, acc=acc)
        acc = tl.dot(tl.cast(x_tile, tl.float8e4nv), tl.cast(y_tile, tl.float8e4nv), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[fp8_gemm.py:N]: out[tile_m, tile_n] = acc.to(torch.float16)
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_0, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    FP8 General Matrix Multiplication (GEMM).
    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.
    Args:
        x (torch.Tensor): Input tensor of shape [m, k] in FP8 format.
        y (torch.Tensor): Input tensor of shape [k, n] in FP8 format.
    Returns:
        torch.Tensor: Output tensor of shape [m, n] in FP16 format.
    """
    # src[fp8_gemm.py:N]: m, k = x.size()
    m, k = x.size()
    # src[fp8_gemm.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[fp8_gemm.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[fp8_gemm.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[fp8_gemm.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[fp8_gemm.py:N]: for tile_k in hl.tile(k):
    # src[fp8_gemm.py:N]:     # Load FP8 tiles directly - no conversion needed
    # src[fp8_gemm.py:N]:     x_tile = x[tile_m, tile_k]
    # src[fp8_gemm.py:N-N]: ...
    _BLOCK_SIZE_2 = 32
    # src[fp8_gemm.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[fp8_gemm.py:N]:     # Accumulate in FP32 for accuracy
    # src[fp8_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[fp8_gemm.py:N-N]: ...
    _launcher(_helion_fp8_gemm, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    # src[fp8_gemm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_fused_linear_jsd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fused_linear_jsd_kernel(student_logits, teacher_logits, loss, temperature, beta, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[fused_linear_jsd.py:N]: for batch in hl.tile(student_logits.shape[0]):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[fused_linear_jsd.py:N]: student_prob = torch.log_softmax(student_logits[batch, :] / temperature, dim=-1)
    load = tl.load(student_logits + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    v_0 = load / temperature
    amax = tl.cast(tl.reshape(tl.max(v_0, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_1 = v_0 - amax
    v_2 = libdevice.exp(v_1)
    sum_1 = tl.cast(tl.reshape(tl.sum(v_2, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_3 = tl_math.log(sum_1)
    v_4 = v_1 - v_3
    # src[fused_linear_jsd.py:N]: teacher_prob = torch.log_softmax(teacher_logits[batch, :] / temperature, dim=-1)
    load_1 = tl.load(teacher_logits + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    v_5 = load_1 / temperature
    amax_1 = tl.cast(tl.reshape(tl.max(v_5, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_6 = v_5 - amax_1
    v_7 = libdevice.exp(v_6)
    sum_2 = tl.cast(tl.reshape(tl.sum(v_7, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_8 = tl_math.log(sum_2)
    v_9 = v_6 - v_8
    # src[fused_linear_jsd.py:N]: student_prob = student_prob.to(torch.float).view(-1, student_prob.size(-1))
    student_prob_1 = tl.reshape(v_4, [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    # src[fused_linear_jsd.py:N]: teacher_prob = teacher_prob.to(torch.float).view(-1, teacher_prob.size(-1))
    teacher_prob_1 = tl.reshape(v_9, [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    # src[fused_linear_jsd.py:N]: m = torch.exp(student_prob) + beta * (
    v_10 = libdevice.exp(student_prob_1)
    # src[fused_linear_jsd.py:N]: torch.exp(teacher_prob) - torch.exp(student_prob)
    v_11 = libdevice.exp(teacher_prob_1)
    v_12 = libdevice.exp(student_prob_1)
    v_13 = v_11 - v_12
    # src[fused_linear_jsd.py:N]: m = torch.exp(student_prob) + beta * (
    # src[fused_linear_jsd.py:N]:     torch.exp(teacher_prob) - torch.exp(student_prob)
    # src[fused_linear_jsd.py:N]: )
    v_14 = v_13 * beta
    v_15 = v_10 + v_14
    # src[fused_linear_jsd.py:N]: torch.log(m), teacher_prob, reduction="none", log_target=True
    v_16 = tl_math.log(v_15)
    # src[fused_linear_jsd.py:N]: teacher_div = torch.nn.functional.kl_div(
    # src[fused_linear_jsd.py:N]:     torch.log(m), teacher_prob, reduction="none", log_target=True
    # src[fused_linear_jsd.py:N]: ).sum(dim=-1)
    v_17 = teacher_prob_1 - v_16
    v_18 = libdevice.exp(teacher_prob_1)
    v_19 = v_18 * v_17
    teacher_div = tl.cast(tl.sum(v_19, 1), tl.float32)
    # src[fused_linear_jsd.py:N]: torch.log(m), student_prob, reduction="none", log_target=True
    v_20 = tl_math.log(v_15)
    # src[fused_linear_jsd.py:N]: student_div = torch.nn.functional.kl_div(
    # src[fused_linear_jsd.py:N]:     torch.log(m), student_prob, reduction="none", log_target=True
    # src[fused_linear_jsd.py:N]: ).sum(dim=-1)
    v_21 = student_prob_1 - v_20
    v_22 = libdevice.exp(student_prob_1)
    v_23 = v_22 * v_21
    student_div = tl.cast(tl.sum(v_23, 1), tl.float32)
    # src[fused_linear_jsd.py:N]: batch_loss = student_div + beta * (teacher_div - student_div)
    v_24 = teacher_div - student_div
    v_25 = v_24 * beta
    v_26 = student_div + v_25
    # src[fused_linear_jsd.py:N]: loss[batch] = batch_loss
    tl.store(loss + indices_0 * 1, v_26, None)

def fused_linear_jsd_kernel(beta: float, ignore_index: int, temperature: float, student_logits: torch.Tensor, teacher_logits: torch.Tensor, *, _launcher=_default_launcher):
    # src[fused_linear_jsd.py:N]: loss = student_logits.new_empty(student_logits.shape[0], dtype=torch.float)
    loss = student_logits.new_empty(student_logits.shape[0], dtype=torch.float)
    # src[fused_linear_jsd.py:N]: for batch in hl.tile(student_logits.shape[0]):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 256
    # src[fused_linear_jsd.py:N]: for batch in hl.tile(student_logits.shape[0]):
    # src[fused_linear_jsd.py:N]:     student_prob = torch.log_softmax(student_logits[batch, :] / temperature, dim=-1)
    # src[fused_linear_jsd.py:N]:     teacher_prob = torch.log_softmax(teacher_logits[batch, :] / temperature, dim=-1)
    # src[fused_linear_jsd.py:N-N]: ...
    _launcher(_helion_fused_linear_jsd_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), student_logits, teacher_logits, loss, temperature, beta, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[fused_linear_jsd.py:N]: return (loss / student_logits.shape[0]).sum()
    return (loss / student_logits.shape[0]).sum()

--- assertExpectedJournal(TestExamples.test_geglu)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_geglu(a_flat, b_flat, out_flat, _BLOCK_SIZE_0: tl.constexpr):
    # src[geglu.py:N]: for tile_idx in hl.tile(total_elements):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[geglu.py:N]: a_vals = a_flat[tile_idx].to(torch.float32)
    load = tl.load(a_flat + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    # src[geglu.py:N]: b_vals = b_flat[tile_idx]
    b_vals = tl.load(b_flat + indices_0 * 1, None)
    # src[geglu.py:N]: a_cubed = a_vals * a_vals * a_vals
    v_1 = v_0 * v_0
    v_2 = v_1 * v_0
    # src[geglu.py:N]: tanh_arg = sqrt_2_over_pi * (a_vals + 0.044715 * a_cubed)
    v_3 = 0.044715
    v_4 = v_2 * v_3
    v_5 = v_0 + v_4
    v_6 = 0.7978845608028654
    v_7 = v_5 * v_6
    # src[geglu.py:N]: tanh_result = torch.tanh(tanh_arg)
    v_8 = libdevice.tanh(v_7)
    # src[geglu.py:N]: gelu_a = 0.5 * a_vals * (1.0 + tanh_result)
    v_9 = 0.5
    v_10 = v_0 * v_9
    v_11 = 1.0
    v_12 = v_8 + v_11
    v_13 = v_10 * v_12
    # src[geglu.py:N]: result = gelu_a.to(b_vals.dtype) * b_vals
    v_14 = tl.cast(v_13, tl.float16)
    v_15 = v_14 * b_vals
    # src[geglu.py:N]: out_flat[tile_idx] = result
    tl.store(out_flat + indices_0 * 1, v_15, None)

def geglu(a: Tensor, b: Tensor, *, _launcher=_default_launcher):
    """
    Performs GEGLU operation: GELU(a) * b using tanh approximation for GELU.

    GELU(a) = 0.5 * a * (1 + tanh(sqrt(2/π) * (a + 0.044715 * a³)))
    GEGLU(a, b) = GELU(a) * b

    Args:
        a (Tensor): Input tensor for GELU activation of any shape.
        b (Tensor): Input tensor for multiplication, must have same shape as a.

    Returns:
        Tensor: Result of GEGLU operation with same shape as inputs.
    """
    # src[geglu.py:N]: assert a.shape == b.shape, (
    # src[geglu.py:N]:     f"Input tensors must have same shape, got {a.shape} != {b.shape}"
    # src[geglu.py:N]: )
    assert a.shape == b.shape, f'Input tensors must have same shape, got {a.shape} != {b.shape}'
    # src[geglu.py:N]: out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    # src[geglu.py:N]: total_elements = a.numel()
    total_elements = a.numel()
    # src[geglu.py:N]: a_flat = a.view(-1)
    a_flat = a.view(-1)
    # src[geglu.py:N]: b_flat = b.view(-1)
    b_flat = b.view(-1)
    # src[geglu.py:N]: out_flat = out.view(-1)
    out_flat = out.view(-1)
    # src[geglu.py:N]: for tile_idx in hl.tile(total_elements):
    _BLOCK_SIZE_0 = 16
    # src[geglu.py:N]: for tile_idx in hl.tile(total_elements):
    # src[geglu.py:N]:     # Load input values and convert to float32 for computation
    # src[geglu.py:N]:     a_vals = a_flat[tile_idx].to(torch.float32)
    # src[geglu.py:N-N]: ...
    _launcher(_helion_geglu, (triton.cdiv(1048576, _BLOCK_SIZE_0),), a_flat, b_flat, out_flat, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[geglu.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_grouped_gemm_jagged)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_grouped_gemm_jagged(group_offsets, A_packed, B, out, A_packed_stride_0, A_packed_stride_1, B_stride_0, B_stride_1, group_offsets_stride_0, out_stride_0, out_stride_1, N, K, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[grouped_gemm.py:N]: for g in hl.grid(G):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[grouped_gemm.py:N]: start = group_offsets[g]
    start = tl.load(group_offsets + offset_0 * group_offsets_stride_0, None)
    # src[grouped_gemm.py:N]: end = group_offsets[g + 1]
    add = 1 + offset_0
    end = tl.load(group_offsets + add * group_offsets_stride_0, None)
    # src[grouped_gemm.py:N]: M_g = end - start
    v_0 = end - start
    # src[grouped_gemm.py:N]: if M_g != 0:
    v_1 = tl.full([], 0, tl.int32)
    v_2 = v_0 != v_1
    # src[grouped_gemm.py:N]: if M_g != 0:
    # src[grouped_gemm.py:N]:     # Create 2D tiling pattern over output dimensions (M_g x N) for current group
    # src[grouped_gemm.py:N]:     for tile_m, tile_n in hl.tile([M_g, N]):
    # src[grouped_gemm.py:N-N]: ...
    if v_2:
        v_0_copy = v_0
        start_copy = start
        v_0_copy_0 = v_0_copy
        start_copy_0 = start_copy
        # src[grouped_gemm.py:N]: for tile_m, tile_n in hl.tile([M_g, N]):
        # src[grouped_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
        # src[grouped_gemm.py:N]:     # K-reduction loop: multiply tiles along K dimension
        # src[grouped_gemm.py:N-N]: ...
        for offset_1 in tl.range(0, v_0_copy_0.to(tl.int32), _BLOCK_SIZE_1):
            indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
            mask_1 = indices_1 < v_0_copy_0
            for offset_2 in tl.range(0, N.to(tl.int32), _BLOCK_SIZE_2):
                indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
                mask_2 = indices_2 < N
                start_copy_0_copy = start_copy_0
                start_copy_0_copy_0 = start_copy_0_copy
                # src[grouped_gemm.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
                acc = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
                # src[grouped_gemm.py:N]: for tile_k in hl.tile(K):
                # src[grouped_gemm.py:N]:     a_blk = A_packed[start + tile_m.index, tile_k]
                # src[grouped_gemm.py:N]:     b_blk = B[tile_k, tile_n]
                # src[grouped_gemm.py:N-N]: ...
                for offset_3 in tl.range(0, K.to(tl.int32), _BLOCK_SIZE_3):
                    indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
                    mask_3 = indices_3 < K
                    start_copy_0_copy_0_copy = start_copy_0_copy_0
                    acc_copy = acc
                    start_copy_0_copy_0_copy_0 = start_copy_0_copy_0_copy
                    acc_copy_0 = acc_copy
                    # src[grouped_gemm.py:N]: a_blk = A_packed[start + tile_m.index, tile_k]
                    v_3 = start_copy_0_copy_0_copy_0[None]
                    v_4 = v_3 + indices_1
                    a_blk = tl.load(A_packed + (v_4[:, None] * A_packed_stride_0 + indices_3[None, :] * A_packed_stride_1), mask_1[:, None] & mask_3[None, :], other=0)
                    # src[grouped_gemm.py:N]: b_blk = B[tile_k, tile_n]
                    b_blk = tl.load(B + (indices_3[:, None] * B_stride_0 + indices_2[None, :] * B_stride_1), mask_3[:, None] & mask_2[None, :], other=0)
                    # src[grouped_gemm.py:N]: acc = torch.addmm(acc, a_blk, b_blk)
                    acc = tl.dot(tl.cast(a_blk, tl.bfloat16), tl.cast(b_blk, tl.bfloat16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
                # src[grouped_gemm.py:N]: out[start + tile_m.index, tile_n] = acc.to(out.dtype)
                v_5 = tl.cast(acc, tl.bfloat16)
                v_6 = start_copy_0_copy_0[None]
                v_7 = v_6 + indices_1
                tl.store(out + (v_7[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), v_5, mask_1[:, None] & mask_2[None, :])

def grouped_gemm_jagged(A_packed: torch.Tensor, B: torch.Tensor, group_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """
    Perform grouped GEMM on jagged inputs using row offsets.

    Args:
        A_packed: Row-wise concatenation of per-group inputs ``A_i``,
            shape ``[sum(M_i), K]``.
        B: Shared weight matrix, shape ``[K, N]``.
        group_offsets: Row offsets delimiting each group within ``A_packed``,
            shape ``[G+1]``. For group ``g``: rows are
            ``start = group_offsets[g]`` to ``end = group_offsets[g+1]``.

    Returns:
        Output tensor of shape ``[sum(M_i), N]`` equal to
        ``torch.cat([A_i @ B for i in groups], dim=0)``.
    """
    # src[grouped_gemm.py:N]: total_M, K = A_packed.shape
    total_M, K = A_packed.shape
    # src[grouped_gemm.py:N]: K2, N = B.shape
    K2, N = B.shape
    # src[grouped_gemm.py:N]: assert K == K2, "K dimension mismatch between A_packed and B"
    assert K == K2, 'K dimension mismatch between A_packed and B'
    # src[grouped_gemm.py:N]: out = torch.empty(
    # src[grouped_gemm.py:N]:     total_M,
    # src[grouped_gemm.py:N]:     N,
    # src[grouped_gemm.py:N-N]: ...
    out = torch.empty(total_M, N, dtype=torch.promote_types(A_packed.dtype, B.dtype), device=A_packed.device)
    # src[grouped_gemm.py:N]: G = group_offsets.size(0) - 1
    G = group_offsets.size(0) - 1
    # src[grouped_gemm.py:N]: for tile_m, tile_n in hl.tile([M_g, N]):
    # src[grouped_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[grouped_gemm.py:N]:     # K-reduction loop: multiply tiles along K dimension
    # src[grouped_gemm.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_1 = 16
    # src[grouped_gemm.py:N]: for tile_k in hl.tile(K):
    # src[grouped_gemm.py:N]:     a_blk = A_packed[start + tile_m.index, tile_k]
    # src[grouped_gemm.py:N]:     b_blk = B[tile_k, tile_n]
    # src[grouped_gemm.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[grouped_gemm.py:N]: for g in hl.grid(G):
    # src[grouped_gemm.py:N]:     start = group_offsets[g]
    # src[grouped_gemm.py:N]:     end = group_offsets[g + 1]
    # src[grouped_gemm.py:N-N]: ...
    _launcher(_helion_grouped_gemm_jagged, (G,), group_offsets, A_packed, B, out, A_packed.stride(0), A_packed.stride(1), B.stride(0), B.stride(1), group_offsets.stride(0), out.stride(0), out.stride(1), N, K, _BLOCK_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[grouped_gemm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_grouped_gemm_jagged_persistent)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_grouped_gemm_jagged_persistent(group_offsets, A_packed, B, out, A_packed_stride_0, A_packed_stride_1, B_stride_0, B_stride_1, group_offsets_stride_0, out_stride_0, out_stride_1, num_workers, G, N, K, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_5: tl.constexpr):
    # src[grouped_gemm.py:N]: for worker_id in hl.grid(num_workers):
    pid_0 = tl.program_id(0)
    offset_2 = pid_0
    # src[grouped_gemm.py:N]: for g in hl.grid(G):
    # src[grouped_gemm.py:N]:     group_start = group_offsets[g]
    # src[grouped_gemm.py:N]:     group_end = group_offsets[g + 1]
    # src[grouped_gemm.py:N-N]: ...
    for offset_3 in tl.range(0, G.to(tl.int32)):
        # src[grouped_gemm.py:N]: group_start = group_offsets[g]
        group_start = tl.load(group_offsets + offset_3 * group_offsets_stride_0, None)
        # src[grouped_gemm.py:N]: group_end = group_offsets[g + 1]
        add = 1 + offset_3
        group_end = tl.load(group_offsets + add * group_offsets_stride_0, None)
        # src[grouped_gemm.py:N]: m_size = group_end - group_start
        v_0 = group_end - group_start
        # src[grouped_gemm.py:N]: if m_size > 0:
        v_1 = tl.full([], 0, tl.int32)
        v_2 = v_0 > v_1
        # src[grouped_gemm.py:N]: if m_size > 0:
        # src[grouped_gemm.py:N]:     # Compute tile grid dimensions for current group
        # src[grouped_gemm.py:N]:     num_m_tiles = (m_size + BLOCK_M - 1) // BLOCK_M
        # src[grouped_gemm.py:N-N]: ...
        if v_2:
            v_0_copy = v_0
            group_start_copy = group_start
            group_end_copy = group_end
            v_0_copy_0 = v_0_copy
            group_start_copy_0 = group_start_copy
            group_end_copy_0 = group_end_copy
            # src[grouped_gemm.py:N]: num_m_tiles = (m_size + BLOCK_M - 1) // BLOCK_M
            _BLOCK_SIZE_0_ = _BLOCK_SIZE_0
            v_3 = tl.cast(v_0_copy_0, tl.int64)
            v_4 = v_3 + _BLOCK_SIZE_0_
            v_5 = tl.full([], 1, tl.int32)
            v_6 = v_4 - v_5
            _BLOCK_SIZE_0__1 = _BLOCK_SIZE_0
            v_7 = tl.cast(v_6, tl.int64)
            v_8 = tl.where((v_7 < 0) != (_BLOCK_SIZE_0__1 < 0), tl.where(v_7 % _BLOCK_SIZE_0__1 != 0, v_7 // _BLOCK_SIZE_0__1 - 1, v_7 // _BLOCK_SIZE_0__1), v_7 // _BLOCK_SIZE_0__1)
            # src[grouped_gemm.py:N]: num_n_tiles = (N + BLOCK_N - 1) // BLOCK_N
            add_1 = N + _BLOCK_SIZE_1
            sub_1 = -1 + N + _BLOCK_SIZE_1
            floordiv = triton_helpers.div_floor_integer(-1 + N + _BLOCK_SIZE_1, _BLOCK_SIZE_1)
            # src[grouped_gemm.py:N]: num_group_tiles = num_m_tiles * num_n_tiles
            v_9 = tl.cast(v_8, tl.int64)
            v_10 = v_9 * floordiv
            # src[grouped_gemm.py:N]: for local_tile in hl.grid(num_group_tiles):
            # src[grouped_gemm.py:N]:     tile_in_group = local_tile * num_workers + worker_id
            # src[grouped_gemm.py:N]:     if tile_in_group < num_group_tiles:
            # src[grouped_gemm.py:N-N]: ...
            for offset_4 in tl.range(0, v_10.to(tl.int32)):
                v_10_copy = v_10
                v_8_copy = v_8
                group_start_copy_0_copy = group_start_copy_0
                group_end_copy_0_copy = group_end_copy_0
                v_10_copy_0 = v_10_copy
                v_8_copy_0 = v_8_copy
                group_start_copy_0_copy_0 = group_start_copy_0_copy
                group_end_copy_0_copy_0 = group_end_copy_0_copy
                # src[grouped_gemm.py:N]: tile_in_group = local_tile * num_workers + worker_id
                mul = num_workers * offset_4
                add_2 = offset_2 + num_workers * offset_4
                # src[grouped_gemm.py:N]: if tile_in_group < num_group_tiles:
                v_11 = tl.cast(v_10_copy_0, tl.int64)
                v_12 = v_11 > add_2
                # src[grouped_gemm.py:N]: if tile_in_group < num_group_tiles:
                # src[grouped_gemm.py:N]:     # Convert linear tile index to 2D (M, N) tile coordinates
                # src[grouped_gemm.py:N]:     m_tile_idx = tile_in_group % num_m_tiles  # pyright: ignore[reportOperatorIssue]
                # src[grouped_gemm.py:N-N]: ...
                if v_12:
                    v_8_copy_0_copy = v_8_copy_0
                    group_start_copy_0_copy_0_copy = group_start_copy_0_copy_0
                    group_end_copy_0_copy_0_copy = group_end_copy_0_copy_0
                    v_8_copy_0_copy_0 = v_8_copy_0_copy
                    group_start_copy_0_copy_0_copy_0 = group_start_copy_0_copy_0_copy
                    group_end_copy_0_copy_0_copy_0 = group_end_copy_0_copy_0_copy
                    # src[grouped_gemm.py:N]: m_tile_idx = tile_in_group % num_m_tiles  # pyright: ignore[reportOperatorIssue]
                    v_13 = tl.cast(v_8_copy_0_copy_0, tl.int64)
                    v_14 = add_2 % v_13
                    v_15 = tl.full([], 0, tl.int32)
                    v_16 = v_14 != v_15
                    v_17 = libdevice.signbit(v_14) != 0 if v_14.dtype is tl.float32 else v_14 < 0
                    v_18 = libdevice.signbit(v_13) != 0 if v_13.dtype is tl.float32 else v_13 < 0
                    v_19 = v_17 != v_18
                    v_20 = v_16 & v_19
                    v_21 = v_14 + v_13
                    v_22 = tl.where(v_20, v_21, v_14)
                    # src[grouped_gemm.py:N]: n_tile_idx = tile_in_group // num_m_tiles
                    v_23 = tl.cast(v_8_copy_0_copy_0, tl.int64)
                    v_24 = tl.where((add_2 < 0) != (v_23 < 0), tl.where(add_2 % v_23 != 0, add_2 // v_23 - 1, add_2 // v_23), add_2 // v_23)
                    # src[grouped_gemm.py:N]: base_row = group_start + m_tile_idx * BLOCK_M
                    _BLOCK_SIZE_0__2 = _BLOCK_SIZE_0
                    v_25 = tl.cast(v_22, tl.int64)
                    v_26 = v_25 * _BLOCK_SIZE_0__2
                    v_27 = group_start_copy_0_copy_0_copy_0 + v_26
                    # src[grouped_gemm.py:N]: base_col = n_tile_idx * BLOCK_N  # pyright: ignore[reportOperatorIssue]
                    _BLOCK_SIZE_1_ = _BLOCK_SIZE_1
                    v_28 = tl.cast(v_24, tl.int64)
                    v_29 = v_28 * _BLOCK_SIZE_1_
                    # src[grouped_gemm.py:N]: row_idx = base_row + hl.arange(BLOCK_M)
                    iota = tl.arange(0, _BLOCK_SIZE_0)
                    v_30 = v_27[None]
                    v_31 = v_30 + iota
                    # src[grouped_gemm.py:N]: col_idx = base_col + hl.arange(BLOCK_N)
                    iota_1 = tl.arange(0, _BLOCK_SIZE_1)
                    v_32 = v_29[None]
                    v_33 = v_32 + iota_1
                    # src[grouped_gemm.py:N]: rows_valid = row_idx < group_end
                    v_34 = group_end_copy_0_copy_0_copy_0[None]
                    v_35 = v_31 < v_34
                    # src[grouped_gemm.py:N]: cols_valid = col_idx < N
                    v_36 = tl.cast(N, tl.int32)
                    v_37 = v_33 < v_36
                    # src[grouped_gemm.py:N]: acc = hl.zeros([BLOCK_M, BLOCK_N], dtype=torch.float32)
                    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
                    # src[grouped_gemm.py:N]: for k_tile in hl.tile(K):
                    # src[grouped_gemm.py:N]:     k_idx = k_tile.index
                    # src[grouped_gemm.py:N-N]: ...
                    for offset_5 in tl.range(0, K.to(tl.int32), _BLOCK_SIZE_5):
                        indices_5 = offset_5 + tl.arange(0, _BLOCK_SIZE_5).to(tl.int32)
                        mask_5 = indices_5 < K
                        v_31_copy = v_31
                        v_35_copy = v_35
                        v_33_copy = v_33
                        v_37_copy = v_37
                        acc_copy = acc
                        v_31_copy_0 = v_31_copy
                        v_35_copy_0 = v_35_copy
                        v_33_copy_0 = v_33_copy
                        v_37_copy_0 = v_37_copy
                        acc_copy_0 = acc_copy
                        # src[grouped_gemm.py:N]: extra_mask=rows_valid[:, None],
                        subscript = v_35_copy_0[:, None]
                        # src[grouped_gemm.py:N]: a_blk = hl.load(
                        # src[grouped_gemm.py:N]:     A_packed,
                        # src[grouped_gemm.py:N]:     [row_idx, k_idx],
                        # src[grouped_gemm.py:N-N]: ...
                        a_blk = tl.load(A_packed + (v_31_copy_0[:, None] * A_packed_stride_0 + indices_5[None, :] * A_packed_stride_1), mask_5[None, :] & subscript, other=0)
                        # src[grouped_gemm.py:N]: extra_mask=cols_valid[None, :],
                        subscript_1 = v_37_copy_0[None, :]
                        # src[grouped_gemm.py:N]: b_blk = hl.load(
                        # src[grouped_gemm.py:N]:     B,
                        # src[grouped_gemm.py:N]:     [k_idx, col_idx],
                        # src[grouped_gemm.py:N-N]: ...
                        b_blk = tl.load(B + (indices_5[:, None] * B_stride_0 + v_33_copy_0[None, :] * B_stride_1), mask_5[:, None] & subscript_1, other=0)
                        # src[grouped_gemm.py:N]: acc = torch.addmm(acc, a_blk, b_blk)
                        acc = tl.dot(tl.cast(a_blk, tl.bfloat16), tl.cast(b_blk, tl.bfloat16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
                    # src[grouped_gemm.py:N]: valid_2d = rows_valid[:, None] & cols_valid[None, :]
                    subscript_2 = v_35[:, None]
                    subscript_3 = v_37[None, :]
                    v_38 = subscript_2 & subscript_3
                    # src[grouped_gemm.py:N]: acc.to(out.dtype),
                    v_39 = tl.cast(acc, tl.bfloat16)
                    # src[grouped_gemm.py:N]: hl.store(
                    # src[grouped_gemm.py:N]:     out,
                    # src[grouped_gemm.py:N]:     [row_idx, col_idx],
                    # src[grouped_gemm.py:N-N]: ...
                    tl.store(out + (v_31[:, None] * out_stride_0 + v_33[None, :] * out_stride_1), v_39, v_38)

def grouped_gemm_jagged_persistent(A_packed: torch.Tensor, B: torch.Tensor, group_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """
    Persistent grouped GEMM with dynamic tile metadata computation.

    This variant computes tile assignments dynamically in the kernel,
    similar to TritonBench's WS variant.

    Args:
        A_packed: Packed A, concatenated by rows across groups, ``[sum(M_i), K]``.
        B: Shared weight matrix, ``[K, N]``.
        group_offsets: Row offsets delimiting each group within ``A_packed``.

    Returns:
        Output tensor of shape ``[sum(M_i), N]``.
    """
    # src[grouped_gemm.py:N]: device = A_packed.device
    device = A_packed.device
    # src[grouped_gemm.py:N]: if device.type == "xpu":
    # src[grouped_gemm.py:N]:     # TODO(EikanWang): gpu_subslice_count is an out-of-date term. we will update it to XeCore number.
    # src[grouped_gemm.py:N]:     num_workers = torch.xpu.get_device_properties(device.index).gpu_subslice_count
    # src[grouped_gemm.py:N-N]: ...
    if device.type == 'xpu':
        # src[grouped_gemm.py:N]: num_workers = torch.xpu.get_device_properties(device.index).gpu_subslice_count
        num_workers = torch.xpu.get_device_properties(device.index).gpu_subslice_count
    else:
        # src[grouped_gemm.py:N]: num_workers = torch.cuda.get_device_properties(
        # src[grouped_gemm.py:N]:     device.index
        # src[grouped_gemm.py:N]: ).multi_processor_count
        num_workers = torch.cuda.get_device_properties(device.index).multi_processor_count
    # src[grouped_gemm.py:N]: total_M, K = A_packed.shape
    total_M, K = A_packed.shape
    # src[grouped_gemm.py:N]: K2, N = B.shape
    K2, N = B.shape
    # src[grouped_gemm.py:N]: assert K == K2
    assert K == K2
    # src[grouped_gemm.py:N]: out = torch.zeros(
    # src[grouped_gemm.py:N]:     total_M,
    # src[grouped_gemm.py:N]:     N,
    # src[grouped_gemm.py:N-N]: ...
    out = torch.zeros(total_M, N, dtype=torch.promote_types(A_packed.dtype, B.dtype), device=A_packed.device)
    # src[grouped_gemm.py:N]: G = group_offsets.size(0) - 1
    G = group_offsets.size(0) - 1
    # src[grouped_gemm.py:N]: num_m_tiles = (m_size + BLOCK_M - 1) // BLOCK_M
    _BLOCK_SIZE_0 = 32
    # src[grouped_gemm.py:N]: num_n_tiles = (N + BLOCK_N - 1) // BLOCK_N
    _BLOCK_SIZE_1 = 32
    # src[grouped_gemm.py:N]: for k_tile in hl.tile(K):
    # src[grouped_gemm.py:N]:     k_idx = k_tile.index
    # src[grouped_gemm.py:N-N]: ...
    _BLOCK_SIZE_5 = 16
    # src[grouped_gemm.py:N]: for worker_id in hl.grid(num_workers):
    # src[grouped_gemm.py:N]:     # Persistent thread pattern: each worker processes tiles across all groups
    # src[grouped_gemm.py:N]:     # using strided/interleaved assignment for load balancing.
    # src[grouped_gemm.py:N-N]: ...
    _launcher(_helion_grouped_gemm_jagged_persistent, (num_workers,), group_offsets, A_packed, B, out, A_packed.stride(0), A_packed.stride(1), B.stride(0), B.stride(1), group_offsets.stride(0), out.stride(0), out.stride(1), num_workers, G, N, K, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_5, num_warps=4, num_stages=1)
    # src[grouped_gemm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_int4_gemm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_bf16_int4(A, B, C, A_stride_0, A_stride_1, B_stride_0, B_stride_1, C_stride_0, C_stride_1, M, N, K, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, mul_1: tl.constexpr):
    # src[int4_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    num_blocks_0 = tl.cdiv(M, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < M
    offset_2 = pid_1 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < N
    # src[int4_gemm.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
    # src[int4_gemm.py:N]: for tile_k_packed in hl.tile(K // 2, block_size=block_size_k_packed):
    floordiv = triton_helpers.div_floor_integer(K, 2)
    # src[int4_gemm.py:N]: for tile_k_packed in hl.tile(K // 2, block_size=block_size_k_packed):
    # src[int4_gemm.py:N]:     # Load corresponding tiles from A (need to load twice the packed tile size)
    # src[int4_gemm.py:N]:     # We need to map tile_k_packed to the corresponding range in A
    # src[int4_gemm.py:N-N]: ...
    for offset_3 in tl.range(0, floordiv.to(tl.int32), _BLOCK_SIZE_0):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_0).to(tl.int32)
        mask_0 = indices_3 < floordiv
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[int4_gemm.py:N]: a_tile_begin = tile_k_packed.begin * 2
        mul = 2 * offset_3
        # src[int4_gemm.py:N]: a_tile = A[tile_m, a_tile_begin : (a_tile_begin + a_tile_len)].to(
        iota = mul + tl.arange(0, mul_1)
        load = tl.load(A + (indices_1[:, None] * A_stride_0 + iota[None, :] * A_stride_1), mask_1[:, None], other=0)
        # src[int4_gemm.py:N]: a_tile = A[tile_m, a_tile_begin : (a_tile_begin + a_tile_len)].to(
        # src[int4_gemm.py:N]:     torch.float32
        # src[int4_gemm.py:N]: )  # [BLOCK_SIZE_M, BLOCK_SIZE_K]
        v_0 = tl.cast(load, tl.float32)
        # src[int4_gemm.py:N]: b_tile = B[tile_k_packed, tile_n]  # [BLOCK_SIZE_K//2, BLOCK_SIZE_N]
        b_tile = tl.load(B + (indices_3[:, None] * B_stride_0 + indices_2[None, :] * B_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        # src[int4_gemm.py:N]: b_lo = ((b_tile << 4) >> 4).to(torch.int8)  # Sign-extend low 4 bits
        v_1 = tl.full([], 4, tl.int8)
        v_2 = b_tile << v_1
        v_3 = tl.full([], 4, tl.int8)
        v_4 = v_2 >> v_3
        # src[int4_gemm.py:N]: b_hi = (b_tile >> 4).to(torch.int8)  # Sign-extend high 4 bits
        v_5 = tl.full([], 4, tl.int8)
        v_6 = b_tile >> v_5
        # src[int4_gemm.py:N]: b_stacked = torch.stack([b_lo, b_hi], dim=1)
        stack_idx = tl.arange(0, 2)
        broadcast_idx = stack_idx[None, :, None]
        expanded_0 = tl.expand_dims(v_4, 1)
        expanded_1 = tl.expand_dims(v_6, 1)
        stacked_result = tl.zeros_like(expanded_0)
        mask_4 = broadcast_idx == 0
        stacked_result = tl.where(mask_4, expanded_0, stacked_result)
        mask_5 = broadcast_idx == 1
        stacked_result = tl.where(mask_5, expanded_1, stacked_result)
        # src[int4_gemm.py:N]: b_unpacked = b_stacked.reshape(
        # src[int4_gemm.py:N]:     tile_k_packed.block_size * 2, tile_n.block_size
        # src[int4_gemm.py:N]: ).to(torch.float32)
        view = tl.reshape(stacked_result, [2 * _BLOCK_SIZE_0, _BLOCK_SIZE_2])
        v_7 = tl.cast(view, tl.float32)
        # src[int4_gemm.py:N]: a_tile = a_tile.unsqueeze(2)  # [BLOCK_SIZE_M, BLOCK_SIZE_K, 1]
        a_tile_1 = v_0[:, :, None]
        # src[int4_gemm.py:N]: b_unpacked = b_unpacked.unsqueeze(0)
        b_unpacked_1 = v_7[None, :, :]
        # src[int4_gemm.py:N]: acc = acc + (a_tile * b_unpacked).sum(dim=1)  # [BLOCK_SIZE_M, BLOCK_SIZE_N]
        v_8 = a_tile_1 * b_unpacked_1
        sum_1 = tl.cast(tl.sum(v_8, 1), tl.float32)
        acc = acc_copy_0 + sum_1
    # src[int4_gemm.py:N]: C[tile_m, tile_n] = acc.to(torch.bfloat16)
    v_10 = tl.cast(acc, tl.bfloat16)
    tl.store(C + (indices_1[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), v_10, mask_1[:, None] & mask_2[None, :])

def matmul_bf16_int4(A: Tensor, B: Tensor, *, _launcher=_default_launcher):
    """
    BFloat16 x INT4 General Matrix Multiplication (GEMM).

    This kernel performs matrix multiplication where:
    - A is a bfloat16 matrix of shape [M, K]
    - B is an int8 matrix of shape [K//2, N] containing packed int4 values
      (two 4-bit values packed into each int8)

    Args:
        A (Tensor): Input tensor of shape [M, K] in bfloat16 format.
        B (Tensor): Packed int4 tensor of shape [K//2, N] in int8 format.

    Returns:
        Tensor: Output tensor of shape [M, N] in bfloat16 format.
    """
    # src[int4_gemm.py:N]: M, K = A.shape
    M, K = A.shape
    # src[int4_gemm.py:N]: _, N = B.shape
    _, N = B.shape
    # src[int4_gemm.py:N]: C = torch.zeros(M, N, dtype=torch.bfloat16, device=A.device)
    C = torch.zeros(M, N, dtype=torch.bfloat16, device=A.device)
    # src[int4_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 32
    # src[int4_gemm.py:N]: for tile_k_packed in hl.tile(K // 2, block_size=block_size_k_packed):
    # src[int4_gemm.py:N]:     # Load corresponding tiles from A (need to load twice the packed tile size)
    # src[int4_gemm.py:N]:     # We need to map tile_k_packed to the corresponding range in A
    # src[int4_gemm.py:N-N]: ...
    _BLOCK_SIZE_0 = 64
    # src[int4_gemm.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    # src[int4_gemm.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[int4_gemm.py:N-N]: ...
    _RDIM_SIZE_3 = triton.next_power_of_2(2 * _BLOCK_SIZE_0)
    _launcher(_helion_matmul_bf16_int4, (triton.cdiv(M, _BLOCK_SIZE_1) * triton.cdiv(N, _BLOCK_SIZE_2),), A, B, C, A.stride(0), A.stride(1), B.stride(0), B.stride(1), C.stride(0), C.stride(1), M, N, K, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_0, 2 * _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[int4_gemm.py:N]: return C
    return C

--- assertExpectedJournal(TestExamples.test_jagged_dense_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_dense_add_2d(x_offsets, x_data, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[jagged_dense_add.py:N]: for tile0 in hl.tile(num_rows):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 500
    # src[jagged_dense_add.py:N]: starts = x_offsets[tile0]
    starts = tl.load(x_offsets + indices_0 * 1, mask_0, other=0)
    # src[jagged_dense_add.py:N]: ends = x_offsets[tile0.index + 1]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + (indices_0 + 1) * 1, mask_0, other=0)
    # src[jagged_dense_add.py:N]: nnz = ends - starts
    v_2 = ends - starts
    # src[jagged_dense_add.py:N]: max_nnz = nnz.amax()
    _mask_to = tl.where(mask_0, v_2, tl.full([], -9223372036854775808, tl.int64))
    max_nnz = tl.cast(tl.max(_mask_to, 0), tl.int64)
    # src[jagged_dense_add.py:N]: for tile1 in hl.tile(0, max_nnz):
    # src[jagged_dense_add.py:N]:     x_slice = hl.load(
    # src[jagged_dense_add.py:N]:         x_data,
    # src[jagged_dense_add.py:N-N]: ...
    for offset_1 in tl.range(0, max_nnz.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_nnz
        starts_copy = starts
        v_2_copy = v_2
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        # src[jagged_dense_add.py:N]: [starts[:, None] + tile1.index[None, :]],
        subscript = starts_copy_0[:, None]
        subscript_1 = indices_1[None, :]
        v_3 = tl.cast(subscript_1, tl.int64)
        v_4 = subscript + v_3
        # src[jagged_dense_add.py:N]: extra_mask=tile1.index[None, :] < nnz[:, None],
        subscript_2 = indices_1[None, :]
        subscript_3 = v_2_copy_0[:, None]
        v_5 = tl.cast(subscript_2, tl.int64)
        v_6 = v_5 < subscript_3
        # src[jagged_dense_add.py:N]: x_slice = hl.load(
        # src[jagged_dense_add.py:N]:     x_data,
        # src[jagged_dense_add.py:N]:     [starts[:, None] + tile1.index[None, :]],
        # src[jagged_dense_add.py:N-N]: ...
        x_slice = tl.load(x_data + v_4 * 1, mask_0[:, None] & mask_1[None, :] & v_6, other=0)
        # src[jagged_dense_add.py:N]: out[tile0, tile1] = y[tile0, tile1] + x_slice
        load_1 = tl.load(y + (indices_0[:, None] * 5000 + indices_1[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        v_7 = load_1 + x_slice
        tl.store(out + (indices_0[:, None] * 5000 + indices_1[None, :] * 1), v_7, mask_0[:, None] & mask_1[None, :])
    # src[jagged_dense_add.py:N]: for tile1 in hl.tile(max_nnz, out.size(1)):
    # src[jagged_dense_add.py:N]:     # fill in any leftover columns with y
    # src[jagged_dense_add.py:N]:     out[tile0, tile1] = y[tile0, tile1]
    for offset_2 in tl.range(max_nnz.to(tl.int32), 5000, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < 5000
        # src[jagged_dense_add.py:N]: out[tile0, tile1] = y[tile0, tile1]
        load = tl.load(y + (indices_0[:, None] * 5000 + indices_2[None, :] * 1), mask_0[:, None] & mask_2[None, :], other=0)
        tl.store(out + (indices_0[:, None] * 5000 + indices_2[None, :] * 1), load, mask_0[:, None] & mask_2[None, :])

def jagged_dense_add_2d(x_data: torch.Tensor, x_offsets: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add a jagged-prefix sparse tensor (x_data, x_offsets) to a dense matrix y
    and return the dense result.

    Args:
        x_data: 1-D tensor holding all non-zero elements row-by-row
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1]] (length K_i)
        y: (num_rows, N) tensor, N >= max(K_i)

    Returns:
        Dense tensor of shape (num_rows, N) containing the sum of the jagged and dense tensors
    """
    # src[jagged_dense_add.py:N]: num_rows = y.size(0)
    num_rows = y.size(0)
    # src[jagged_dense_add.py:N]: assert x_offsets.size(0) == num_rows + 1
    assert x_offsets.size(0) == num_rows + 1
    # src[jagged_dense_add.py:N]: out = torch.zeros_like(y)
    out = torch.zeros_like(y)
    # src[jagged_dense_add.py:N]: for tile0 in hl.tile(num_rows):
    _BLOCK_SIZE_0 = 16
    # src[jagged_dense_add.py:N]: for tile1 in hl.tile(0, max_nnz):
    # src[jagged_dense_add.py:N]:     x_slice = hl.load(
    # src[jagged_dense_add.py:N]:         x_data,
    # src[jagged_dense_add.py:N-N]: ...
    _BLOCK_SIZE_1 = 16
    # src[jagged_dense_add.py:N]: for tile1 in hl.tile(max_nnz, out.size(1)):
    # src[jagged_dense_add.py:N]:     # fill in any leftover columns with y
    # src[jagged_dense_add.py:N]:     out[tile0, tile1] = y[tile0, tile1]
    _BLOCK_SIZE_2 = 16
    # src[jagged_dense_add.py:N]: for tile0 in hl.tile(num_rows):
    # src[jagged_dense_add.py:N]:     starts = x_offsets[tile0]
    # src[jagged_dense_add.py:N]:     ends = x_offsets[tile0.index + 1]
    # src[jagged_dense_add.py:N-N]: ...
    _launcher(_helion_jagged_dense_add_2d, (triton.cdiv(500, _BLOCK_SIZE_0),), x_offsets, x_data, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[jagged_dense_add.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_jagged_hstu_attn)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion__helion_jagged_attention_kernel(seq_offsets, q, k, v, out, max_seq_len, alpha, scale, _BLOCK_SIZE_2: tl.constexpr, _RDIM_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr):
    # src[jagged_hstu_attn.py:N]: for tile_b, tile_h, tile_q in hl.tile(
    # src[jagged_hstu_attn.py:N]:     [num_batches, num_heads, max_seq_len], block_size=[1, 1, None]
    # src[jagged_hstu_attn.py:N]: ):
    num_blocks_0 = 4
    num_blocks_1 = 8
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0
    offset_1 = pid_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < max_seq_len
    indices_5 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    # src[jagged_hstu_attn.py:N]: starts = seq_offsets[tile_b.begin]
    starts = tl.load(seq_offsets + offset_0 * 1, None)
    # src[jagged_hstu_attn.py:N]: ends = seq_offsets[tile_b.begin + 1]
    add = 1 + offset_0
    ends = tl.load(seq_offsets + add * 1, None)
    # src[jagged_hstu_attn.py:N]: seq_len = ends - starts
    v_0 = ends - starts
    # src[jagged_hstu_attn.py:N]: if tile_q.begin < seq_len:
    v_1 = v_0 > offset_2
    # src[jagged_hstu_attn.py:N]: if tile_q.begin < seq_len:
    # src[jagged_hstu_attn.py:N]:     mask_q = tile_q.index < seq_len
    # src[jagged_hstu_attn.py:N]:     q_blk = q[tile_q.index + starts, tile_h.begin, :]
    # src[jagged_hstu_attn.py:N-N]: ...
    if v_1:
        v_0_copy = v_0
        starts_copy = starts
        v_0_copy_0 = v_0_copy
        starts_copy_0 = starts_copy
        # src[jagged_hstu_attn.py:N]: mask_q = tile_q.index < seq_len
        v_2 = v_0_copy_0[None]
        v_3 = tl.cast(v_2, tl.int32)
        v_4 = indices_2 < v_3
        # src[jagged_hstu_attn.py:N]: q_blk = q[tile_q.index + starts, tile_h.begin, :]
        v_5 = starts_copy_0[None]
        v_6 = tl.cast(v_5, tl.int32)
        v_7 = indices_2 + v_6
        q_blk = tl.load(q + (v_7[:, None] * 256 + offset_1 * 32 + indices_5[None, :] * 1), mask_2[:, None], other=0)
        # src[jagged_hstu_attn.py:N]: acc = hl.zeros([tile_q, dimV], dtype=torch.float32)
        acc = tl.full([_BLOCK_SIZE_2, 32], 0.0, tl.float32)
        # src[jagged_hstu_attn.py:N]: for tile_kv in hl.tile(0, tile_q.end, block_size=None):
        tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, max_seq_len)
        # src[jagged_hstu_attn.py:N]: for tile_kv in hl.tile(0, tile_q.end, block_size=None):
        # src[jagged_hstu_attn.py:N]:     mask_kv = tile_kv.index < seq_len
        # src[jagged_hstu_attn.py:N]:     k_blk = k[tile_kv.index + starts, tile_h.begin, :]
        # src[jagged_hstu_attn.py:N-N]: ...
        for offset_3 in tl.range(0, tile_end.to(tl.int32), _BLOCK_SIZE_4):
            indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
            mask_4 = indices_3 < tile_end
            v_0_copy_0_copy = v_0_copy_0
            starts_copy_0_copy = starts_copy_0
            q_blk_copy = q_blk
            v_4_copy = v_4
            acc_copy = acc
            v_0_copy_0_copy_0 = v_0_copy_0_copy
            starts_copy_0_copy_0 = starts_copy_0_copy
            q_blk_copy_0 = q_blk_copy
            v_4_copy_0 = v_4_copy
            acc_copy_0 = acc_copy
            # src[jagged_hstu_attn.py:N]: mask_kv = tile_kv.index < seq_len
            v_8 = v_0_copy_0_copy_0[None]
            v_9 = tl.cast(v_8, tl.int32)
            v_10 = indices_3 < v_9
            # src[jagged_hstu_attn.py:N]: k_blk = k[tile_kv.index + starts, tile_h.begin, :]
            v_11 = starts_copy_0_copy_0[None]
            v_12 = tl.cast(v_11, tl.int32)
            v_13 = indices_3 + v_12
            k_blk = tl.load(k + (v_13[:, None] * 256 + offset_1 * 32 + indices_5[None, :] * 1), mask_4[:, None], other=0)
            # src[jagged_hstu_attn.py:N]: v_blk = v[tile_kv.index + starts, tile_h.begin, :]
            v_14 = starts_copy_0_copy_0[None]
            v_15 = tl.cast(v_14, tl.int32)
            v_16 = indices_3 + v_15
            v_blk = tl.load(v + (v_16[:, None] * 256 + offset_1 * 32 + indices_5[None, :] * 1), mask_4[:, None], other=0)
            # src[jagged_hstu_attn.py:N]: torch.nn.functional.silu(torch.matmul(q_blk, k_blk.T) * alpha)
            permute = tl.permute(k_blk, [1, 0])
            mm = tl.cast(tl.dot(tl.cast(q_blk_copy_0, tl.bfloat16), tl.cast(permute, tl.bfloat16), input_precision='tf32', out_dtype=tl.float32), tl.bfloat16)
            v_17 = tl.cast(alpha, tl.bfloat16)
            v_18 = mm * v_17
            v_19 = tl.cast(v_18, tl.float32)
            v_20 = tl.sigmoid(tl.cast(v_19, tl.float32))
            v_21 = v_19 * v_20
            v_22 = tl.cast(v_21, tl.bfloat16)
            # src[jagged_hstu_attn.py:N]: torch.nn.functional.silu(torch.matmul(q_blk, k_blk.T) * alpha)
            # src[jagged_hstu_attn.py:N]: * scale
            v_23 = tl.cast(scale, tl.bfloat16)
            v_24 = v_22 * v_23
            # src[jagged_hstu_attn.py:N]: (tile_q.index.unsqueeze(1) > tile_kv.index.unsqueeze(0))
            unsqueeze = indices_2[:, None]
            unsqueeze_1 = indices_3[None, :]
            v_25 = unsqueeze > unsqueeze_1
            # src[jagged_hstu_attn.py:N]: & mask_q[:, None]
            subscript = v_4_copy_0[:, None]
            # src[jagged_hstu_attn.py:N]: (tile_q.index.unsqueeze(1) > tile_kv.index.unsqueeze(0))
            # src[jagged_hstu_attn.py:N]: & mask_q[:, None]
            v_26 = v_25 & subscript
            # src[jagged_hstu_attn.py:N]: & mask_kv[None, :],
            subscript_1 = v_10[None, :]
            # src[jagged_hstu_attn.py:N]: (tile_q.index.unsqueeze(1) > tile_kv.index.unsqueeze(0))
            # src[jagged_hstu_attn.py:N]: & mask_q[:, None]
            # src[jagged_hstu_attn.py:N]: & mask_kv[None, :],
            v_27 = v_26 & subscript_1
            # src[jagged_hstu_attn.py:N]: scores = torch.where(
            # src[jagged_hstu_attn.py:N]:     (tile_q.index.unsqueeze(1) > tile_kv.index.unsqueeze(0))
            # src[jagged_hstu_attn.py:N]:     & mask_q[:, None]
            # src[jagged_hstu_attn.py:N-N]: ...
            v_28 = 0.0
            v_29 = v_28[None, None]
            v_30 = tl.where(v_27, v_24, v_29)
            # src[jagged_hstu_attn.py:N]: acc += torch.matmul(scores.to(v.dtype), v_blk)
            _mask_to_2 = tl.where(mask_2[:, None] & mask_4[None, :], v_30, tl.full([], 0, tl.bfloat16))
            mm_1 = tl.cast(tl.dot(tl.cast(_mask_to_2, tl.bfloat16), tl.cast(v_blk, tl.bfloat16), input_precision='tf32', out_dtype=tl.float32), tl.bfloat16)
            v_31 = tl.cast(mm_1, tl.float32)
            acc = acc_copy_0 + v_31
        # src[jagged_hstu_attn.py:N]: out[tile_q.index + starts, tile_h.begin, :] = acc.to(out.dtype)
        v_33 = tl.cast(acc, tl.bfloat16)
        v_34 = starts_copy_0[None]
        v_35 = tl.cast(v_34, tl.int32)
        v_36 = indices_2 + v_35
        tl.store(out + (v_36[:, None] * 256 + offset_1 * 32 + indices_5[None, :] * 1), v_33, mask_2[:, None])

def _helion_jagged_attention_kernel(max_seq_len: int, alpha: float, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seq_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """Helion implementation of HSTU jagged attention"""
    # src[jagged_hstu_attn.py:N]: scale = 1.0 / max_seq_len
    scale = 1.0 / max_seq_len
    # src[jagged_hstu_attn.py:N]: out = torch.zeros_like(v)
    out = torch.zeros_like(v)
    # src[jagged_hstu_attn.py:N]: for tile_b, tile_h, tile_q in hl.tile(
    # src[jagged_hstu_attn.py:N]:     [num_batches, num_heads, max_seq_len], block_size=[1, 1, None]
    # src[jagged_hstu_attn.py:N]: ):
    _BLOCK_SIZE_2 = 16
    _RDIM_SIZE_3 = 32
    # src[jagged_hstu_attn.py:N]: for tile_kv in hl.tile(0, tile_q.end, block_size=None):
    # src[jagged_hstu_attn.py:N]:     mask_kv = tile_kv.index < seq_len
    # src[jagged_hstu_attn.py:N]:     k_blk = k[tile_kv.index + starts, tile_h.begin, :]
    # src[jagged_hstu_attn.py:N-N]: ...
    _BLOCK_SIZE_4 = 16
    # src[jagged_hstu_attn.py:N]: for tile_b, tile_h, tile_q in hl.tile(
    # src[jagged_hstu_attn.py:N]:     [num_batches, num_heads, max_seq_len], block_size=[1, 1, None]
    # src[jagged_hstu_attn.py:N]: ):
    # src[jagged_hstu_attn.py:N-N]: ...
    _launcher(_helion__helion_jagged_attention_kernel, (4 * 8 * triton.cdiv(max_seq_len, _BLOCK_SIZE_2),), seq_offsets, q, k, v, out, max_seq_len, alpha, scale, _BLOCK_SIZE_2, _RDIM_SIZE_3, _BLOCK_SIZE_4, num_warps=4, num_stages=1)
    # src[jagged_hstu_attn.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_jagged_layer_norm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_layer_norm_kernel(x_offsets, x_flat, out_flat, eps, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr, _BLOCK_SIZE_5: tl.constexpr, _BLOCK_SIZE_6: tl.constexpr):
    # src[jagged_layer_norm.py:N]: for tile_b in hl.tile(B):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[jagged_layer_norm.py:N]: starts = x_offsets[tile_b]
    starts = tl.load(x_offsets + indices_0 * 1, None)
    # src[jagged_layer_norm.py:N]: ends = x_offsets[tile_b.index + 1]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + (indices_0 + 1) * 1, None)
    # src[jagged_layer_norm.py:N]: seq_lengths = ends - starts
    v_2 = ends - starts
    # src[jagged_layer_norm.py:N]: max_seq_len = seq_lengths.amax()
    max_seq_len = tl.cast(tl.max(v_2, 0), tl.int64)
    # src[jagged_layer_norm.py:N]: mean_acc = hl.zeros([tile_b], dtype=x_values.dtype)
    mean_acc = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[jagged_layer_norm.py:N]: var_acc = hl.zeros([tile_b], dtype=x_values.dtype)
    var_acc = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     row_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N-N]: ...
    for offset_1 in tl.range(0, 8, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        max_seq_len_copy = max_seq_len
        starts_copy = starts
        v_2_copy = v_2
        mean_acc_copy = mean_acc
        max_seq_len_copy_0 = max_seq_len_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        mean_acc_copy_0 = mean_acc_copy
        # src[jagged_layer_norm.py:N]: row_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
        # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
        # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_layer_norm.py:N-N]: ...
        for offset_2 in tl.range(0, max_seq_len_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_seq_len_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            row_sums_copy_0 = row_sums_copy
            # src[jagged_layer_norm.py:N]: indices = starts[:, None] + tile_k.index[None, :]
            subscript = starts_copy_0_copy_0[:, None]
            subscript_1 = indices_2[None, :]
            v_3 = tl.cast(subscript_1, tl.int64)
            v_4 = subscript + v_3
            # src[jagged_layer_norm.py:N]: flat_indices = indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_2 = v_4[:, :, None]
            v_5 = tl.full([], 8, tl.int64)
            v_6 = tl.cast(subscript_2 * v_5, tl.int64)
            subscript_3 = indices_1[None, None, :]
            v_7 = tl.cast(subscript_3, tl.int64)
            v_8 = v_6 + v_7
            # src[jagged_layer_norm.py:N]: row_mask = tile_k.index[None, :] < seq_lengths[:, None]
            subscript_4 = indices_2[None, :]
            subscript_5 = v_2_copy_0_copy_0[:, None]
            v_9 = tl.cast(subscript_4, tl.int64)
            v_10 = v_9 < subscript_5
            # src[jagged_layer_norm.py:N]: combined_mask = row_mask[:, :, None]
            combined_mask = v_10[:, :, None]
            # src[jagged_layer_norm.py:N]: x_slice = hl.load(
            # src[jagged_layer_norm.py:N]:     x_flat,
            # src[jagged_layer_norm.py:N]:     [flat_indices],
            # src[jagged_layer_norm.py:N-N]: ...
            x_slice = tl.load(x_flat + v_8 * 1, mask_2[None, :, None] & combined_mask, other=0)
            # src[jagged_layer_norm.py:N]: row_sums = row_sums + x_slice.sum(dim=1)
            sum_1 = tl.cast(tl.sum(x_slice, 1), tl.float32)
            row_sums = row_sums_copy_0 + sum_1
        # src[jagged_layer_norm.py:N]: mean_acc = mean_acc + row_sums.sum(dim=1)
        sum_2 = tl.cast(tl.sum(row_sums, 1), tl.float32)
        mean_acc = mean_acc_copy_0 + sum_2
    # src[jagged_layer_norm.py:N]: seq_lengths_float = seq_lengths.to(x_values.dtype)
    v_13 = tl.cast(v_2, tl.float32)
    # src[jagged_layer_norm.py:N]: mean_acc = mean_acc / (seq_lengths_float * M)
    v_14 = 8.0
    v_15 = v_13 * v_14
    v_16 = mean_acc / v_15
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     var_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N-N]: ...
    for offset_3 in tl.range(0, 8, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        max_seq_len_copy_1 = max_seq_len
        starts_copy_1 = starts
        v_2_copy_1 = v_2
        v_16_copy = v_16
        var_acc_copy = var_acc
        max_seq_len_copy_1_0 = max_seq_len_copy_1
        starts_copy_1_0 = starts_copy_1
        v_2_copy_1_0 = v_2_copy_1
        v_16_copy_0 = v_16_copy
        var_acc_copy_0 = var_acc_copy
        # src[jagged_layer_norm.py:N]: var_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
        var_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_3], 0.0, tl.float32)
        # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
        # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
        # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_layer_norm.py:N-N]: ...
        for offset_4 in tl.range(0, max_seq_len_copy_1_0.to(tl.int32), _BLOCK_SIZE_4):
            indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
            mask_4 = indices_4 < max_seq_len_copy_1_0
            starts_copy_1_0_copy = starts_copy_1_0
            v_2_copy_1_0_copy = v_2_copy_1_0
            v_16_copy_0_copy = v_16_copy_0
            var_sums_copy = var_sums
            starts_copy_1_0_copy_0 = starts_copy_1_0_copy
            v_2_copy_1_0_copy_0 = v_2_copy_1_0_copy
            v_16_copy_0_copy_0 = v_16_copy_0_copy
            var_sums_copy_0 = var_sums_copy
            # src[jagged_layer_norm.py:N]: indices = starts[:, None] + tile_k.index[None, :]
            subscript_6 = starts_copy_1_0_copy_0[:, None]
            subscript_7 = indices_4[None, :]
            v_17 = tl.cast(subscript_7, tl.int64)
            v_18 = subscript_6 + v_17
            # src[jagged_layer_norm.py:N]: flat_indices = indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_8 = v_18[:, :, None]
            v_19 = tl.full([], 8, tl.int64)
            v_20 = tl.cast(subscript_8 * v_19, tl.int64)
            subscript_9 = indices_3[None, None, :]
            v_21 = tl.cast(subscript_9, tl.int64)
            v_22 = v_20 + v_21
            # src[jagged_layer_norm.py:N]: row_mask = tile_k.index[None, :] < seq_lengths[:, None]
            subscript_10 = indices_4[None, :]
            subscript_11 = v_2_copy_1_0_copy_0[:, None]
            v_23 = tl.cast(subscript_10, tl.int64)
            v_24 = v_23 < subscript_11
            # src[jagged_layer_norm.py:N]: combined_mask = row_mask[:, :, None]
            combined_mask_1 = v_24[:, :, None]
            # src[jagged_layer_norm.py:N]: x_slice = hl.load(
            # src[jagged_layer_norm.py:N]:     x_flat,
            # src[jagged_layer_norm.py:N]:     [flat_indices],
            # src[jagged_layer_norm.py:N-N]: ...
            x_slice_1 = tl.load(x_flat + v_22 * 1, mask_4[None, :, None] & combined_mask_1, other=0)
            # src[jagged_layer_norm.py:N]: x_slice.to(torch.float32) - mean_acc[:, None, None],
            subscript_12 = v_16_copy_0_copy_0[:, None, None]
            v_25 = x_slice_1 - subscript_12
            # src[jagged_layer_norm.py:N]: centered = torch.where(
            # src[jagged_layer_norm.py:N]:     combined_mask,
            # src[jagged_layer_norm.py:N]:     x_slice.to(torch.float32) - mean_acc[:, None, None],
            # src[jagged_layer_norm.py:N-N]: ...
            v_26 = 0.0
            v_27 = v_26[None, None, None]
            v_28 = tl.where(combined_mask_1, v_25, v_27)
            # src[jagged_layer_norm.py:N]: var_sums = var_sums + (centered * centered).sum(dim=1)
            v_29 = v_28 * v_28
            _mask_to = tl.where(tl.broadcast_to(mask_4[None, :, None], [_BLOCK_SIZE_0, _BLOCK_SIZE_4, _BLOCK_SIZE_3]), v_29, tl.full([], 0, tl.float32))
            sum_3 = tl.cast(tl.sum(_mask_to, 1), tl.float32)
            var_sums = var_sums_copy_0 + sum_3
        # src[jagged_layer_norm.py:N]: var_acc = var_acc + var_sums.sum(dim=1)
        sum_4 = tl.cast(tl.sum(var_sums, 1), tl.float32)
        var_acc = var_acc_copy_0 + sum_4
    # src[jagged_layer_norm.py:N]: variance = var_acc / (seq_lengths_float * M)
    v_32 = 8.0
    v_33 = v_13 * v_32
    v_34 = var_acc / v_33
    # src[jagged_layer_norm.py:N]: rstd = torch.rsqrt(variance + eps)
    v_35 = v_34 + eps
    v_36 = tl.rsqrt(v_35)
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N]:         # Compute indices into x_values
    # src[jagged_layer_norm.py:N-N]: ...
    for offset_5 in tl.range(0, 8, _BLOCK_SIZE_5):
        indices_5 = offset_5 + tl.arange(0, _BLOCK_SIZE_5).to(tl.int32)
        max_seq_len_copy_2 = max_seq_len
        starts_copy_2 = starts
        v_2_copy_2 = v_2
        v_16_copy_1 = v_16
        v_36_copy = v_36
        max_seq_len_copy_2_0 = max_seq_len_copy_2
        starts_copy_2_0 = starts_copy_2
        v_2_copy_2_0 = v_2_copy_2
        v_16_copy_1_0 = v_16_copy_1
        v_36_copy_0 = v_36_copy
        # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
        # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
        # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_layer_norm.py:N-N]: ...
        for offset_6 in tl.range(0, max_seq_len_copy_2_0.to(tl.int32), _BLOCK_SIZE_6):
            indices_6 = offset_6 + tl.arange(0, _BLOCK_SIZE_6).to(tl.int32)
            mask_6 = indices_6 < max_seq_len_copy_2_0
            starts_copy_2_0_copy = starts_copy_2_0
            v_2_copy_2_0_copy = v_2_copy_2_0
            v_16_copy_1_0_copy = v_16_copy_1_0
            v_36_copy_0_copy = v_36_copy_0
            starts_copy_2_0_copy_0 = starts_copy_2_0_copy
            v_2_copy_2_0_copy_0 = v_2_copy_2_0_copy
            v_16_copy_1_0_copy_0 = v_16_copy_1_0_copy
            v_36_copy_0_copy_0 = v_36_copy_0_copy
            # src[jagged_layer_norm.py:N]: indices = starts[:, None] + tile_k.index[None, :]
            subscript_13 = starts_copy_2_0_copy_0[:, None]
            subscript_14 = indices_6[None, :]
            v_37 = tl.cast(subscript_14, tl.int64)
            v_38 = subscript_13 + v_37
            # src[jagged_layer_norm.py:N]: flat_indices = indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_15 = v_38[:, :, None]
            v_39 = tl.full([], 8, tl.int64)
            v_40 = tl.cast(subscript_15 * v_39, tl.int64)
            subscript_16 = indices_5[None, None, :]
            v_41 = tl.cast(subscript_16, tl.int64)
            v_42 = v_40 + v_41
            # src[jagged_layer_norm.py:N]: row_mask = tile_k.index[None, :] < seq_lengths[:, None]
            subscript_17 = indices_6[None, :]
            subscript_18 = v_2_copy_2_0_copy_0[:, None]
            v_43 = tl.cast(subscript_17, tl.int64)
            v_44 = v_43 < subscript_18
            # src[jagged_layer_norm.py:N]: combined_mask = row_mask[:, :, None]
            combined_mask_2 = v_44[:, :, None]
            # src[jagged_layer_norm.py:N]: x_slice = hl.load(
            # src[jagged_layer_norm.py:N]:     x_flat,
            # src[jagged_layer_norm.py:N]:     [flat_indices],
            # src[jagged_layer_norm.py:N-N]: ...
            x_slice_2 = tl.load(x_flat + v_42 * 1, mask_6[None, :, None] & combined_mask_2, other=0)
            # src[jagged_layer_norm.py:N]: (x_slice.to(torch.float32) - mean_acc[:, None, None])
            subscript_19 = v_16_copy_1_0_copy_0[:, None, None]
            v_45 = x_slice_2 - subscript_19
            # src[jagged_layer_norm.py:N]: * rstd[:, None, None],
            subscript_20 = v_36_copy_0_copy_0[:, None, None]
            # src[jagged_layer_norm.py:N]: (x_slice.to(torch.float32) - mean_acc[:, None, None])
            # src[jagged_layer_norm.py:N]: * rstd[:, None, None],
            v_46 = v_45 * subscript_20
            # src[jagged_layer_norm.py:N]: normalized = torch.where(
            # src[jagged_layer_norm.py:N]:     combined_mask,
            # src[jagged_layer_norm.py:N]:     (x_slice.to(torch.float32) - mean_acc[:, None, None])
            # src[jagged_layer_norm.py:N-N]: ...
            v_47 = 0.0
            v_48 = v_47[None, None, None]
            v_49 = tl.where(combined_mask_2, v_46, v_48)
            # src[jagged_layer_norm.py:N]: hl.store(
            # src[jagged_layer_norm.py:N]:     out_flat,
            # src[jagged_layer_norm.py:N]:     [flat_indices],
            # src[jagged_layer_norm.py:N-N]: ...
            tl.store(out_flat + v_42 * 1, v_49, mask_6[None, :, None] & combined_mask_2)

def jagged_layer_norm_kernel(x_values: torch.Tensor, x_offsets: torch.Tensor, eps: float=1e-06, *, _launcher=_default_launcher):
    """
    Compute layer normalization on jagged tensor using Helion.

    This kernel implements layer normalization for jagged tensors by:
    1. Computing mean and variance for each sequence individually
    2. Normalizing values within each sequence
    3. Applying optional affine transformation (weight/bias)

    Args:
        x_values: Compressed values tensor of shape [total_L, M]
        x_offsets: Sequence boundary offsets of shape [B+1]
        eps: Small value for numerical stability

    Returns:
        Normalized tensor of same shape as x_values [total_L, M]
    """
    # src[jagged_layer_norm.py:N]: total_L, M = x_values.shape
    total_L, M = x_values.shape
    # src[jagged_layer_norm.py:N]: B = x_offsets.size(0) - 1
    B = x_offsets.size(0) - 1
    # src[jagged_layer_norm.py:N]: out = torch.empty_like(x_values)
    out = torch.empty_like(x_values)
    # src[jagged_layer_norm.py:N]: x_flat = x_values.view(-1)
    x_flat = x_values.view(-1)
    # src[jagged_layer_norm.py:N]: out_flat = out.view(-1)
    out_flat = out.view(-1)
    # src[jagged_layer_norm.py:N]: for tile_b in hl.tile(B):
    _BLOCK_SIZE_0 = 4
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     row_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_1 = 8
    # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
    # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_2 = 8
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     var_sums = hl.zeros([tile_b, tile_m], dtype=x_values.dtype)
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_3 = 8
    # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
    # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_4 = 8
    # src[jagged_layer_norm.py:N]: for tile_m in hl.tile(M):
    # src[jagged_layer_norm.py:N]:     for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N]:         # Compute indices into x_values
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_5 = 8
    # src[jagged_layer_norm.py:N]: for tile_k in hl.tile(0, max_seq_len):
    # src[jagged_layer_norm.py:N]:     # Compute indices into x_values
    # src[jagged_layer_norm.py:N]:     indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_layer_norm.py:N-N]: ...
    _BLOCK_SIZE_6 = 8
    # src[jagged_layer_norm.py:N]: for tile_b in hl.tile(B):
    # src[jagged_layer_norm.py:N]:     # Get sequence boundaries for this tile
    # src[jagged_layer_norm.py:N]:     starts = x_offsets[tile_b]
    # src[jagged_layer_norm.py:N-N]: ...
    _launcher(_helion_jagged_layer_norm_kernel, (triton.cdiv(128, _BLOCK_SIZE_0),), x_offsets, x_flat, out_flat, eps, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, _BLOCK_SIZE_4, _BLOCK_SIZE_5, _BLOCK_SIZE_6, num_warps=4, num_stages=1)
    # src[jagged_layer_norm.py:N]: return out.reshape(total_L, M)
    return out.reshape(total_L, M)

--- assertExpectedJournal(TestExamples.test_jagged_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_mean_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[jagged_mean.py:N]: for tile_b in hl.tile(num_rows):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[jagged_mean.py:N]: starts = x_offsets[tile_b]
    starts = tl.load(x_offsets + indices_0 * 1, None)
    # src[jagged_mean.py:N]: ends = x_offsets[tile_b.index + 1]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + (indices_0 + 1) * 1, None)
    # src[jagged_mean.py:N]: nnz = ends - starts
    v_2 = ends - starts
    # src[jagged_mean.py:N]: max_nnz = nnz.amax()
    max_nnz = tl.cast(tl.max(v_2, 0), tl.int64)
    # src[jagged_mean.py:N]: feature_counts = x_feature_counts[tile_b]
    feature_counts = tl.load(x_feature_counts + indices_0 * 1, None)
    # src[jagged_mean.py:N]: for tile_m in hl.tile(max_M):
    # src[jagged_mean.py:N]:     # Create mask for valid features
    # src[jagged_mean.py:N]:     feature_valid = tile_m.index < feature_counts[:, None]
    # src[jagged_mean.py:N-N]: ...
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        # src[jagged_mean.py:N]: feature_valid = tile_m.index < feature_counts[:, None]
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        # src[jagged_mean.py:N]: row_sums = hl.zeros([tile_b, tile_m], dtype=x_data.dtype)
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_mean.py:N]: for tile_k in hl.tile(0, max_nnz):
        # src[jagged_mean.py:N]:     # Compute flattened indices
        # src[jagged_mean.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_mean.py:N-N]: ...
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            # src[jagged_mean.py:N]: base_indices = starts[:, None] + tile_k.index[None, :]
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = tl.cast(subscript_2, tl.int64)
            v_6 = subscript_1 + v_5
            # src[jagged_mean.py:N]: base_indices[:, :, None] * max_M + tile_m.index[None, None, :]
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = tl.cast(subscript_4, tl.int64)
            v_9 = v_7 + v_8
            # src[jagged_mean.py:N]: row_mask = tile_k.index[None, :] < nnz[:, None]
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = tl.cast(subscript_5, tl.int64)
            v_11 = v_10 < subscript_6
            # src[jagged_mean.py:N]: combined_mask = row_mask[:, :, None] & feature_valid[:, None, :]
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            # src[jagged_mean.py:N]: x_slice = hl.load(
            # src[jagged_mean.py:N]:     x_flat,
            # src[jagged_mean.py:N]:     [flat_indices],
            # src[jagged_mean.py:N-N]: ...
            x_slice = tl.load(x_flat + v_9 * 1, mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            # src[jagged_mean.py:N]: row_sums = row_sums + x_slice.sum(dim=1)
            sum_1 = tl.cast(tl.sum(x_slice, 1), tl.float32)
            row_sums = row_sums_copy_0 + sum_1
        # src[jagged_mean.py:N]: nnz_float = nnz.to(x_data.dtype)
        v_14 = tl.cast(v_2_copy_0, tl.float32)
        # src[jagged_mean.py:N]: nnz_expanded = nnz_float[:, None]
        nnz_expanded = v_14[:, None]
        # src[jagged_mean.py:N]: result = torch.where(nnz_expanded > 0, row_sums / nnz_expanded, 0.0)
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        # src[jagged_mean.py:N]: out[tile_b, tile_m] = torch.where(feature_valid, result, 0.0)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * 1), v_23, mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M: int, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args:
        x_data: 2-D tensor of shape (total_elements, max_M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]
        x_feature_counts: (num_rows) tensor. Number of valid features for each row
        max_M: Maximum number of features

    Returns:
        2-D tensor of shape (num_rows, max_M) containing the mean of each row.
        Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    # src[jagged_mean.py:N]: num_rows = x_offsets.size(0) - 1
    num_rows = x_offsets.size(0) - 1
    # src[jagged_mean.py:N]: out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    # src[jagged_mean.py:N]: x_flat = x_data.view(-1)
    x_flat = x_data.view(-1)
    # src[jagged_mean.py:N]: for tile_b in hl.tile(num_rows):
    _BLOCK_SIZE_0 = 16
    # src[jagged_mean.py:N]: for tile_m in hl.tile(max_M):
    # src[jagged_mean.py:N]:     # Create mask for valid features
    # src[jagged_mean.py:N]:     feature_valid = tile_m.index < feature_counts[:, None]
    # src[jagged_mean.py:N-N]: ...
    _BLOCK_SIZE_1 = 8
    # src[jagged_mean.py:N]: for tile_k in hl.tile(0, max_nnz):
    # src[jagged_mean.py:N]:     # Compute flattened indices
    # src[jagged_mean.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_mean.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[jagged_mean.py:N]: for tile_b in hl.tile(num_rows):
    # src[jagged_mean.py:N]:     starts = x_offsets[tile_b]
    # src[jagged_mean.py:N]:     ends = x_offsets[tile_b.index + 1]
    # src[jagged_mean.py:N-N]: ...
    _launcher(_helion_jagged_mean_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[jagged_mean.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_jagged_softmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_softmax_kernel(x_offsets, x_flat, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[jagged_softmax.py:N]: for tile_b in hl.tile(num_rows):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[jagged_softmax.py:N]: starts = x_offsets[tile_b]
    starts = tl.load(x_offsets + indices_0 * 1, None)
    # src[jagged_softmax.py:N]: ends = x_offsets[tile_b.index + 1]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + (indices_0 + 1) * 1, None)
    # src[jagged_softmax.py:N]: seqlens = ends - starts
    v_2 = ends - starts
    # src[jagged_softmax.py:N]: max_seqlen = seqlens.amax()
    max_seqlen = tl.cast(tl.max(v_2, 0), tl.int64)
    # src[jagged_softmax.py:N]: for tile_m in hl.tile(M):
    # src[jagged_softmax.py:N]:     block_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
    # src[jagged_softmax.py:N]:     block_new_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
    # src[jagged_softmax.py:N-N]: ...
    for offset_1 in tl.range(0, 8, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        max_seqlen_copy = max_seqlen
        starts_copy = starts
        v_2_copy = v_2
        max_seqlen_copy_0 = max_seqlen_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        # src[jagged_softmax.py:N]: block_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
        block_max = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_softmax.py:N]: block_new_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
        block_new_max = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_softmax.py:N]: block_L = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
        block_L = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_softmax.py:N]: for tile_k in hl.tile(max_seqlen):
        # src[jagged_softmax.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_softmax.py:N]:     flat_indices = (
        # src[jagged_softmax.py:N-N]: ...
        for offset_2 in tl.range(0, max_seqlen_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_seqlen_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            block_max_copy = block_max
            block_L_copy = block_L
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            block_max_copy_0 = block_max_copy
            block_L_copy_0 = block_L_copy
            # src[jagged_softmax.py:N]: base_indices = starts[:, None] + tile_k.index[None, :]
            subscript = starts_copy_0_copy_0[:, None]
            subscript_1 = indices_2[None, :]
            v_3 = tl.cast(subscript_1, tl.int64)
            v_4 = subscript + v_3
            # src[jagged_softmax.py:N]: base_indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_2 = v_4[:, :, None]
            v_5 = tl.full([], 8, tl.int64)
            v_6 = tl.cast(subscript_2 * v_5, tl.int64)
            subscript_3 = indices_1[None, None, :]
            v_7 = tl.cast(subscript_3, tl.int64)
            v_8 = v_6 + v_7
            # src[jagged_softmax.py:N]: row_mask = tile_k.index[None, :] < seqlens[:, None]
            subscript_4 = indices_2[None, :]
            subscript_5 = v_2_copy_0_copy_0[:, None]
            v_9 = tl.cast(subscript_4, tl.int64)
            v_10 = v_9 < subscript_5
            # src[jagged_softmax.py:N]: combined_mask = row_mask[:, :, None] & (tile_m.index < M)[None, None, :]
            subscript_6 = v_10[:, :, None]
            v_11 = tl.full([], 8, tl.int32)
            v_12 = indices_1 < v_11
            subscript_7 = v_12[None, None, :]
            v_13 = subscript_6 & subscript_7
            # src[jagged_softmax.py:N]: x_slice = hl.load(
            # src[jagged_softmax.py:N]:     x_flat,
            # src[jagged_softmax.py:N]:     [flat_indices],
            # src[jagged_softmax.py:N-N]: ...
            x_slice = tl.load(x_flat + v_8 * 1, mask_2[None, :, None] & v_13, other=0)
            # src[jagged_softmax.py:N]: slice_max = torch.where(combined_mask, x_slice, float("-inf")).amax(
            v_14 = float('-inf')
            v_15 = v_14[None, None, None]
            v_16 = tl.where(v_13, x_slice, v_15)
            # src[jagged_softmax.py:N]: slice_max = torch.where(combined_mask, x_slice, float("-inf")).amax(
            # src[jagged_softmax.py:N]:     dim=1
            # src[jagged_softmax.py:N]: )
            _mask_to = tl.where(tl.broadcast_to(mask_2[None, :, None], [_BLOCK_SIZE_0, _BLOCK_SIZE_2, _BLOCK_SIZE_1]), v_16, tl.full([], float('-inf'), tl.float32))
            slice_max = tl.cast(tl.max(_mask_to, 1), tl.float32)
            # src[jagged_softmax.py:N]: block_new_max = torch.maximum(block_max, slice_max)
            block_new_max = triton_helpers.maximum(block_max_copy_0, slice_max)
            # src[jagged_softmax.py:N]: block_L *= torch.exp(block_max - block_new_max)
            v_18 = block_max_copy_0 - block_new_max
            v_19 = libdevice.exp(v_18)
            v_20 = block_L_copy_0 * v_19
            # src[jagged_softmax.py:N]: x_slice - block_new_max[:, None, :],
            subscript_8 = block_new_max[:, None, :]
            v_21 = x_slice - subscript_8
            # src[jagged_softmax.py:N]: torch.where(
            # src[jagged_softmax.py:N]:     combined_mask,
            # src[jagged_softmax.py:N]:     x_slice - block_new_max[:, None, :],
            # src[jagged_softmax.py:N-N]: ...
            v_22 = float('-inf')
            v_23 = v_22[None, None, None]
            v_24 = tl.where(v_13, v_21, v_23)
            # src[jagged_softmax.py:N]: block_L += torch.exp(
            # src[jagged_softmax.py:N]:     torch.where(
            # src[jagged_softmax.py:N]:         combined_mask,
            # src[jagged_softmax.py:N-N]: ...
            v_25 = libdevice.exp(v_24)
            _mask_to_1 = tl.where(tl.broadcast_to(mask_2[None, :, None], [_BLOCK_SIZE_0, _BLOCK_SIZE_2, _BLOCK_SIZE_1]), v_25, tl.full([], 0, tl.float32))
            sum_1 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
            block_L = v_20 + sum_1
            # src[jagged_softmax.py:N]: block_max = block_new_max
            block_max = block_new_max
        # src[jagged_softmax.py:N]: for tile_k in hl.tile(max_seqlen):
        # src[jagged_softmax.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_softmax.py:N]:     flat_indices = (
        # src[jagged_softmax.py:N-N]: ...
        for offset_3 in tl.range(0, max_seqlen_copy_0.to(tl.int32), _BLOCK_SIZE_3):
            indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            mask_3 = indices_3 < max_seqlen_copy_0
            starts_copy_0_copy_1 = starts_copy_0
            v_2_copy_0_copy_1 = v_2_copy_0
            block_max_copy_1 = block_max
            block_L_copy_1 = block_L
            starts_copy_0_copy_1_0 = starts_copy_0_copy_1
            v_2_copy_0_copy_1_0 = v_2_copy_0_copy_1
            block_max_copy_1_0 = block_max_copy_1
            block_L_copy_1_0 = block_L_copy_1
            # src[jagged_softmax.py:N]: base_indices = starts[:, None] + tile_k.index[None, :]
            subscript_9 = starts_copy_0_copy_1_0[:, None]
            subscript_10 = indices_3[None, :]
            v_27 = tl.cast(subscript_10, tl.int64)
            v_28 = subscript_9 + v_27
            # src[jagged_softmax.py:N]: base_indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_11 = v_28[:, :, None]
            v_29 = tl.full([], 8, tl.int64)
            v_30 = tl.cast(subscript_11 * v_29, tl.int64)
            subscript_12 = indices_1[None, None, :]
            v_31 = tl.cast(subscript_12, tl.int64)
            v_32 = v_30 + v_31
            # src[jagged_softmax.py:N]: row_mask = tile_k.index[None, :] < seqlens[:, None]
            subscript_13 = indices_3[None, :]
            subscript_14 = v_2_copy_0_copy_1_0[:, None]
            v_33 = tl.cast(subscript_13, tl.int64)
            v_34 = v_33 < subscript_14
            # src[jagged_softmax.py:N]: combined_mask = row_mask[:, :, None] & (tile_m.index < M)[None, None, :]
            subscript_15 = v_34[:, :, None]
            v_35 = tl.full([], 8, tl.int32)
            v_36 = indices_1 < v_35
            subscript_16 = v_36[None, None, :]
            v_37 = subscript_15 & subscript_16
            # src[jagged_softmax.py:N]: x_slice = hl.load(
            # src[jagged_softmax.py:N]:     x_flat,
            # src[jagged_softmax.py:N]:     [flat_indices],
            # src[jagged_softmax.py:N-N]: ...
            x_slice_1 = tl.load(x_flat + v_32 * 1, mask_3[None, :, None] & v_37, other=0)
            # src[jagged_softmax.py:N]: torch.exp(x_slice - block_max[:, None, :]) / block_L[:, None, :]
            subscript_17 = block_max_copy_1_0[:, None, :]
            v_38 = x_slice_1 - subscript_17
            v_39 = libdevice.exp(v_38)
            subscript_18 = block_L_copy_1_0[:, None, :]
            v_40 = v_39 / subscript_18
            # src[jagged_softmax.py:N]: hl.store(
            # src[jagged_softmax.py:N]:     out,
            # src[jagged_softmax.py:N]:     [flat_indices],
            # src[jagged_softmax.py:N-N]: ...
            tl.store(out + v_32 * 1, v_40, mask_3[None, :, None] & v_37)

def jagged_softmax_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the per-batch softmax in a jagged tensor.

    Args:
        x_data: 2-D tensor of shape (total_elements, max_M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]

    Returns:
        2-D tensor of shape (total_elements, max_M), containing the per-batch softmax scores.
    """
    # src[jagged_softmax.py:N]: N = int(x_offsets[-1].item())
    N = int(x_offsets[-1].item())
    # src[jagged_softmax.py:N]: num_rows, M = x_offsets.size(0) - 1, x_data.size(1)
    num_rows, M = (x_offsets.size(0) - 1, x_data.size(1))
    # src[jagged_softmax.py:N]: out = torch.zeros(N * M, dtype=x_data.dtype, device=x_data.device)
    out = torch.zeros(N * M, dtype=x_data.dtype, device=x_data.device)
    # src[jagged_softmax.py:N]: x_flat = x_data.view(-1)
    x_flat = x_data.view(-1)
    # src[jagged_softmax.py:N]: for tile_b in hl.tile(num_rows):
    _BLOCK_SIZE_0 = 16
    # src[jagged_softmax.py:N]: for tile_m in hl.tile(M):
    # src[jagged_softmax.py:N]:     block_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
    # src[jagged_softmax.py:N]:     block_new_max = hl.full([tile_b, tile_m], 0.0, dtype=x_data.dtype)
    # src[jagged_softmax.py:N-N]: ...
    _BLOCK_SIZE_1 = 8
    # src[jagged_softmax.py:N]: for tile_k in hl.tile(max_seqlen):
    # src[jagged_softmax.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_softmax.py:N]:     flat_indices = (
    # src[jagged_softmax.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[jagged_softmax.py:N]: for tile_k in hl.tile(max_seqlen):
    # src[jagged_softmax.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_softmax.py:N]:     flat_indices = (
    # src[jagged_softmax.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[jagged_softmax.py:N]: for tile_b in hl.tile(num_rows):
    # src[jagged_softmax.py:N]:     starts = x_offsets[tile_b]
    # src[jagged_softmax.py:N]:     ends = x_offsets[tile_b.index + 1]
    # src[jagged_softmax.py:N-N]: ...
    _launcher(_helion_jagged_softmax_kernel, (triton.cdiv(128, _BLOCK_SIZE_0),), x_offsets, x_flat, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[jagged_softmax.py:N]: return out.reshape(N, M)
    return out.reshape(N, M)

--- assertExpectedJournal(TestExamples.test_jagged_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_sum_kernel(x_offsets, x_flat, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[jagged_sum.py:N]: for tile_b in hl.tile(num_rows):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[jagged_sum.py:N]: starts = x_offsets[tile_b]
    starts = tl.load(x_offsets + indices_0 * 1, None)
    # src[jagged_sum.py:N]: ends = x_offsets[tile_b.index + 1]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + (indices_0 + 1) * 1, None)
    # src[jagged_sum.py:N]: nnz = ends - starts
    v_2 = ends - starts
    # src[jagged_sum.py:N]: max_nnz = nnz.amax()
    max_nnz = tl.cast(tl.max(v_2, 0), tl.int64)
    # src[jagged_sum.py:N]: for tile_m in hl.tile(M):
    # src[jagged_sum.py:N]:     # Initialize accumulator
    # src[jagged_sum.py:N]:     row_sums = hl.zeros([tile_b, tile_m], dtype=x_data.dtype)
    # src[jagged_sum.py:N-N]: ...
    for offset_1 in tl.range(0, 8, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        # src[jagged_sum.py:N]: row_sums = hl.zeros([tile_b, tile_m], dtype=x_data.dtype)
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[jagged_sum.py:N]: for tile_k in hl.tile(0, max_nnz):
        # src[jagged_sum.py:N]:     # Compute flattened indices
        # src[jagged_sum.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
        # src[jagged_sum.py:N-N]: ...
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            row_sums_copy_0 = row_sums_copy
            # src[jagged_sum.py:N]: base_indices = starts[:, None] + tile_k.index[None, :]
            subscript = starts_copy_0_copy_0[:, None]
            subscript_1 = indices_2[None, :]
            v_3 = tl.cast(subscript_1, tl.int64)
            v_4 = subscript + v_3
            # src[jagged_sum.py:N]: base_indices[:, :, None] * M + tile_m.index[None, None, :]
            subscript_2 = v_4[:, :, None]
            v_5 = tl.full([], 8, tl.int64)
            v_6 = tl.cast(subscript_2 * v_5, tl.int64)
            subscript_3 = indices_1[None, None, :]
            v_7 = tl.cast(subscript_3, tl.int64)
            v_8 = v_6 + v_7
            # src[jagged_sum.py:N]: row_mask = tile_k.index[None, :] < nnz[:, None]
            subscript_4 = indices_2[None, :]
            subscript_5 = v_2_copy_0_copy_0[:, None]
            v_9 = tl.cast(subscript_4, tl.int64)
            v_10 = v_9 < subscript_5
            # src[jagged_sum.py:N]: combined_mask = row_mask[:, :, None]
            combined_mask = v_10[:, :, None]
            # src[jagged_sum.py:N]: x_slice = hl.load(
            # src[jagged_sum.py:N]:     x_flat,
            # src[jagged_sum.py:N]:     [flat_indices],
            # src[jagged_sum.py:N-N]: ...
            x_slice = tl.load(x_flat + v_8 * 1, mask_2[None, :, None] & combined_mask, other=0)
            # src[jagged_sum.py:N]: row_sums = row_sums + x_slice.sum(dim=1)
            sum_1 = tl.cast(tl.sum(x_slice, 1), tl.float32)
            row_sums = row_sums_copy_0 + sum_1
        # src[jagged_sum.py:N]: out[tile_b, tile_m] = row_sums
        tl.store(out + (indices_0[:, None] * 8 + indices_1[None, :] * 1), row_sums, None)

def jagged_sum_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args:
        x_data: 2-D tensor of shape (total_elements, M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]

    Returns:
        2-D tensor of shape (num_rows, M) containing the sum of jagged dimension.
    """
    # src[jagged_sum.py:N]: M = x_data.shape[1]
    M = x_data.shape[1]
    # src[jagged_sum.py:N]: num_rows = x_offsets.size(0) - 1
    num_rows = x_offsets.size(0) - 1
    # src[jagged_sum.py:N]: out = torch.zeros([num_rows, M], dtype=x_data.dtype, device=x_data.device)
    out = torch.zeros([num_rows, M], dtype=x_data.dtype, device=x_data.device)
    # src[jagged_sum.py:N]: x_flat = x_data.view(-1)
    x_flat = x_data.view(-1)
    # src[jagged_sum.py:N]: for tile_b in hl.tile(num_rows):
    _BLOCK_SIZE_0 = 16
    # src[jagged_sum.py:N]: for tile_m in hl.tile(M):
    # src[jagged_sum.py:N]:     # Initialize accumulator
    # src[jagged_sum.py:N]:     row_sums = hl.zeros([tile_b, tile_m], dtype=x_data.dtype)
    # src[jagged_sum.py:N-N]: ...
    _BLOCK_SIZE_1 = 8
    # src[jagged_sum.py:N]: for tile_k in hl.tile(0, max_nnz):
    # src[jagged_sum.py:N]:     # Compute flattened indices
    # src[jagged_sum.py:N]:     base_indices = starts[:, None] + tile_k.index[None, :]
    # src[jagged_sum.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[jagged_sum.py:N]: for tile_b in hl.tile(num_rows):
    # src[jagged_sum.py:N]:     starts = x_offsets[tile_b]
    # src[jagged_sum.py:N]:     ends = x_offsets[tile_b.index + 1]
    # src[jagged_sum.py:N-N]: ...
    _launcher(_helion_jagged_sum_kernel, (triton.cdiv(128, _BLOCK_SIZE_0),), x_offsets, x_flat, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[jagged_sum.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_jsd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jsd_forward(_input, target, loss, dX, beta, one_minus_beta, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[jsd.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[jsd.py:N]: intermediate_loss = hl.zeros([tile_bt, block_size_n], dtype=torch.float32)
    intermediate_loss = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_0], 0.0, tl.float32)
    # src[jsd.py:N]: intermediate_dX = hl.zeros([tile_bt, block_size_n], dtype=_input.dtype)
    intermediate_dX = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_0], 0.0, tl.float32)
    # src[jsd.py:N]: for tile_v in hl.tile(V, block_size=block_size_n):
    # src[jsd.py:N]:     # Load log probabilities and convert to float32
    # src[jsd.py:N]:     X = _input[tile_bt, tile_v]
    # src[jsd.py:N-N]: ...
    for offset_0 in tl.range(0, 4096):
        indices_0 = offset_0 + tl.arange(0, 1).to(tl.int32)
        intermediate_loss_copy = intermediate_loss
        intermediate_dX_copy = intermediate_dX
        intermediate_loss = intermediate_loss_copy
        intermediate_dX = intermediate_dX_copy
        # src[jsd.py:N]: X = _input[tile_bt, tile_v]
        X = tl.load(_input + (indices_1[:, None] * 4096 + indices_0[None, :] * 1), None)
        # src[jsd.py:N]: Y = target[tile_bt, tile_v]
        Y = tl.load(target + (indices_1[:, None] * 4096 + indices_0[None, :] * 1), None)
        # src[jsd.py:N]: if beta == 0.0:  # Forward KL: KL(P || Q)
        eq = beta == 0.0
        # src[jsd.py:N]: if beta == 0.0:  # Forward KL: KL(P || Q)
        # src[jsd.py:N]:     Y_max = torch.amax(Y, dim=0)
        # src[jsd.py:N]:     Y_shift = Y - Y_max
        # src[jsd.py:N-N]: ...
        if eq:
            Y_copy = Y
            X_copy = X
            intermediate_loss_copy_0_copy = intermediate_loss
            intermediate_dX_copy_0_copy = intermediate_dX
            Y_copy_0 = Y_copy
            X_copy_0 = X_copy
            intermediate_loss_copy_0_copy_0 = intermediate_loss_copy_0_copy
            intermediate_dX_copy_0_copy_0 = intermediate_dX_copy_0_copy
            # src[jsd.py:N]: Y_max = torch.amax(Y, dim=0)
            Y_max = tl.cast(tl.max(Y_copy_0, 0), tl.float32)
            # src[jsd.py:N]: Y_shift = Y - Y_max
            v_0 = Y_max[None, :]
            v_1 = Y_copy_0 - v_0
            # src[jsd.py:N]: Y_prob = torch.exp(Y_shift) * torch.exp(
            v_2 = libdevice.exp(v_1)
            # src[jsd.py:N]: Y_prob = torch.exp(Y_shift) * torch.exp(
            # src[jsd.py:N]:     Y_max
            # src[jsd.py:N]: )  # Compensate for the shift
            v_3 = libdevice.exp(Y_max)
            v_4 = v_3[None, :]
            v_5 = v_2 * v_4
            # src[jsd.py:N]: intermediate_loss += Y_prob * (Y - X)
            v_6 = Y_copy_0 - X_copy_0
            v_7 = v_5 * v_6
            intermediate_loss = intermediate_loss_copy_0_copy_0 + v_7
            # src[jsd.py:N]: intermediate_dX += -Y_prob
            v_9 = -v_5
            intermediate_dX = intermediate_dX_copy_0_copy_0 + v_9
        # src[jsd.py:N]: if beta == 0.0:  # Forward KL: KL(P || Q)
        # src[jsd.py:N]:     Y_max = torch.amax(Y, dim=0)
        # src[jsd.py:N]:     Y_shift = Y - Y_max
        # src[jsd.py:N-N]: ...
        _not = not eq
        if _not:
            X_copy_1 = X
            Y_copy_1 = Y
            intermediate_loss_copy_0_copy_1 = intermediate_loss
            intermediate_dX_copy_0_copy_1 = intermediate_dX
            X_copy_1_0 = X_copy_1
            Y_copy_1_0 = Y_copy_1
            intermediate_loss = intermediate_loss_copy_0_copy_1
            intermediate_dX = intermediate_dX_copy_0_copy_1
            # src[jsd.py:N]: elif beta == 1.0:  # Reverse KL: KL(Q || P)
            eq_1 = beta == 1.0
            # src[jsd.py:N]: elif beta == 1.0:  # Reverse KL: KL(Q || P)
            # src[jsd.py:N]:     X_max = torch.amax(X, dim=0)
            # src[jsd.py:N]:     X_shift = X - X_max
            # src[jsd.py:N-N]: ...
            if eq_1:
                X_copy_1_0_copy = X_copy_1_0
                Y_copy_1_0_copy = Y_copy_1_0
                intermediate_loss_copy_0_copy_1_0_copy = intermediate_loss
                intermediate_dX_copy_0_copy_1_0_copy = intermediate_dX
                X_copy_1_0_copy_0 = X_copy_1_0_copy
                Y_copy_1_0_copy_0 = Y_copy_1_0_copy
                intermediate_loss_copy_0_copy_1_0_copy_0 = intermediate_loss_copy_0_copy_1_0_copy
                intermediate_dX_copy_0_copy_1_0_copy_0 = intermediate_dX_copy_0_copy_1_0_copy
                # src[jsd.py:N]: X_max = torch.amax(X, dim=0)
                X_max = tl.cast(tl.max(X_copy_1_0_copy_0, 0), tl.float32)
                # src[jsd.py:N]: X_shift = X - X_max
                v_11 = X_max[None, :]
                v_12 = X_copy_1_0_copy_0 - v_11
                # src[jsd.py:N]: X_prob = torch.exp(X_shift) * torch.exp(
                v_13 = libdevice.exp(v_12)
                # src[jsd.py:N]: X_prob = torch.exp(X_shift) * torch.exp(
                # src[jsd.py:N]:     X_max
                # src[jsd.py:N]: )  # Compensate for the shift
                v_14 = libdevice.exp(X_max)
                v_15 = v_14[None, :]
                v_16 = v_13 * v_15
                # src[jsd.py:N]: intermediate_loss += X_prob * (X - Y)
                v_17 = X_copy_1_0_copy_0 - Y_copy_1_0_copy_0
                v_18 = v_16 * v_17
                intermediate_loss = intermediate_loss_copy_0_copy_1_0_copy_0 + v_18
                # src[jsd.py:N]: intermediate_dX += intermediate_loss + X_prob
                v_20 = intermediate_loss + v_16
                intermediate_dX = intermediate_dX_copy_0_copy_1_0_copy_0 + v_20
            # src[jsd.py:N]: elif beta == 1.0:  # Reverse KL: KL(Q || P)
            # src[jsd.py:N]:     X_max = torch.amax(X, dim=0)
            # src[jsd.py:N]:     X_shift = X - X_max
            # src[jsd.py:N-N]: ...
            _not_1 = not eq_1
            if _not_1:
                X_copy_1_0_copy_1 = X_copy_1_0
                Y_copy_1_0_copy_1 = Y_copy_1_0
                intermediate_loss_copy_0_copy_1_0_copy_1 = intermediate_loss
                intermediate_dX_copy_0_copy_1_0_copy_1 = intermediate_dX
                X_copy_1_0_copy_1_0 = X_copy_1_0_copy_1
                Y_copy_1_0_copy_1_0 = Y_copy_1_0_copy_1
                intermediate_loss_copy_0_copy_1_0_copy_1_0 = intermediate_loss_copy_0_copy_1_0_copy_1
                intermediate_dX_copy_0_copy_1_0_copy_1_0 = intermediate_dX_copy_0_copy_1_0_copy_1
                # src[jsd.py:N]: Q = torch.exp(X)  # = exp(X)
                v_22 = libdevice.exp(X_copy_1_0_copy_1_0)
                # src[jsd.py:N]: P = torch.exp(Y)  # = exp(Y)
                v_23 = libdevice.exp(Y_copy_1_0_copy_1_0)
                # src[jsd.py:N]: beta_P = beta * P
                v_24 = v_23 * beta
                # src[jsd.py:N]: one_minus_beta_Q = one_minus_beta * Q
                v_25 = v_22 * one_minus_beta
                # src[jsd.py:N]: M = beta_P + one_minus_beta_Q
                v_26 = v_24 + v_25
                # src[jsd.py:N]: log_M = torch.log(M)
                v_27 = tl_math.log(v_26)
                # src[jsd.py:N]: x_minus_log_m = X - log_M
                v_28 = X_copy_1_0_copy_1_0 - v_27
                # src[jsd.py:N]: kl_q_m = one_minus_beta_Q * x_minus_log_m
                v_29 = v_25 * v_28
                # src[jsd.py:N]: intermediate_loss += beta_P * (Y - log_M) + kl_q_m
                v_30 = Y_copy_1_0_copy_1_0 - v_27
                v_31 = v_24 * v_30
                v_32 = v_31 + v_29
                intermediate_loss = intermediate_loss_copy_0_copy_1_0_copy_1_0 + v_32
                # src[jsd.py:N]: intermediate_dX += kl_q_m
                intermediate_dX = intermediate_dX_copy_0_copy_1_0_copy_1_0 + v_29
    # src[jsd.py:N]: loss[tile_bt] = torch.sum(intermediate_loss * scale, dim=1)
    v_35 = 0.0001220703125
    v_36 = intermediate_loss * v_35
    sum_1 = tl.cast(tl.sum(v_36, 1), tl.float32)
    tl.store(loss + indices_1 * 1, sum_1, None)
    # src[jsd.py:N]: dX[tile_bt] = torch.sum(intermediate_dX * scale, dim=1)
    v_37 = 0.0001220703125
    v_38 = intermediate_dX * v_37
    sum_2 = tl.cast(tl.sum(v_38, 1), tl.float32)
    tl.store(dX + indices_1 * 1, sum_2, None)

def jsd_forward(_input: Tensor, target: Tensor, shift_labels: Tensor | None=None, beta: float=0.5, ignore_index: int=-100, *, _launcher=_default_launcher):
    """
    Compute Jensen-Shannon Divergence loss.

    Args:
        _input: Student predictions in log-space, shape (BT, V)
        target: Teacher targets in log-space, shape (BT, V)
        shift_labels: Optional labels for masking, shape (BT,)
        beta: Coefficient for generalized JSD in [0, 1]
        ignore_index: Index to ignore in labels

    Returns:
        loss: Scalar JSD loss
        dX: Gradient of loss wrt input
    """
    # src[jsd.py:N]: BT, V = _input.shape
    BT, V = _input.shape
    # src[jsd.py:N]: assert target.shape == _input.shape, (
    # src[jsd.py:N]:     f"Shape mismatch: {target.shape} != {_input.shape}"
    # src[jsd.py:N]: )
    assert target.shape == _input.shape, f'Shape mismatch: {target.shape} != {_input.shape}'
    # src[jsd.py:N]: loss = torch.zeros([BT], dtype=torch.float32, device=_input.device)
    loss = torch.zeros([BT], dtype=torch.float32, device=_input.device)
    # src[jsd.py:N]: dX = torch.empty_like(loss)
    dX = torch.empty_like(loss)
    # src[jsd.py:N]: one_minus_beta = 1 - beta
    one_minus_beta = 1 - beta
    # src[jsd.py:N]: n_non_ignore = float(BT)
    n_non_ignore = float(BT)
    # src[jsd.py:N]: if shift_labels is not None:
    # src[jsd.py:N]:     n_non_ignore = float((shift_labels != ignore_index).sum().item())
    # src[jsd.py:N]:     if n_non_ignore == 0:
    # src[jsd.py:N-N]: ...
    if shift_labels is not None:
        # src[jsd.py:N]: n_non_ignore = float((shift_labels != ignore_index).sum().item())
        n_non_ignore = float((shift_labels != ignore_index).sum().item())
        # src[jsd.py:N]: if n_non_ignore == 0:
        # src[jsd.py:N]:     return torch.zeros(
        # src[jsd.py:N]:         [], dtype=_input.dtype, device=_input.device
        # src[jsd.py:N-N]: ...
        if n_non_ignore == 0:
            # src[jsd.py:N]: return torch.zeros(
            # src[jsd.py:N]:     [], dtype=_input.dtype, device=_input.device
            # src[jsd.py:N]: ), torch.zeros_like(_input)
            return (torch.zeros([], dtype=_input.dtype, device=_input.device), torch.zeros_like(_input))
    # src[jsd.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    _BLOCK_SIZE_1 = 4096
    # src[jsd.py:N]: intermediate_loss = hl.zeros([tile_bt, block_size_n], dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[jsd.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    # src[jsd.py:N]:     # Check for label masking
    # src[jsd.py:N]:     if shift_labels is not None:
    # src[jsd.py:N-N]: ...
    _launcher(_helion_jsd_forward, (triton.cdiv(8192, _BLOCK_SIZE_1),), _input, target, loss, dX, beta, one_minus_beta, _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[jsd.py:N]: final_loss = torch.sum(
    # src[jsd.py:N]:     loss
    # src[jsd.py:N]: )  # This line raises a warning: helion.exc.TensorOperationInWrapper
    final_loss = torch.sum(loss)
    # src[jsd.py:N]: return final_loss, dX
    return (final_loss, dX)

--- assertExpectedJournal(TestExamples.test_kl_div)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_kl_div_forward(y_pred, y_true, loss, log_target, eps, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[kl_div.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[kl_div.py:N]: loss_sum = hl.zeros([tile_bt, block_size_n], dtype=torch.float32)
    loss_sum = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_0], 0.0, tl.float32)
    # src[kl_div.py:N]: for tile_v in hl.tile(V, block_size=block_size_n):
    # src[kl_div.py:N]:     kl_loss = hl.zeros([block_size_m, block_size_n], dtype=torch.float32)
    # src[kl_div.py:N-N]: ...
    for offset_0 in tl.range(0, 4096):
        indices_0 = offset_0 + tl.arange(0, 1).to(tl.int32)
        loss_sum_copy = loss_sum
        loss_sum_copy_0 = loss_sum_copy
        # src[kl_div.py:N]: kl_loss = hl.zeros([block_size_m, block_size_n], dtype=torch.float32)
        kl_loss = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_0], 0.0, tl.float32)
        # src[kl_div.py:N]: y_pred_val = y_pred[tile_bt, tile_v]
        y_pred_val = tl.load(y_pred + (indices_1[:, None] * 4096 + indices_0[None, :] * 1), None)
        # src[kl_div.py:N]: y_true_val = y_true[tile_bt, tile_v]
        y_true_val = tl.load(y_true + (indices_1[:, None] * 4096 + indices_0[None, :] * 1), None)
        # src[kl_div.py:N]: if log_target:
        # src[kl_div.py:N]:     # KL(P || Q) = exp(y_true) * (y_true - y_pred) when both in log-space
        # src[kl_div.py:N]:     prob_true = torch.exp(y_true_val)
        # src[kl_div.py:N-N]: ...
        if log_target:
            y_true_val_copy = y_true_val
            y_pred_val_copy = y_pred_val
            kl_loss_copy = kl_loss
            y_true_val_copy_0 = y_true_val_copy
            y_pred_val_copy_0 = y_pred_val_copy
            kl_loss_copy_0 = kl_loss_copy
            # src[kl_div.py:N]: prob_true = torch.exp(y_true_val)
            v_0 = libdevice.exp(y_true_val_copy_0)
            # src[kl_div.py:N]: kl_loss += prob_true * (y_true_val - y_pred_val)
            v_1 = y_true_val_copy_0 - y_pred_val_copy_0
            v_2 = v_0 * v_1
            kl_loss = kl_loss_copy_0 + v_2
        # src[kl_div.py:N]: if log_target:
        # src[kl_div.py:N]:     # KL(P || Q) = exp(y_true) * (y_true - y_pred) when both in log-space
        # src[kl_div.py:N]:     prob_true = torch.exp(y_true_val)
        # src[kl_div.py:N-N]: ...
        _not = not log_target
        if _not:
            y_true_val_copy_1 = y_true_val
            y_pred_val_copy_1 = y_pred_val
            kl_loss_copy_1 = kl_loss
            y_true_val_copy_1_0 = y_true_val_copy_1
            y_pred_val_copy_1_0 = y_pred_val_copy_1
            kl_loss_copy_1_0 = kl_loss_copy_1
            # src[kl_div.py:N]: log_true = torch.log(torch.clamp(y_true_val, min=eps))
            v_4 = triton_helpers.maximum(y_true_val_copy_1_0, eps)
            v_5 = tl_math.log(v_4)
            # src[kl_div.py:N]: kl_loss += y_true_val * (log_true - y_pred_val)
            v_6 = v_5 - y_pred_val_copy_1_0
            v_7 = y_true_val_copy_1_0 * v_6
            kl_loss = kl_loss_copy_1_0 + v_7
        # src[kl_div.py:N]: loss_sum += kl_loss
        loss_sum = loss_sum_copy_0 + kl_loss
    # src[kl_div.py:N]: loss[tile_bt] = loss_sum.sum(dim=-1)
    sum_1 = tl.cast(tl.sum(loss_sum, 1), tl.float32)
    tl.store(loss + indices_1 * 1, sum_1, None)

def kl_div_forward(y_pred: Tensor, y_true: Tensor, log_target: bool=False, reduction: str='batchmean', eps: float=1e-10, *, _launcher=_default_launcher):
    """
    Compute KL Divergence loss.

    Args:
        y_pred: Input predictions in log-space, shape (BT, V)
        y_true: Target values (probabilities or log-probabilities), shape (BT, V)
        log_target: If True, y_true is in log-space; if False, y_true is probabilities
        reduction: Reduction mode ('none', 'sum', 'mean', 'batchmean')
        eps: Small value to avoid numerical issues

    Returns:
        loss: KL divergence loss
    """
    # src[kl_div.py:N]: BT, V = y_pred.shape
    BT, V = y_pred.shape
    # src[kl_div.py:N]: assert y_true.shape == y_pred.shape, (
    # src[kl_div.py:N]:     f"Shape mismatch: {y_true.shape} != {y_pred.shape}"
    # src[kl_div.py:N]: )
    assert y_true.shape == y_pred.shape, f'Shape mismatch: {y_true.shape} != {y_pred.shape}'
    # src[kl_div.py:N]: if reduction == "none":
    # src[kl_div.py:N]:     loss = torch.zeros_like(y_pred)
    # src[kl_div.py:N]: else:
    # src[kl_div.py:N-N]: ...
    if reduction == 'none':
        # src[kl_div.py:N]: loss = torch.zeros_like(y_pred)
        loss = torch.zeros_like(y_pred)
    else:
        # src[kl_div.py:N]: loss = torch.zeros((BT,), dtype=torch.float32, device=y_pred.device)
        loss = torch.zeros((BT,), dtype=torch.float32, device=y_pred.device)
    # src[kl_div.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    _BLOCK_SIZE_1 = 4096
    # src[kl_div.py:N]: loss_sum = hl.zeros([tile_bt, block_size_n], dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[kl_div.py:N]: for tile_bt in hl.tile(BT, block_size=block_size_m):
    # src[kl_div.py:N]:     loss_sum = hl.zeros([tile_bt, block_size_n], dtype=torch.float32)
    # src[kl_div.py:N-N]: ...
    _launcher(_helion_kl_div_forward, (triton.cdiv(4096, _BLOCK_SIZE_1),), y_pred, y_true, loss, log_target, eps, _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[kl_div.py:N]: if reduction == "batchmean":
    # src[kl_div.py:N]:     final_loss = torch.sum(loss) / BT
    # src[kl_div.py:N]: elif reduction == "sum":
    # src[kl_div.py:N-N]: ...
    if reduction == 'batchmean':
        # src[kl_div.py:N]: final_loss = torch.sum(loss) / BT
        final_loss = torch.sum(loss) / BT
    elif reduction == 'sum':
        # src[kl_div.py:N]: final_loss = torch.sum(loss, dim=0)
        final_loss = torch.sum(loss, dim=0)
    elif reduction == 'mean':
        # src[kl_div.py:N]: final_loss = torch.sum(loss) / (BT * V)
        final_loss = torch.sum(loss) / (BT * V)
    else:
        # src[kl_div.py:N]: final_loss = loss
        final_loss = loss
    # src[kl_div.py:N]: return final_loss
    return final_loss

--- assertExpectedJournal(TestExamples.test_layernorm_bwd)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_bwd(weight, x, grad_out, mean, rstd, grad_x, grad_weight_blocks, grad_bias_blocks, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[layer_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_3 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[layer_norm.py:N]: grad_w_acc = weight.new_zeros(n, dtype=torch.float32)
    grad_w_acc = tl.full([64], 0, tl.float32)
    # src[layer_norm.py:N]: grad_b_acc = weight.new_zeros(n, dtype=torch.float32)
    grad_b_acc = tl.full([64], 0, tl.float32)
    # src[layer_norm.py:N]: weight_cta = weight[None, :].to(torch.float32)
    load = tl.load(weight + indices_3[None, :] * 1, None)
    v_0 = tl.cast(load, tl.float32)
    # src[layer_norm.py:N]: for mb in hl.tile(mb_cta.begin, mb_cta.end):
    tile_end = offset_0 + _BLOCK_SIZE_0
    # src[layer_norm.py:N]: for mb in hl.tile(mb_cta.begin, mb_cta.end):
    # src[layer_norm.py:N]:     x_mb = x[mb, :].to(torch.float32)
    # src[layer_norm.py:N]:     dy_mb = grad_out[mb, :].to(torch.float32)
    # src[layer_norm.py:N-N]: ...
    for offset_1 in tl.range(offset_0.to(tl.int32), tile_end.to(tl.int32)):
        indices_1 = offset_1 + tl.arange(0, 1).to(tl.int32)
        v_0_copy = v_0
        grad_w_acc_copy = grad_w_acc
        grad_b_acc_copy = grad_b_acc
        v_0_copy_0 = v_0_copy
        grad_w_acc_copy_0 = grad_w_acc_copy
        grad_b_acc_copy_0 = grad_b_acc_copy
        # src[layer_norm.py:N]: x_mb = x[mb, :].to(torch.float32)
        load_1 = tl.load(x + (indices_1[:, None] * 64 + indices_3[None, :] * 1), None)
        v_1 = tl.cast(load_1, tl.float32)
        # src[layer_norm.py:N]: dy_mb = grad_out[mb, :].to(torch.float32)
        load_2 = tl.load(grad_out + (indices_1[:, None] * 64 + indices_3[None, :] * 1), None)
        v_2 = tl.cast(load_2, tl.float32)
        # src[layer_norm.py:N]: mean_mb = mean[mb].to(torch.float32)
        mean_mb = tl.load(mean + indices_1 * 1, None)
        # src[layer_norm.py:N]: rstd_mb = rstd[mb].to(torch.float32)
        rstd_mb = tl.load(rstd + indices_1 * 1, None)
        # src[layer_norm.py:N]: x_hat = (x_mb - mean_mb[:, None]) * rstd_mb[:, None]
        subscript = mean_mb[:, None]
        v_3 = v_1 - subscript
        subscript_1 = rstd_mb[:, None]
        v_4 = v_3 * subscript_1
        # src[layer_norm.py:N]: grad_w_acc += torch.sum(dy_mb * x_hat, dim=0)
        v_5 = v_2 * v_4
        sum_1 = tl.cast(tl.sum(v_5, 0), tl.float32)
        grad_w_acc = grad_w_acc_copy_0 + sum_1
        # src[layer_norm.py:N]: grad_b_acc += torch.sum(dy_mb, dim=0)  # pyright: ignore[reportPossiblyUnboundVariable]
        sum_2 = tl.cast(tl.sum(v_2, 0), tl.float32)
        grad_b_acc = grad_b_acc_copy_0 + sum_2
        # src[layer_norm.py:N]: wdy = weight_cta * dy_mb
        v_8 = v_0_copy_0 * v_2
        # src[layer_norm.py:N]: c1 = torch.sum(x_hat * wdy, dim=-1) / n
        v_9 = v_4 * v_8
        sum_3 = tl.cast(tl.sum(v_9, 1), tl.float32)
        v_10 = 0.015625
        v_11 = sum_3 * v_10
        # src[layer_norm.py:N]: c2 = torch.sum(wdy, dim=-1) / n
        sum_4 = tl.cast(tl.sum(v_8, 1), tl.float32)
        v_12 = 0.015625
        v_13 = sum_4 * v_12
        # src[layer_norm.py:N]: dx = (wdy - (x_hat * c1[:, None] + c2[:, None])) * rstd_mb[:, None]
        subscript_2 = v_11[:, None]
        v_14 = v_4 * subscript_2
        subscript_3 = v_13[:, None]
        v_15 = v_14 + subscript_3
        v_16 = v_8 - v_15
        subscript_4 = rstd_mb[:, None]
        v_17 = v_16 * subscript_4
        # src[layer_norm.py:N]: grad_x[mb, :] = dx.to(x.dtype)
        v_18 = tl.cast(v_17, tl.float16)
        tl.store(grad_x + (indices_1[:, None] * 64 + indices_3[None, :] * 1), v_18, None)
    # src[layer_norm.py:N]: grad_weight_blocks[mb_cta.id, :] = grad_w_acc
    tile_id = offset_0 // _BLOCK_SIZE_0
    tl.store(grad_weight_blocks + (tile_id * 64 + indices_3 * 1), grad_w_acc, None)
    # src[layer_norm.py:N]: grad_bias_blocks[mb_cta.id, :] = grad_b_acc  # type: ignore[index]
    tl.store(grad_bias_blocks + (tile_id * 64 + indices_3 * 1), grad_b_acc, None)

def layer_norm_bwd(grad_out: torch.Tensor, x: torch.Tensor, mean: torch.Tensor, rstd: torch.Tensor, weight: torch.Tensor, compute_bias_grad: hl.constexpr=True, *, _launcher=_default_launcher):
    """
    Compute gradients for weight (dW) and optionally bias (dB) parameters.

    This kernel performs reduction across the batch dimension (M) to accumulate
    gradients for each feature dimension's weight and bias parameters.

    Args:
        grad_out: Gradient w.r.t layer norm output [M, N]
        x: Original input tensor [M, N]
        mean: Per-sample mean computed in forward pass [M]
        rstd: Per-sample reciprocal standard deviation from forward pass [M]
        weight: Weight parameter (used only for dtype/device info) [N]
        compute_bias_grad: Whether to compute bias gradient (default: True)

    Returns:
        (grad_x, grad_weight, grad_bias): Gradients for input, weight, and bias (if computed)
            grad_bias is None if compute_bias_grad is False
    """
    # src[layer_norm.py:N]: m_block = hl.register_block_size(x.size(0))
    m_block = 32
    # src[layer_norm.py:N]: n = hl.specialize(x.size(1))
    n = 64
    # src[layer_norm.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[layer_norm.py:N]: num_blocks = (x.size(0) + m_block - 1) // m_block
    num_blocks = (x.size(0) + m_block - 1) // m_block
    # src[layer_norm.py:N]: grad_weight_blocks = x.new_empty([num_blocks, n], dtype=torch.float32)
    grad_weight_blocks = x.new_empty([num_blocks, n], dtype=torch.float32)
    # src[layer_norm.py:N]: grad_bias_blocks = x.new_empty([num_blocks, n], dtype=torch.float32)
    grad_bias_blocks = x.new_empty([num_blocks, n], dtype=torch.float32)
    # src[layer_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[layer_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    # src[layer_norm.py:N]:     grad_w_acc = weight.new_zeros(n, dtype=torch.float32)
    # src[layer_norm.py:N]:     if compute_bias_grad:
    # src[layer_norm.py:N-N]: ...
    _launcher(_helion_layer_norm_bwd, (triton.cdiv(32, _BLOCK_SIZE_0),), weight, x, grad_out, mean, rstd, grad_x, grad_weight_blocks, grad_bias_blocks, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    # src[layer_norm.py:N]: grad_weight = grad_weight_blocks.sum(0).to(weight.dtype)
    grad_weight = grad_weight_blocks.sum(0).to(weight.dtype)
    # src[layer_norm.py:N]: if compute_bias_grad:
    # src[layer_norm.py:N]:     grad_bias = grad_bias_blocks.sum(0).to(weight.dtype)
    # src[layer_norm.py:N]:     return grad_x, grad_weight, grad_bias
    if True:
        # src[layer_norm.py:N]: grad_bias = grad_bias_blocks.sum(0).to(weight.dtype)
        grad_bias = grad_bias_blocks.sum(0).to(weight.dtype)
        # src[layer_norm.py:N]: return grad_x, grad_weight, grad_bias
        return (grad_x, grad_weight, grad_bias)
    # src[layer_norm.py:N]: return grad_x, grad_weight, None
    return (grad_x, grad_weight, None)

--- assertExpectedJournal(TestExamples.test_layernorm_no_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, out, mean, rstd, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[layer_norm.py:N]: acc = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = tl.cast(load, tl.float32)
    # src[layer_norm.py:N]: mean_val = torch.sum(acc, dim=-1) / n
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = 0.015625
    v_2 = sum_1 * v_1
    # src[layer_norm.py:N]: centered = acc - mean_val[:, None]
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    # src[layer_norm.py:N]: var_val = torch.sum(centered * centered, dim=-1) / n
    v_4 = v_3 * v_3
    sum_2 = tl.cast(tl.sum(v_4, 1), tl.float32)
    v_5 = 0.015625
    v_6 = sum_2 * v_5
    # src[layer_norm.py:N]: rstd_val = torch.rsqrt(var_val + eps)
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    # src[layer_norm.py:N]: normalized = centered * rstd_val[:, None]
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    # src[layer_norm.py:N]: acc = normalized * (weight[:].to(torch.float32))
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    # src[layer_norm.py:N]: out[tile_m, :] = acc.to(x.dtype)
    v_13 = tl.cast(v_12, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_13, None)
    # src[layer_norm.py:N]: mean[tile_m] = mean_val
    tl.store(mean + indices_0 * 1, v_2, None)
    # src[layer_norm.py:N]: rstd[tile_m] = rstd_val
    tl.store(rstd + indices_0 * 1, v_8, None)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    # src[layer_norm.py:N]: m, n = x.size()
    m, n = x.size()
    # src[layer_norm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[layer_norm.py:N]: if bias is not None:
    # src[layer_norm.py:N]:     assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
    if bias is not None:
        # src[layer_norm.py:N]: assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    # src[layer_norm.py:N]: assert len(normalized_shape) == 1, (
    # src[layer_norm.py:N]:     "Helion layer norm only supports 1D layer norm currently"
    # src[layer_norm.py:N]: )
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    # src[layer_norm.py:N]: assert normalized_shape[0] == n, (
    # src[layer_norm.py:N]:     f"normalized shape mismatch {normalized_shape[0]} != {n}"
    # src[layer_norm.py:N]: )
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    # src[layer_norm.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[layer_norm.py:N]: mean = torch.empty([m], dtype=torch.float32, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    # src[layer_norm.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[layer_norm.py:N]:     # Compute mean
    # src[layer_norm.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, out, mean, rstd, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    # src[layer_norm.py:N]: return out, mean, rstd
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_layernorm_with_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[layer_norm.py:N]: acc = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = tl.cast(load, tl.float32)
    # src[layer_norm.py:N]: mean_val = torch.sum(acc, dim=-1) / n
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = 0.015625
    v_2 = sum_1 * v_1
    # src[layer_norm.py:N]: centered = acc - mean_val[:, None]
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    # src[layer_norm.py:N]: var_val = torch.sum(centered * centered, dim=-1) / n
    v_4 = v_3 * v_3
    sum_2 = tl.cast(tl.sum(v_4, 1), tl.float32)
    v_5 = 0.015625
    v_6 = sum_2 * v_5
    # src[layer_norm.py:N]: rstd_val = torch.rsqrt(var_val + eps)
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    # src[layer_norm.py:N]: normalized = centered * rstd_val[:, None]
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    # src[layer_norm.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    # src[layer_norm.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, None)
    v_13 = tl.cast(load_2, tl.float32)
    # src[layer_norm.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    # src[layer_norm.py:N]:     bias[:].to(torch.float32)
    # src[layer_norm.py:N]: )
    v_14 = v_13[None, :]
    v_15 = v_12 + v_14
    # src[layer_norm.py:N]: out[tile_m, :] = acc.to(x.dtype)
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_16, None)
    # src[layer_norm.py:N]: mean[tile_m] = mean_val
    tl.store(mean + indices_0 * 1, v_2, None)
    # src[layer_norm.py:N]: rstd[tile_m] = rstd_val
    tl.store(rstd + indices_0 * 1, v_8, None)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    # src[layer_norm.py:N]: m, n = x.size()
    m, n = x.size()
    # src[layer_norm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[layer_norm.py:N]: if bias is not None:
    # src[layer_norm.py:N]:     assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
    if bias is not None:
        # src[layer_norm.py:N]: assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    # src[layer_norm.py:N]: assert len(normalized_shape) == 1, (
    # src[layer_norm.py:N]:     "Helion layer norm only supports 1D layer norm currently"
    # src[layer_norm.py:N]: )
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    # src[layer_norm.py:N]: assert normalized_shape[0] == n, (
    # src[layer_norm.py:N]:     f"normalized shape mismatch {normalized_shape[0]} != {n}"
    # src[layer_norm.py:N]: )
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    # src[layer_norm.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[layer_norm.py:N]: mean = torch.empty([m], dtype=torch.float32, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    # src[layer_norm.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[layer_norm.py:N]:     # Compute mean
    # src[layer_norm.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    # src[layer_norm.py:N]: return out, mean, rstd
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_layernorm_without_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, out, mean, rstd, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[layer_norm.py:N]: acc = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = tl.cast(load, tl.float32)
    # src[layer_norm.py:N]: mean_val = torch.sum(acc, dim=-1) / n
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = 0.015625
    v_2 = sum_1 * v_1
    # src[layer_norm.py:N]: centered = acc - mean_val[:, None]
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    # src[layer_norm.py:N]: var_val = torch.sum(centered * centered, dim=-1) / n
    v_4 = v_3 * v_3
    sum_2 = tl.cast(tl.sum(v_4, 1), tl.float32)
    v_5 = 0.015625
    v_6 = sum_2 * v_5
    # src[layer_norm.py:N]: rstd_val = torch.rsqrt(var_val + eps)
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    # src[layer_norm.py:N]: normalized = centered * rstd_val[:, None]
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    # src[layer_norm.py:N]: acc = normalized * (weight[:].to(torch.float32))
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    # src[layer_norm.py:N]: out[tile_m, :] = acc.to(x.dtype)
    v_13 = tl.cast(v_12, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_13, None)
    # src[layer_norm.py:N]: mean[tile_m] = mean_val
    tl.store(mean + indices_0 * 1, v_2, None)
    # src[layer_norm.py:N]: rstd[tile_m] = rstd_val
    tl.store(rstd + indices_0 * 1, v_8, None)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    # src[layer_norm.py:N]: m, n = x.size()
    m, n = x.size()
    # src[layer_norm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[layer_norm.py:N]: if bias is not None:
    # src[layer_norm.py:N]:     assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
    if bias is not None:
        # src[layer_norm.py:N]: assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    # src[layer_norm.py:N]: assert len(normalized_shape) == 1, (
    # src[layer_norm.py:N]:     "Helion layer norm only supports 1D layer norm currently"
    # src[layer_norm.py:N]: )
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    # src[layer_norm.py:N]: assert normalized_shape[0] == n, (
    # src[layer_norm.py:N]:     f"normalized shape mismatch {normalized_shape[0]} != {n}"
    # src[layer_norm.py:N]: )
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    # src[layer_norm.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[layer_norm.py:N]: mean = torch.empty([m], dtype=torch.float32, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[layer_norm.py:N]: for tile_m in hl.tile(m):
    # src[layer_norm.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[layer_norm.py:N]:     # Compute mean
    # src[layer_norm.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, out, mean, rstd, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    # src[layer_norm.py:N]: return out, mean, rstd
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_low_mem_dropout)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_low_mem_dropout(x_flat, out_flat, out_flat_stride_0, x_flat_stride_0, n, seed, p, scale, _BLOCK_SIZE_0: tl.constexpr):
    # src[low_mem_dropout.py:N]: for tidx in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    # src[low_mem_dropout.py:N]: xi = x_flat[tidx].to(torch.float32)
    xi = tl.load(x_flat + indices_0 * x_flat_stride_0, mask_0, other=0)
    # src[low_mem_dropout.py:N]: r = hl.rand([tidx], seed=seed)
    rand = tl.rand(seed, indices_0)
    # src[low_mem_dropout.py:N]: keep = r > p
    v_0 = rand > p
    # src[low_mem_dropout.py:N]: yscaled = xi * scale
    v_1 = xi * scale
    # src[low_mem_dropout.py:N]: yi = torch.where(keep, yscaled, 0.0)
    v_2 = 0.0
    v_3 = v_2[None]
    v_4 = tl.where(v_0, v_1, v_3)
    # src[low_mem_dropout.py:N]: out_flat[tidx] = yi.to(x.dtype)
    tl.store(out_flat + indices_0 * out_flat_stride_0, v_4, mask_0)

def low_mem_dropout(p: float, x: torch.Tensor, seed: int, *, _launcher=_default_launcher):
    """
    Applies dropout on x using p
    Args:
        p (float): dropout probability
        x (torch.Tensor): input tensor
    Returns:
        Output tensor
    """
    # src[low_mem_dropout.py:N]: scale = 1.0 / (1.0 - p)
    scale = 1.0 / (1.0 - p)
    # src[low_mem_dropout.py:N]: n = x.numel()
    n = x.numel()
    # src[low_mem_dropout.py:N]: x_flat = x.view(-1)
    x_flat = x.view(-1)
    # src[low_mem_dropout.py:N]: out_flat = torch.empty_like(x_flat)
    out_flat = torch.empty_like(x_flat)
    # src[low_mem_dropout.py:N]: for tidx in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    # src[low_mem_dropout.py:N]: for tidx in hl.tile(n):
    # src[low_mem_dropout.py:N]:     xi = x_flat[tidx].to(torch.float32)
    # src[low_mem_dropout.py:N]:     r = hl.rand([tidx], seed=seed)
    # src[low_mem_dropout.py:N-N]: ...
    _launcher(_helion_low_mem_dropout, (triton.cdiv(n, _BLOCK_SIZE_0),), x_flat, out_flat, out_flat.stride(0), x_flat.stride(0), n, seed, p, scale, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[low_mem_dropout.py:N]: return out_flat.view_as(x)
    return out_flat.view_as(x)

--- assertExpectedJournal(TestExamples.test_matmul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(128, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(128, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 128, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 128 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 128 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    tl.store(out + (indices_0[:, None] * 128 + indices_1[None, :] * 1), acc, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _BLOCK_SIZE_2 = 16
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(128, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[matmul.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_matmul_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_bwd(grad_out, mat2, grad_mat1, mat1, grad_mat2, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr, _BLOCK_SIZE_5: tl.constexpr):
    # src[matmul.py:N]: for tile_m1, tile_k1 in hl.tile([m, k]):
    # src[matmul.py:N]:     acc1 = hl.zeros([tile_m1, tile_k1], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_n1 in hl.tile(n):
    # src[matmul.py:N-N]: ...
    pid_shared = tl.program_id(0)
    if pid_shared < tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1):
        # src[matmul.py:N]: for tile_m1, tile_k1 in hl.tile([m, k]):
        num_blocks_0 = tl.cdiv(128, _BLOCK_SIZE_0)
        pid_0 = pid_shared % num_blocks_0
        pid_1 = pid_shared // num_blocks_0
        offset_0 = pid_0 * _BLOCK_SIZE_0
        indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
        offset_1 = pid_1 * _BLOCK_SIZE_1
        indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
        # src[matmul.py:N]: acc1 = hl.zeros([tile_m1, tile_k1], dtype=torch.float32)
        acc1 = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        # src[matmul.py:N]: for tile_n1 in hl.tile(n):
        # src[matmul.py:N]:     # Need mat2.T: mat2 is [k, n], so mat2[tile_k, tile_n].T gives [tile_n, tile_k]
        # src[matmul.py:N]:     acc1 = torch.addmm(
        # src[matmul.py:N-N]: ...
        for offset_2 in tl.range(0, 128, _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            acc1_copy = acc1
            acc1_copy_0 = acc1_copy
            # src[matmul.py:N]: acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
            load = tl.load(grad_out + (indices_0[:, None] * 128 + indices_2[None, :] * 1), None)
            load_1 = tl.load(mat2 + (indices_1[:, None] * 128 + indices_2[None, :] * 1), None)
            permute = tl.permute(load_1, [1, 0])
            # src[matmul.py:N]: acc1 = torch.addmm(
            # src[matmul.py:N]:     acc1, grad_out[tile_m1, tile_n1], mat2[tile_k1, tile_n1].T
            # src[matmul.py:N]: )
            acc1 = tl.dot(tl.cast(load, tl.float32), tl.cast(permute, tl.float32), acc=acc1_copy_0, input_precision='tf32', out_dtype=tl.float32)
        # src[matmul.py:N]: grad_mat1[tile_m1, tile_k1] = acc1.to(mat1.dtype)
        tl.store(grad_mat1 + (indices_0[:, None] * 128 + indices_1[None, :] * 1), acc1, None)
    else:
        # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
        pid_shared -= tl.cdiv(128, _BLOCK_SIZE_0) * tl.cdiv(128, _BLOCK_SIZE_1)
        num_blocks_1 = tl.cdiv(128, _BLOCK_SIZE_3)
        pid_2 = pid_shared % num_blocks_1
        pid_3 = pid_shared // num_blocks_1
        offset_3 = pid_2 * _BLOCK_SIZE_3
        indices_3 = (offset_3 + tl.arange(0, _BLOCK_SIZE_3)).to(tl.int32)
        offset_4 = pid_3 * _BLOCK_SIZE_4
        indices_4 = (offset_4 + tl.arange(0, _BLOCK_SIZE_4)).to(tl.int32)
        # src[matmul.py:N]: acc2 = hl.zeros([tile_k2, tile_n2], dtype=torch.float32)
        acc2 = tl.full([_BLOCK_SIZE_3, _BLOCK_SIZE_4], 0.0, tl.float32)
        # src[matmul.py:N]: for tile_m2 in hl.tile(m):
        # src[matmul.py:N]:     # Need mat1.T: mat1 is [m, k], so mat1[tile_m, tile_k].T gives [tile_k, tile_m]
        # src[matmul.py:N]:     acc2 = torch.addmm(
        # src[matmul.py:N-N]: ...
        for offset_5 in tl.range(0, 128, _BLOCK_SIZE_5):
            indices_5 = offset_5 + tl.arange(0, _BLOCK_SIZE_5).to(tl.int32)
            acc2_copy = acc2
            acc2_copy_0 = acc2_copy
            # src[matmul.py:N]: acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
            load_2 = tl.load(mat1 + (indices_5[:, None] * 128 + indices_3[None, :] * 1), None)
            permute_1 = tl.permute(load_2, [1, 0])
            load_3 = tl.load(grad_out + (indices_5[:, None] * 128 + indices_4[None, :] * 1), None)
            # src[matmul.py:N]: acc2 = torch.addmm(
            # src[matmul.py:N]:     acc2, mat1[tile_m2, tile_k2].T, grad_out[tile_m2, tile_n2]
            # src[matmul.py:N]: )
            acc2 = tl.dot(tl.cast(permute_1, tl.float32), tl.cast(load_3, tl.float32), acc=acc2_copy_0, input_precision='tf32', out_dtype=tl.float32)
        # src[matmul.py:N]: grad_mat2[tile_k2, tile_n2] = acc2.to(mat2.dtype)
        tl.store(grad_mat2 + (indices_3[:, None] * 128 + indices_4[None, :] * 1), acc2, None)

def matmul_bwd(grad_out: Tensor, mat1: Tensor, mat2: Tensor, *, _launcher=_default_launcher):
    """
    Backward pass for matrix multiplication following Triton reference pattern.

    For C = A @ B, given grad_C, computes:
    - grad_A = grad_C @ B.T
    - grad_B = A.T @ grad_C

    Args:
        grad_out: Gradient w.r.t output [m, n]
        mat1: First matrix [m, k]
        mat2: Second matrix [k, n]

    Returns:
        tuple[Tensor, Tensor]: (grad_mat1, grad_mat2)
    """
    # src[matmul.py:N]: m, n = grad_out.size()
    m, n = grad_out.size()
    # src[matmul.py:N]: m2, k = mat1.size()
    m2, k = mat1.size()
    # src[matmul.py:N]: k2, n2 = mat2.size()
    k2, n2 = mat2.size()
    # src[matmul.py:N]: assert m == m2 and n == n2 and k == k2, "Size mismatch in matmul backward"
    assert m == m2 and n == n2 and (k == k2), 'Size mismatch in matmul backward'
    # src[matmul.py:N]: grad_mat1 = torch.empty_like(mat1)
    grad_mat1 = torch.empty_like(mat1)
    # src[matmul.py:N]: grad_mat2 = torch.empty_like(mat2)
    grad_mat2 = torch.empty_like(mat2)
    # src[matmul.py:N]: for tile_m1, tile_k1 in hl.tile([m, k]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[matmul.py:N]: for tile_n1 in hl.tile(n):
    # src[matmul.py:N]:     # Need mat2.T: mat2 is [k, n], so mat2[tile_k, tile_n].T gives [tile_n, tile_k]
    # src[matmul.py:N]:     acc1 = torch.addmm(
    # src[matmul.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
    _BLOCK_SIZE_3 = 16
    _BLOCK_SIZE_4 = 16
    # src[matmul.py:N]: for tile_m2 in hl.tile(m):
    # src[matmul.py:N]:     # Need mat1.T: mat1 is [m, k], so mat1[tile_m, tile_k].T gives [tile_k, tile_m]
    # src[matmul.py:N]:     acc2 = torch.addmm(
    # src[matmul.py:N-N]: ...
    _BLOCK_SIZE_5 = 16
    # src[matmul.py:N]: for tile_k2, tile_n2 in hl.tile([k, n]):
    # src[matmul.py:N]:     acc2 = hl.zeros([tile_k2, tile_n2], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_m2 in hl.tile(m):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul_bwd, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(128, _BLOCK_SIZE_1) + triton.cdiv(128, _BLOCK_SIZE_3) * triton.cdiv(128, _BLOCK_SIZE_4),), grad_out, mat2, grad_mat1, mat1, grad_mat2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, _BLOCK_SIZE_4, _BLOCK_SIZE_5, num_warps=4, num_stages=1)
    # src[matmul.py:N]: return grad_mat1, grad_mat2
    return (grad_mat1, grad_mat2)

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_dynamic_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(x, y, weight, bias, out, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, y_stride_0, y_stride_1, m, k, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_5 < 400
    # src[matmul_layernorm.py:N]: acc = hl.zeros([tile_m, n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, 512], 0.0, tl.float32)
    # src[matmul_layernorm.py:N]: for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[matmul_layernorm.py:N]:     acc = acc + mm
    for offset_4 in tl.range(0, k.to(tl.int32), _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_4 < k
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul_layernorm.py:N]: mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_4[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        load_1 = tl.load(y + (indices_4[:, None] * y_stride_0 + indices_5[None, :] * y_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
        mm = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        # src[matmul_layernorm.py:N]: acc = acc + mm
        acc = acc_copy_0 + mm
    # src[matmul_layernorm.py:N]: sum_vals = acc.sum(dim=-1, keepdim=True)
    _mask_to = tl.where(tl.broadcast_to(mask_0[:, None], [_BLOCK_SIZE_0, 512]), acc, tl.full([], 0, tl.float32))
    sum_vals = tl.cast(tl.reshape(tl.sum(_mask_to, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[matmul_layernorm.py:N]: mean = sum_vals / n
    v_1 = 0.0025
    v_2 = sum_vals * v_1
    # src[matmul_layernorm.py:N]: centered = acc - mean
    v_3 = acc - v_2
    # src[matmul_layernorm.py:N]: var = (centered * centered).sum(dim=-1, keepdim=True) / n
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(tl.broadcast_to(mask_0[:, None], [_BLOCK_SIZE_0, 512]), v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_5 = 0.0025
    v_6 = sum_2 * v_5
    # src[matmul_layernorm.py:N]: normalized = centered * torch.rsqrt(var + eps)
    v_7 = 1e-05
    v_8 = v_6 + v_7
    v_9 = tl.rsqrt(v_8)
    v_10 = v_3 * v_9
    # src[matmul_layernorm.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (bias[:].to(torch.float32))
    load_2 = tl.load(weight + indices_5 * weight_stride_0, mask_2, other=0)
    v_11 = load_2[None, :]
    v_12 = v_10 * v_11
    load_3 = tl.load(bias + indices_5 * bias_stride_0, mask_2, other=0)
    v_13 = load_3[None, :]
    v_14 = v_12 + v_13
    # src[matmul_layernorm.py:N]: out[tile_m, :] = acc
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_5[None, :] * out_stride_1), v_14, mask_0[:, None] & mask_2[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    # src[matmul_layernorm.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul_layernorm.py:N]: k2 = y.size(0)
    k2 = y.size(0)
    # src[matmul_layernorm.py:N]: n = hl.specialize(y.size(1))
    n = 400
    # src[matmul_layernorm.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul_layernorm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[matmul_layernorm.py:N]: assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    # src[matmul_layernorm.py:N]: out = torch.empty(
    # src[matmul_layernorm.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul_layernorm.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_2 = 512
    # src[matmul_layernorm.py:N]: for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[matmul_layernorm.py:N]:     acc = acc + mm
    _BLOCK_SIZE_1 = 16
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    # src[matmul_layernorm.py:N]:     acc = hl.zeros([tile_m, n], dtype=torch.float32)
    # src[matmul_layernorm.py:N]:     for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N-N]: ...
    _launcher(_helion_matmul_layernorm, (triton.cdiv(m, _BLOCK_SIZE_0),), x, y, weight, bias, out, bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), y.stride(0), y.stride(1), m, k, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[matmul_layernorm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_static_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(x, y, weight, bias, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_5 < 400
    # src[matmul_layernorm.py:N]: acc = hl.zeros([tile_m, n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, 512], 0.0, tl.float32)
    # src[matmul_layernorm.py:N]: for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[matmul_layernorm.py:N]:     acc = acc + mm
    for offset_4 in tl.range(0, 256, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul_layernorm.py:N]: mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
        load = tl.load(x + (indices_0[:, None] * 256 + indices_4[None, :] * 1), None)
        load_1 = tl.load(y + (indices_4[:, None] * 400 + indices_5[None, :] * 1), mask_2[None, :], other=0)
        mm = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        # src[matmul_layernorm.py:N]: acc = acc + mm
        acc = acc_copy_0 + mm
    # src[matmul_layernorm.py:N]: sum_vals = acc.sum(dim=-1, keepdim=True)
    sum_vals = tl.cast(tl.reshape(tl.sum(acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[matmul_layernorm.py:N]: mean = sum_vals / n
    v_1 = 0.0025
    v_2 = sum_vals * v_1
    # src[matmul_layernorm.py:N]: centered = acc - mean
    v_3 = acc - v_2
    # src[matmul_layernorm.py:N]: var = (centered * centered).sum(dim=-1, keepdim=True) / n
    v_4 = v_3 * v_3
    sum_2 = tl.cast(tl.reshape(tl.sum(v_4, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_5 = 0.0025
    v_6 = sum_2 * v_5
    # src[matmul_layernorm.py:N]: normalized = centered * torch.rsqrt(var + eps)
    v_7 = 1e-05
    v_8 = v_6 + v_7
    v_9 = tl.rsqrt(v_8)
    v_10 = v_3 * v_9
    # src[matmul_layernorm.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (bias[:].to(torch.float32))
    load_2 = tl.load(weight + indices_5 * 1, mask_2, other=0)
    v_11 = load_2[None, :]
    v_12 = v_10 * v_11
    load_3 = tl.load(bias + indices_5 * 1, mask_2, other=0)
    v_13 = load_3[None, :]
    v_14 = v_12 + v_13
    # src[matmul_layernorm.py:N]: out[tile_m, :] = acc
    tl.store(out + (indices_0[:, None] * 400 + indices_5[None, :] * 1), v_14, mask_2[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    # src[matmul_layernorm.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul_layernorm.py:N]: k2 = y.size(0)
    k2 = y.size(0)
    # src[matmul_layernorm.py:N]: n = hl.specialize(y.size(1))
    n = 400
    # src[matmul_layernorm.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul_layernorm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[matmul_layernorm.py:N]: assert bias.size(0) == n, f"bias size mismatch {bias.size(0)} != {n}"
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    # src[matmul_layernorm.py:N]: out = torch.empty(
    # src[matmul_layernorm.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul_layernorm.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_2 = 512
    # src[matmul_layernorm.py:N]: for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[matmul_layernorm.py:N]:     acc = acc + mm
    _BLOCK_SIZE_1 = 16
    # src[matmul_layernorm.py:N]: for tile_m in hl.tile(m):
    # src[matmul_layernorm.py:N]:     acc = hl.zeros([tile_m, n], dtype=torch.float32)
    # src[matmul_layernorm.py:N]:     for tile_k in hl.tile(k):
    # src[matmul_layernorm.py:N-N]: ...
    _launcher(_helion_matmul_layernorm, (triton.cdiv(128, _BLOCK_SIZE_0),), x, y, weight, bias, out, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[matmul_layernorm.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_matmul_split_k)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import helion._testing.matmul_split_k as _source_module

@triton.jit
def _helion_matmul_split_k(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[matmul_split_k.py:N]: for tile_m, tile_n, outer_k in hl.tile([m, n, k], block_size=[None, None, k_block]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(64, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    # src[matmul_split_k.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul_split_k.py:N]: for inner_k in hl.tile(outer_k.begin, outer_k.end):
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1024)
    # src[matmul_split_k.py:N]: for inner_k in hl.tile(outer_k.begin, outer_k.end):
    # src[matmul_split_k.py:N]:     acc = torch.addmm(acc, x[tile_m, inner_k], y[inner_k, tile_n])
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul_split_k.py:N]: acc = torch.addmm(acc, x[tile_m, inner_k], y[inner_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 64 + indices_1[None, :] * 1), mask_3[:, None], other=0)
        acc = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul_split_k.py:N]: if outer_k.begin == 0:
    eq = offset_2 == 0
    # src[matmul_split_k.py:N]: if outer_k.begin == 0:
    # src[matmul_split_k.py:N]:     acc = epilogue(acc, (tile_m, tile_n))
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    # src[matmul_split_k.py:N]: hl.atomic_add(out, [tile_m, tile_n], acc)
    tl.atomic_add(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, tuple[torch.Tensor, ...]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Matrix multiplication kernel using split-K parallelism.
    This kernel splits the reduction (K) dimension into multiple fragments to improve
    parallelism and performance, especially for large K. The results from each split
    are accumulated atomically into the output tensor. An optional epilogue function
    can be applied to the accumulator, e.g., for adding bias.
    Args:
        x (torch.Tensor): Left input matrix of shape [m, k].
        y (torch.Tensor): Right input matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        torch.Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul_split_k.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul_split_k.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul_split_k.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul_split_k.py:N]: out = torch.zeros(
    # src[matmul_split_k.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul_split_k.py:N]: )
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul_split_k.py:N]: split_k = hl.register_tunable("split_k", PowerOfTwoFragment(1, 256))
    split_k = 8
    # src[matmul_split_k.py:N]: k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    # src[matmul_split_k.py:N]: for tile_m, tile_n, outer_k in hl.tile([m, n, k], block_size=[None, None, k_block]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = k_block
    # src[matmul_split_k.py:N]: for inner_k in hl.tile(outer_k.begin, outer_k.end):
    # src[matmul_split_k.py:N]:     acc = torch.addmm(acc, x[tile_m, inner_k], y[inner_k, tile_n])
    _BLOCK_SIZE_3 = 32
    # src[matmul_split_k.py:N]: for tile_m, tile_n, outer_k in hl.tile([m, n, k], block_size=[None, None, k_block]):
    # src[matmul_split_k.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul_split_k.py:N]:     for inner_k in hl.tile(outer_k.begin, outer_k.end):
    # src[matmul_split_k.py:N-N]: ...
    _launcher(_helion_matmul_split_k, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[matmul_split_k.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_moe_matmul_ogs)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_moe_matmul_ogs(expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A_stride_0, A_stride_1, C_stride_0, C_stride_1, W_stride_0, W_stride_1, W_stride_2, expert_token_counts_stride_0, expert_token_offsets_stride_0, sorted_to_orig_token_idx_stride_0, max_T_per_expert, N, K, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[moe_matmul_ogs.py:N]: for e_idx in hl.grid(E):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[moe_matmul_ogs.py:N]: start = expert_token_offsets[e_idx]
    start = tl.load(expert_token_offsets + offset_0 * expert_token_offsets_stride_0, None)
    # src[moe_matmul_ogs.py:N]: num_tokens = expert_token_counts[e_idx]
    num_tokens = tl.load(expert_token_counts + offset_0 * expert_token_counts_stride_0, None)
    # src[moe_matmul_ogs.py:N]: if num_tokens != 0:
    v_0 = tl.full([], 0, tl.int32)
    v_1 = num_tokens != v_0
    # src[moe_matmul_ogs.py:N]: if num_tokens != 0:
    # src[moe_matmul_ogs.py:N]:     for tile_t, tile_n in hl.tile([max_T_per_expert, N]):
    # src[moe_matmul_ogs.py:N]:         local_token_offsets = tile_t.index
    # src[moe_matmul_ogs.py:N-N]: ...
    if v_1:
        num_tokens_copy = num_tokens
        start_copy = start
        num_tokens_copy_0 = num_tokens_copy
        start_copy_0 = start_copy
        # src[moe_matmul_ogs.py:N]: for tile_t, tile_n in hl.tile([max_T_per_expert, N]):
        # src[moe_matmul_ogs.py:N]:     local_token_offsets = tile_t.index
        # src[moe_matmul_ogs.py:N]:     token_valid = local_token_offsets < num_tokens
        # src[moe_matmul_ogs.py:N-N]: ...
        for offset_1 in tl.range(0, max_T_per_expert.to(tl.int32), _BLOCK_SIZE_1):
            indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
            mask_1 = indices_1 < max_T_per_expert
            for offset_2 in tl.range(0, N.to(tl.int32), _BLOCK_SIZE_2):
                indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
                mask_2 = indices_2 < N
                num_tokens_copy_0_copy = num_tokens_copy_0
                start_copy_0_copy = start_copy_0
                num_tokens_copy_0_copy_0 = num_tokens_copy_0_copy
                start_copy_0_copy_0 = start_copy_0_copy
                # src[moe_matmul_ogs.py:N]: token_valid = local_token_offsets < num_tokens
                v_2 = num_tokens_copy_0_copy_0[None]
                v_3 = indices_1 < v_2
                # src[moe_matmul_ogs.py:N]: local_token_offsets_valid = torch.where(
                # src[moe_matmul_ogs.py:N]:     token_valid, local_token_offsets, 0
                # src[moe_matmul_ogs.py:N]: )
                v_4 = tl.full([], 0, tl.int32)
                v_5 = v_4[None]
                v_6 = tl.where(v_3, indices_1, v_5)
                # src[moe_matmul_ogs.py:N]: expert_sorted_token_indices = start + local_token_offsets_valid
                v_7 = start_copy_0_copy_0[None]
                v_8 = v_7 + v_6
                # src[moe_matmul_ogs.py:N]: expert_sorted_token_indices.squeeze(0)
                squeeze = tl.reshape(v_8, [_BLOCK_SIZE_1])
                # src[moe_matmul_ogs.py:N]: expert_orig_token_indices = sorted_to_orig_token_idx[
                # src[moe_matmul_ogs.py:N]:     expert_sorted_token_indices.squeeze(0)
                # src[moe_matmul_ogs.py:N]: ]
                expert_orig_token_indices = tl.load(sorted_to_orig_token_idx + squeeze * sorted_to_orig_token_idx_stride_0, mask_1, other=0)
                # src[moe_matmul_ogs.py:N]: acc = hl.zeros([tile_t, tile_n], dtype=torch.float32)
                acc = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
                # src[moe_matmul_ogs.py:N]: for tile_k in hl.tile(K):
                # src[moe_matmul_ogs.py:N]:     A_frag = A[expert_orig_token_indices, tile_k]
                # src[moe_matmul_ogs.py:N]:     W_frag = W[e_idx, tile_k, tile_n]
                # src[moe_matmul_ogs.py:N-N]: ...
                for offset_3 in tl.range(0, K.to(tl.int32), _BLOCK_SIZE_3):
                    indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
                    mask_3 = indices_3 < K
                    expert_orig_token_indices_copy = expert_orig_token_indices
                    acc_copy = acc
                    expert_orig_token_indices_copy_0 = expert_orig_token_indices_copy
                    acc_copy_0 = acc_copy
                    # src[moe_matmul_ogs.py:N]: A_frag = A[expert_orig_token_indices, tile_k]
                    A_frag = tl.load(A + (expert_orig_token_indices_copy_0[:, None] * A_stride_0 + indices_3[None, :] * A_stride_1), mask_1[:, None] & mask_3[None, :], other=0)
                    # src[moe_matmul_ogs.py:N]: W_frag = W[e_idx, tile_k, tile_n]
                    W_frag = tl.load(W + (offset_0 * W_stride_0 + indices_3[:, None] * W_stride_1 + indices_2[None, :] * W_stride_2), mask_3[:, None] & mask_2[None, :], other=0)
                    # src[moe_matmul_ogs.py:N]: acc = torch.addmm(acc, A_frag, W_frag)
                    acc = tl.dot(tl.cast(A_frag, tl.float16), tl.cast(W_frag, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
                # src[moe_matmul_ogs.py:N]: existing_values = C[expert_orig_token_indices, tile_n]
                existing_values = tl.load(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
                # src[moe_matmul_ogs.py:N]: mask_2d = token_valid.view(block_T, 1).expand(block_T, block_N)
                view = tl.reshape(v_3, [_BLOCK_SIZE_1, 1])
                mask_2d = tl.broadcast_to(view, [_BLOCK_SIZE_1, _BLOCK_SIZE_2])
                # src[moe_matmul_ogs.py:N]: mask_2d, acc.to(C.dtype), existing_values
                v_9 = tl.cast(acc, tl.float16)
                # src[moe_matmul_ogs.py:N]: C[expert_orig_token_indices, tile_n] = torch.where(
                # src[moe_matmul_ogs.py:N]:     mask_2d, acc.to(C.dtype), existing_values
                # src[moe_matmul_ogs.py:N]: )
                v_10 = tl.where(mask_2d, v_9, existing_values)
                tl.store(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), v_10, mask_1[:, None] & mask_2[None, :])

def moe_matmul_ogs(A: torch.Tensor, W: torch.Tensor, expert_token_counts: torch.Tensor, expert_token_offsets: torch.Tensor, sorted_to_orig_token_idx: torch.Tensor, max_T_per_expert: int, *, _launcher=_default_launcher):
    """
    Helion kernel implementing MoE matmul with Outer-Gather-Scatter.
    Args:
        A (torch.Tensor): Input activations of shape [T, K].
        W (torch.Tensor): Expert weights of shape [E, K, N].
        expert_token_counts (torch.Tensor): Number of tokens per expert [E].
        expert_token_offsets (torch.Tensor): Starting offsets of tokens per expert [E+1].
        sorted_to_orig_token_idx (torch.Tensor): Maps sorted token indices to original token indices [T].
        max_T_per_expert (int): Maximum number of tokens per expert.
    Returns:
        torch.Tensor: Output activations of shape [T, N].
    """
    # src[moe_matmul_ogs.py:N]: T, K = A.shape
    T, K = A.shape
    # src[moe_matmul_ogs.py:N]: E, _, N = W.shape
    E, _, N = W.shape
    # src[moe_matmul_ogs.py:N]: C = torch.zeros(
    # src[moe_matmul_ogs.py:N]:     T,
    # src[moe_matmul_ogs.py:N]:     N,
    # src[moe_matmul_ogs.py:N-N]: ...
    C = torch.zeros(T, N, dtype=torch.promote_types(A.dtype, W.dtype), device=A.device)
    # src[moe_matmul_ogs.py:N]: for tile_t, tile_n in hl.tile([max_T_per_expert, N]):
    # src[moe_matmul_ogs.py:N]:     local_token_offsets = tile_t.index
    # src[moe_matmul_ogs.py:N]:     token_valid = local_token_offsets < num_tokens
    # src[moe_matmul_ogs.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_1 = 16
    # src[moe_matmul_ogs.py:N]: for tile_k in hl.tile(K):
    # src[moe_matmul_ogs.py:N]:     A_frag = A[expert_orig_token_indices, tile_k]
    # src[moe_matmul_ogs.py:N]:     W_frag = W[e_idx, tile_k, tile_n]
    # src[moe_matmul_ogs.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[moe_matmul_ogs.py:N]: for e_idx in hl.grid(E):
    # src[moe_matmul_ogs.py:N]:     start = expert_token_offsets[e_idx]
    # src[moe_matmul_ogs.py:N]:     num_tokens = expert_token_counts[e_idx]
    # src[moe_matmul_ogs.py:N-N]: ...
    _launcher(_helion_moe_matmul_ogs, (E,), expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A.stride(0), A.stride(1), C.stride(0), C.stride(1), W.stride(0), W.stride(1), W.stride(2), expert_token_counts.stride(0), expert_token_offsets.stride(0), sorted_to_orig_token_idx.stride(0), max_T_per_expert, N, K, _BLOCK_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[moe_matmul_ogs.py:N]: return C
    return C

--- assertExpectedJournal(TestExamples.test_rms_norm_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm_bwd(x, grad_out, rsqrt, weight, grad_x, grad_weight, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[rms_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[rms_norm.py:N]: grad_w_m = weight.new_zeros(weight_shape, dtype=torch.float32)
    grad_w_m = tl.full([64], 0, tl.float32)
    # src[rms_norm.py:N]: for mb in hl.tile(mb_cta.begin, mb_cta.end):
    tile_end = offset_0 + _BLOCK_SIZE_0
    # src[rms_norm.py:N]: for mb in hl.tile(mb_cta.begin, mb_cta.end):
    # src[rms_norm.py:N]:     x_m = x[mb, :].to(torch.float32)
    # src[rms_norm.py:N]:     do_m = grad_out[mb, :].to(torch.float32)
    # src[rms_norm.py:N-N]: ...
    for offset_2 in tl.range(offset_0.to(tl.int32), tile_end.to(tl.int32)):
        indices_2 = offset_2 + tl.arange(0, 1).to(tl.int32)
        grad_w_m_copy = grad_w_m
        grad_w_m_copy_0 = grad_w_m_copy
        # src[rms_norm.py:N]: x_m = x[mb, :].to(torch.float32)
        load = tl.load(x + (indices_2[:, None] * 64 + indices_3[None, :] * 1), None)
        v_0 = tl.cast(load, tl.float32)
        # src[rms_norm.py:N]: do_m = grad_out[mb, :].to(torch.float32)
        load_1 = tl.load(grad_out + (indices_2[:, None] * 64 + indices_3[None, :] * 1), None)
        v_1 = tl.cast(load_1, tl.float32)
        # src[rms_norm.py:N]: rsqrt_m = rsqrt[mb, :].to(torch.float32)
        load_2 = tl.load(rsqrt + indices_2[:, None] * 1, None)
        v_2 = tl.cast(load_2, tl.float32)
        # src[rms_norm.py:N]: grad_w_m += (x_m * do_m * rsqrt_m).sum(0)
        v_3 = v_0 * v_1
        v_4 = v_3 * v_2
        sum_1 = tl.cast(tl.sum(v_4, 0), tl.float32)
        grad_w_m = grad_w_m_copy_0 + sum_1
        # src[rms_norm.py:N]: w_m = weight[None, :].to(torch.float32)
        load_3 = tl.load(weight + indices_3[None, :] * 1, None)
        v_6 = tl.cast(load_3, tl.float32)
        # src[rms_norm.py:N]: w_m * do_m * rsqrt_m
        v_7 = v_6 * v_1
        v_8 = v_7 * v_2
        # src[rms_norm.py:N]: - x_m * rsqrt_m**3 * (w_m * do_m * x_m).mean(-1)[:, None]
        v_9 = v_2 * v_2
        v_10 = v_9 * v_2
        v_11 = v_0 * v_10
        v_12 = v_6 * v_1
        v_13 = v_12 * v_0
        mean_extra = tl.cast(tl.sum(v_13, 1), tl.float32)
        v_14 = 64
        v_15 = mean_extra / v_14.to(tl.float32)
        subscript = v_15[:, None]
        v_16 = v_11 * subscript
        # src[rms_norm.py:N]: w_m * do_m * rsqrt_m
        # src[rms_norm.py:N]: - x_m * rsqrt_m**3 * (w_m * do_m * x_m).mean(-1)[:, None]
        v_17 = v_8 - v_16
        # src[rms_norm.py:N]: grad_x[mb, :] = (
        # src[rms_norm.py:N]:     w_m * do_m * rsqrt_m
        # src[rms_norm.py:N]:     - x_m * rsqrt_m**3 * (w_m * do_m * x_m).mean(-1)[:, None]
        # src[rms_norm.py:N-N]: ...
        v_18 = tl.cast(v_17, tl.float16)
        tl.store(grad_x + (indices_2[:, None] * 64 + indices_3[None, :] * 1), v_18, None)
    # src[rms_norm.py:N]: grad_weight[mb_cta.id, :] = grad_w_m
    tile_id = offset_0 // _BLOCK_SIZE_0
    tl.store(grad_weight + (tile_id * 64 + indices_3 * 1), grad_w_m, None)

def rms_norm_bwd(grad_out: torch.Tensor, x: torch.Tensor, weight: torch.Tensor, rsqrt: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute gradient for input tensor (dX) and weights (dW).

    This kernel computes per-sample gradients by performing reductions across
    the feature dimension (N) for each sample in the batch and across the batches
    in a split fashion.

    Args:
        grad_out: Gradient w.r.t rms norm output [M, N]
        x: Original input tensor [M, N]
        weight: Weight parameter [N]
        inv_rms: Inverse RMS tensor [M, 1]

    Returns:
        grad_x: Gradient w.r.t input tensor, shape [M, N]
        grad_weight: Gradient w.r.t eight tensor, shape [N]
    """
    # src[rms_norm.py:N]: m_block = hl.register_block_size(x.size(0))
    m_block = 32
    # src[rms_norm.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[rms_norm.py:N]: grad_weight = x.new_empty(
    # src[rms_norm.py:N]:     [(x.size(0) + m_block - 1) // m_block, *weight.shape], dtype=torch.float32
    # src[rms_norm.py:N]: )
    grad_weight = x.new_empty([(x.size(0) + m_block - 1) // m_block, *weight.shape], dtype=torch.float32)
    # src[rms_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 64
    # src[rms_norm.py:N]: for mb_cta in hl.tile(x.size(0), block_size=m_block):
    # src[rms_norm.py:N]:     grad_w_m = weight.new_zeros(weight_shape, dtype=torch.float32)
    # src[rms_norm.py:N]:     for mb in hl.tile(mb_cta.begin, mb_cta.end):
    # src[rms_norm.py:N-N]: ...
    _launcher(_helion_rms_norm_bwd, (triton.cdiv(32, _BLOCK_SIZE_0),), x, grad_out, rsqrt, weight, grad_x, grad_weight, _BLOCK_SIZE_0, _RDIM_SIZE_2, num_warps=4, num_stages=3)
    # src[rms_norm.py:N]: return grad_x, grad_weight.sum(0).to(weight.dtype)
    return (grad_x, grad_weight.sum(0).to(weight.dtype))

--- assertExpectedJournal(TestExamples.test_rms_norm_fwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm_fwd(x, weight, out, inv_rms, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[rms_norm.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[rms_norm.py:N]: x_tile = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    v_0 = tl.cast(load, tl.float32)
    # src[rms_norm.py:N]: x_squared = x_tile * x_tile
    v_1 = v_0 * v_0
    # src[rms_norm.py:N]: mean_x_squared = torch.mean(x_squared, dim=-1)
    mean_x_squared_extra = tl.cast(tl.sum(v_1, 1), tl.float32)
    v_2 = 256
    v_3 = mean_x_squared_extra / v_2.to(tl.float32)
    # src[rms_norm.py:N]: inv_rms_tile = torch.rsqrt(mean_x_squared + eps)
    v_4 = v_3 + eps
    v_5 = tl.rsqrt(v_4)
    # src[rms_norm.py:N]: normalized = x_tile * inv_rms_tile[:, None]
    subscript = v_5[:, None]
    v_6 = v_0 * subscript
    # src[rms_norm.py:N]: out[tile_m, :] = (normalized * weight[:].to(torch.float32)).to(out.dtype)
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_7 = tl.cast(load_1, tl.float32)
    v_8 = v_7[None, :]
    v_9 = v_6 * v_8
    v_10 = tl.cast(v_9, tl.float16)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_10, None)
    # src[rms_norm.py:N]: inv_rms[tile_m] = inv_rms_tile.to(out.dtype)
    v_11 = tl.cast(v_5, tl.float16)
    tl.store(inv_rms + indices_0 * 1, v_11, None)

def rms_norm_fwd(x: torch.Tensor, weight: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs Root Mean Square (RMS) normalization on the input tensor.

    RMS normalization normalizes by the root mean square of the elements:
    output = x / sqrt(mean(x^2) + eps) * weight

    Args:
        x: Input tensor of shape [M, N]
        weight: Scale parameter of shape [N]
        eps: Small constant for numerical stability

    Returns:
        Output tensor of shape [M, N] with RMS normalization applied
        RMS tensor of shape [M, 1] with RMS values for each element
    """
    # src[rms_norm.py:N]: m, n = x.size()
    m, n = x.size()
    # src[rms_norm.py:N]: assert weight.size(0) == n, f"weight size mismatch {weight.size(0)} != {n}"
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    # src[rms_norm.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[rms_norm.py:N]: inv_rms = torch.empty([m], dtype=x.dtype, device=x.device)
    inv_rms = torch.empty([m], dtype=x.dtype, device=x.device)
    # src[rms_norm.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = 256
    # src[rms_norm.py:N]: for tile_m in hl.tile(m):
    # src[rms_norm.py:N]:     x_tile = x[tile_m, :].to(torch.float32)
    # src[rms_norm.py:N-N]: ...
    _launcher(_helion_rms_norm_fwd, (triton.cdiv(128, _BLOCK_SIZE_0),), x, weight, out, inv_rms, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[rms_norm.py:N]: return out, inv_rms.reshape(-1, 1)
    return (out, inv_rms.reshape(-1, 1))

--- assertExpectedJournal(TestExamples.test_segment_reduction)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import helion._testing.segment_reduction as _source_module

@triton.jit
def combine_fn_helion_0(param_0, param_1, param_2, param_3):
    # src[segment_reduction.py:N]: out_vals, _ = hl.associative_scan(combine_fn_helion, tuple_in, dim=0)
    v_0 = param_1 == param_3
    v_1 = param_0 + param_2
    v_2 = tl.where(v_0, v_1, param_2)
    # src[segment_reduction.py:N]: def segmented_reduction_helion(
    # src[segment_reduction.py:N]:     indices: torch.Tensor, input_data: torch.Tensor, num_nodes: int
    # src[segment_reduction.py:N]: ) -> torch.Tensor:
    # src[segment_reduction.py:N-N]: ...
    return (v_2, param_3)

@triton.jit
def _helion_segmented_reduction_helion(input_data, indices, output, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[segment_reduction.py:N]: for tile_e, tile_f in hl.tile([num_elements, num_features]):
    num_blocks_0 = tl.cdiv(1000, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 1000
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[segment_reduction.py:N]: vals = input_data[tile_e, tile_f]
    vals = tl.load(input_data + (indices_0[:, None] * 32 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    # src[segment_reduction.py:N]: idxs = indices[tile_e]
    idxs = tl.load(indices + indices_0 * 1, mask_0, other=0)
    # src[segment_reduction.py:N]: indices, [tile_e.index + 1], extra_mask=tile_e.index < num_elements - 1
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    v_2 = tl.full([], 999, tl.int32)
    v_3 = indices_0 < v_2
    # src[segment_reduction.py:N]: idxs_next = hl.load(
    # src[segment_reduction.py:N]:     indices, [tile_e.index + 1], extra_mask=tile_e.index < num_elements - 1
    # src[segment_reduction.py:N]: )
    idxs_next = tl.load(indices + (indices_0 + 1) * 1, mask_0 & v_3, other=0)
    # src[segment_reduction.py:N]: tuple_in = (vals, idxs.float().unsqueeze(1).expand_as(vals))
    v_4 = tl.cast(idxs, tl.float32)
    unsqueeze = v_4[:, None]
    expand = tl.broadcast_to(unsqueeze, [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    # src[segment_reduction.py:N]: out_vals, _ = hl.associative_scan(combine_fn_helion, tuple_in, dim=0)
    out_vals = tl.associative_scan((vals, expand), 0, combine_fn_helion_0)[0]
    # src[segment_reduction.py:N]: mask = (idxs != idxs_next) | (
    v_5 = idxs != idxs_next
    # src[segment_reduction.py:N]: vals = input_data[tile_e, tile_f]
    _BLOCK_SIZE_0_ = _BLOCK_SIZE_0
    # src[segment_reduction.py:N]: tile_e.index % tile_e.block_size == tile_e.block_size - 1
    v_6 = tl.cast(_BLOCK_SIZE_0_, tl.int32)
    v_7 = indices_0 % v_6
    v_8 = tl.full([], 0, tl.int32)
    v_9 = v_7 != v_8
    v_10 = libdevice.signbit(v_7) != 0 if v_7.dtype is tl.float32 else v_7 < 0
    v_11 = libdevice.signbit(v_6) != 0 if v_6.dtype is tl.float32 else v_6 < 0
    v_12 = v_10 != v_11
    v_13 = v_9 & v_12
    v_14 = v_7 + v_6
    v_15 = tl.where(v_13, v_14, v_7)
    sub = -1 + _BLOCK_SIZE_0
    v_16 = tl.cast(sub, tl.int32)
    v_17 = v_15 == v_16
    # src[segment_reduction.py:N]: mask = (idxs != idxs_next) | (
    # src[segment_reduction.py:N]:     tile_e.index % tile_e.block_size == tile_e.block_size - 1
    # src[segment_reduction.py:N]: )
    v_18 = v_5 | v_17
    # src[segment_reduction.py:N]: segment_vals = torch.where(mask.unsqueeze(1), out_vals, 0.0)
    unsqueeze_1 = v_18[:, None]
    v_19 = 0.0
    v_20 = v_19[None, None]
    v_21 = tl.where(unsqueeze_1, out_vals, v_20)
    # src[segment_reduction.py:N]: hl.atomic_add(output, [idxs, tile_f], segment_vals)
    tl.atomic_add(output + (idxs[:, None] * 32 + indices_1[None, :] * 1), v_21, mask=mask_0[:, None], sem='relaxed')

def segmented_reduction_helion(indices: torch.Tensor, input_data: torch.Tensor, num_nodes: int, *, _launcher=_default_launcher):
    """
    Performs segmented reduction using Helion.

    Reduces input data by summing values with the same index.

    Args:
        indices: Tensor of segment indices for each element
        input_data: Input tensor of shape [num_elements, num_features]
        num_nodes: Number of output nodes/segments

    Returns:
        Output tensor of shape [num_nodes, num_features] with reduced values
    """
    # src[segment_reduction.py:N]: num_elements, num_features = input_data.shape
    num_elements, num_features = input_data.shape
    # src[segment_reduction.py:N]: output = torch.zeros(
    # src[segment_reduction.py:N]:     (num_nodes, num_features), dtype=input_data.dtype, device=input_data.device
    # src[segment_reduction.py:N]: )
    output = torch.zeros((num_nodes, num_features), dtype=input_data.dtype, device=input_data.device)
    # src[segment_reduction.py:N]: for tile_e, tile_f in hl.tile([num_elements, num_features]):
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    # src[segment_reduction.py:N]: for tile_e, tile_f in hl.tile([num_elements, num_features]):
    # src[segment_reduction.py:N]:     vals = input_data[tile_e, tile_f]
    # src[segment_reduction.py:N]:     idxs = indices[tile_e]
    # src[segment_reduction.py:N-N]: ...
    _launcher(_helion_segmented_reduction_helion, (triton.cdiv(1000, _BLOCK_SIZE_0) * triton.cdiv(32, _BLOCK_SIZE_1),), input_data, indices, output, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[segment_reduction.py:N]: return output
    return output

--- assertExpectedJournal(TestExamples.test_softmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[softmax.py:N]: out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    amax = tl.cast(tl.reshape(tl.max(load, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_0 = load - amax
    v_1 = libdevice.exp(v_0)
    sum_1 = tl.cast(tl.reshape(tl.sum(v_1, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_2 = v_1 / sum_1
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    # src[softmax.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[softmax.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 1024
    # src[softmax.py:N]: out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    _BLOCK_SIZE_0 = 1
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    _launcher(_helion_softmax, (1024,), x, out, _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_softmax_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_bwd(softmax_output, grad_output, grad_input, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[softmax.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[softmax.py:N]: sum_per_row = hl.zeros([tile_m], dtype=torch.float32)
    sum_per_row = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     sum_per_row += torch.sum(
    # src[softmax.py:N]:         softmax_output[tile_m, tile_n] * grad_output[tile_m, tile_n], dim=1
    # src[softmax.py:N-N]: ...
    for offset_1 in tl.range(0, 2048, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        sum_per_row_copy = sum_per_row
        sum_per_row_copy_0 = sum_per_row_copy
        # src[softmax.py:N]: softmax_output[tile_m, tile_n] * grad_output[tile_m, tile_n], dim=1
        load = tl.load(softmax_output + (indices_0[:, None] * 2048 + indices_1[None, :] * 1), None)
        load_1 = tl.load(grad_output + (indices_0[:, None] * 2048 + indices_1[None, :] * 1), None)
        v_0 = load * load_1
        # src[softmax.py:N]: sum_per_row += torch.sum(
        # src[softmax.py:N]:     softmax_output[tile_m, tile_n] * grad_output[tile_m, tile_n], dim=1
        # src[softmax.py:N]: )
        sum_1 = tl.cast(tl.sum(v_0, 1), tl.float16)
        v_1 = tl.cast(sum_1, tl.float32)
        sum_per_row = sum_per_row_copy_0 + v_1
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     grad_input[tile_m, tile_n] = softmax_output[tile_m, tile_n] * (
    # src[softmax.py:N]:         grad_output[tile_m, tile_n] - sum_per_row[:, None]
    # src[softmax.py:N-N]: ...
    for offset_2 in tl.range(0, 2048, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        sum_per_row_copy_1 = sum_per_row
        sum_per_row_copy_1_0 = sum_per_row_copy_1
        # src[softmax.py:N]: grad_input[tile_m, tile_n] = softmax_output[tile_m, tile_n] * (
        load_2 = tl.load(softmax_output + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None)
        # src[softmax.py:N]: grad_output[tile_m, tile_n] - sum_per_row[:, None]
        load_3 = tl.load(grad_output + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None)
        subscript = sum_per_row_copy_1_0[:, None]
        v_3 = tl.cast(load_3, tl.float32)
        v_4 = v_3 - subscript
        # src[softmax.py:N]: grad_input[tile_m, tile_n] = softmax_output[tile_m, tile_n] * (
        # src[softmax.py:N]:     grad_output[tile_m, tile_n] - sum_per_row[:, None]
        # src[softmax.py:N]: )
        v_5 = tl.cast(load_2, tl.float32)
        v_6 = v_5 * v_4
        v_7 = tl.cast(v_6, tl.float16)
        tl.store(grad_input + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), v_7, None)

def softmax_bwd(grad_output: torch.Tensor, softmax_output: torch.Tensor, *, _launcher=_default_launcher):
    """
    Helion kernel implementing softmax backward pass.

    dy/dx = softmax_output * (grad_output - sum(softmax_output * grad_output))

    Args:
        grad_output (torch.Tensor): Gradient from downstream layers of shape [m, n]
        softmax_output (torch.Tensor): Output from forward softmax pass of shape [m, n]

    Returns:
        torch.Tensor: Gradient with respect to input of shape [m, n]
    """
    # src[softmax.py:N]: m, n = grad_output.size()
    m, n = grad_output.size()
    # src[softmax.py:N]: grad_input = torch.empty_like(grad_output)
    grad_input = torch.empty_like(grad_output)
    # src[softmax.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     sum_per_row += torch.sum(
    # src[softmax.py:N]:         softmax_output[tile_m, tile_n] * grad_output[tile_m, tile_n], dim=1
    # src[softmax.py:N-N]: ...
    _BLOCK_SIZE_1 = 16
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     grad_input[tile_m, tile_n] = softmax_output[tile_m, tile_n] * (
    # src[softmax.py:N]:         grad_output[tile_m, tile_n] - sum_per_row[:, None]
    # src[softmax.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[softmax.py:N]: for tile_m in hl.tile(m):
    # src[softmax.py:N]:     sum_per_row = hl.zeros([tile_m], dtype=torch.float32)
    # src[softmax.py:N]:     for tile_n in hl.tile(n):
    # src[softmax.py:N-N]: ...
    _launcher(_helion_softmax_bwd, (triton.cdiv(2048, _BLOCK_SIZE_0),), softmax_output, grad_output, grad_input, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return grad_input
    return grad_input

--- assertExpectedJournal(TestExamples.test_softmax_decomposed)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_decomposed(x, out, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[softmax.py:N]: values = x[tile_n, :]
    values = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[softmax.py:N]: amax = torch.amax(values, dim=1, keepdim=True)
    amax = tl.cast(tl.reshape(tl.max(values, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[softmax.py:N]: exp = torch.exp(values - amax)
    v_0 = values - amax
    v_1 = libdevice.exp(v_0)
    # src[softmax.py:N]: sum_exp = torch.sum(exp, dim=1, keepdim=True)
    sum_exp = tl.cast(tl.reshape(tl.sum(v_1, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    # src[softmax.py:N]: out[tile_n, :] = exp / sum_exp
    v_2 = v_1 / sum_exp
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax_decomposed(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Helion kernel implementing softmax by decomposing into max, exp, and normalization steps.
    This avoids using PyTorch's built-in softmax decomposition.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    # src[softmax.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[softmax.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 1024
    # src[softmax.py:N]: values = x[tile_n, :]
    _BLOCK_SIZE_0 = 1
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     values = x[tile_n, :]
    # src[softmax.py:N]:     amax = torch.amax(values, dim=1, keepdim=True)
    # src[softmax.py:N-N]: ...
    _launcher(_helion_softmax_decomposed, (1024,), x, out, _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_softmax_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, _REDUCTION_BLOCK_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[softmax.py:N]: out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    amax_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, 1024, _REDUCTION_BLOCK_1):
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, roffset_1], [_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_0 = triton_helpers.maximum(amax_acc, load)
        amax_acc = v_0
    amax = tl.cast(tl.reshape(tl.max(amax_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    sum_1_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, 1024, _REDUCTION_BLOCK_1):
        amax_copy = amax
        load_1 = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, roffset_1], [_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_1 = load_1 - amax_copy
        v_2 = libdevice.exp(v_1)
        v_3 = sum_1_acc + v_2
        sum_1_acc = v_3
    sum_1 = tl.cast(tl.reshape(tl.sum(sum_1_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    for roffset_1 in tl.range(0, 1024, _REDUCTION_BLOCK_1):
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, roffset_1], [_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_4 = load_2 - amax_copy_1
        v_5 = libdevice.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, roffset_1], [_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], [1, 0]), v_6, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    # src[softmax.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[softmax.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[softmax.py:N]: out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    _REDUCTION_BLOCK_1 = 32
    _BLOCK_SIZE_0 = 1
    # src[softmax.py:N]: for tile_n in hl.tile(n):
    # src[softmax.py:N]:     out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)
    _launcher(_helion_softmax, (1024,), x, out, _REDUCTION_BLOCK_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[softmax.py:N]: mi = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    # src[softmax.py:N]: di = hl.zeros([tile_m], dtype=torch.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     local_amax = torch.amax(values, dim=1)
    # src[softmax.py:N-N]: ...
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        # src[softmax.py:N]: values = x[tile_m, tile_n]
        values = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        # src[softmax.py:N]: local_amax = torch.amax(values, dim=1)
        local_amax = tl.cast(tl.max(values, 1), tl.float32)
        # src[softmax.py:N]: mi_next = torch.maximum(mi, local_amax)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        # src[softmax.py:N]: di = di * torch.exp(mi - mi_next) + torch.exp(
        v_1 = mi_copy_0 - v_0
        v_2 = libdevice.exp(v_1)
        v_3 = di_copy_0 * v_2
        # src[softmax.py:N]: values - mi_next[:, None]
        subscript = v_0[:, None]
        v_4 = values - subscript
        # src[softmax.py:N]: di = di * torch.exp(mi - mi_next) + torch.exp(
        # src[softmax.py:N]:     values - mi_next[:, None]
        # src[softmax.py:N]: ).sum(dim=1)
        v_5 = libdevice.exp(v_4)
        sum_1 = tl.cast(tl.sum(v_5, 1), tl.float32)
        di = v_3 + sum_1
        # src[softmax.py:N]: mi = mi_next
        mi = v_0
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        # src[softmax.py:N]: values = x[tile_m, tile_n]
        values_1 = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        # src[softmax.py:N]: out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = libdevice.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(out + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), v_9, None)

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    # src[softmax.py:N]: m, n = x.size()
    m, n = x.size()
    # src[softmax.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    _BLOCK_SIZE_0 = 32
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     local_amax = torch.amax(values, dim=1)
    # src[softmax.py:N-N]: ...
    _BLOCK_SIZE_1 = 32
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    # src[softmax.py:N]:     mi = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    # src[softmax.py:N]:     di = hl.zeros([tile_m], dtype=torch.float32)
    # src[softmax.py:N-N]: ...
    _launcher(_helion_softmax_two_pass, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    # src[softmax.py:N]: mi = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    # src[softmax.py:N]: di = hl.zeros([tile_m], dtype=torch.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     local_amax = torch.amax(values, dim=1)
    # src[softmax.py:N-N]: ...
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_1):
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        # src[softmax.py:N]: values = x[tile_m, tile_n]
        values = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        # src[softmax.py:N]: local_amax = torch.amax(values, dim=1)
        local_amax = tl.cast(tl.max(values, 1), tl.float32)
        # src[softmax.py:N]: mi_next = torch.maximum(mi, local_amax)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        # src[softmax.py:N]: di = di * torch.exp(mi - mi_next) + torch.exp(
        v_1 = mi_copy_0 - v_0
        v_2 = libdevice.exp(v_1)
        v_3 = di_copy_0 * v_2
        # src[softmax.py:N]: values - mi_next[:, None]
        subscript = v_0[:, None]
        v_4 = values - subscript
        # src[softmax.py:N]: di = di * torch.exp(mi - mi_next) + torch.exp(
        # src[softmax.py:N]:     values - mi_next[:, None]
        # src[softmax.py:N]: ).sum(dim=1)
        v_5 = libdevice.exp(v_4)
        sum_1 = tl.cast(tl.sum(v_5, 1), tl.float32)
        di = v_3 + sum_1
        # src[softmax.py:N]: mi = mi_next
        mi = v_0
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_1):
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        # src[softmax.py:N]: values = x[tile_m, tile_n]
        values_1 = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        # src[softmax.py:N]: out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = libdevice.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_9, boundary_check=[0, 1])

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    # src[softmax.py:N]: m, n = x.size()
    m, n = x.size()
    # src[softmax.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    _BLOCK_SIZE_0 = 8
    # src[softmax.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[softmax.py:N]:     values = x[tile_m, tile_n]
    # src[softmax.py:N]:     local_amax = torch.amax(values, dim=1)
    # src[softmax.py:N-N]: ...
    _BLOCK_SIZE_1 = 64
    # src[softmax.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    # src[softmax.py:N]:     mi = hl.full([tile_m], float("-inf"), dtype=torch.float32)
    # src[softmax.py:N]:     di = hl.zeros([tile_m], dtype=torch.float32)
    # src[softmax.py:N-N]: ...
    _launcher(_helion_softmax_two_pass, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[softmax.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_squeeze_and_excitation_net_bwd_da)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_squeeze_and_excitation_net_bwd_da(grad_out, x, d, b, c, grad_a, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_3: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[squeeze_and_excitation_net.py:N]: for tile_n, tile_k in hl.tile([n, k]):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_3 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    # src[squeeze_and_excitation_net.py:N]: acc_a = hl.zeros([tile_n, tile_k], dtype=torch.float32)
    acc_a = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N]:     # Backprop through sigmoid: need full row for matmul with b.T
    # src[squeeze_and_excitation_net.py:N]:     grad_to_d = grad_out[tile_m, :] * x[tile_m, :]
    # src[squeeze_and_excitation_net.py:N-N]: ...
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_a_copy = acc_a
        acc_a_copy_0 = acc_a_copy
        # src[squeeze_and_excitation_net.py:N]: grad_to_d = grad_out[tile_m, :] * x[tile_m, :]
        load = tl.load(grad_out + (indices_2[:, None] * 256 + indices_3[None, :] * 1), None)
        load_1 = tl.load(x + (indices_2[:, None] * 256 + indices_3[None, :] * 1), None)
        v_0 = load * load_1
        # src[squeeze_and_excitation_net.py:N]: grad_to_cb = grad_to_d * d[tile_m, :] * (1.0 - d[tile_m, :])
        load_2 = tl.load(d + (indices_2[:, None] * 256 + indices_3[None, :] * 1), None)
        v_1 = v_0 * load_2
        load_3 = tl.load(d + (indices_2[:, None] * 256 + indices_3[None, :] * 1), None)
        v_2 = 1.0
        v_3 = v_2 - load_3
        v_4 = v_1 * v_3
        # src[squeeze_and_excitation_net.py:N]: grad_to_c = grad_to_cb @ b[tile_k, :].T
        load_4 = tl.load(b + (indices_1[:, None] * 256 + indices_3[None, :] * 1), None)
        permute = tl.permute(load_4, [1, 0])
        grad_to_c = tl.cast(tl.dot(tl.cast(v_4, tl.float16), tl.cast(permute, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
        # src[squeeze_and_excitation_net.py:N]: grad_through_relu = grad_to_c * (c[tile_m, tile_k] > 0)
        load_5 = tl.load(c + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        v_5 = 0.0
        v_6 = load_5 > v_5
        v_7 = tl.cast(v_6, tl.float16)
        v_8 = grad_to_c * v_7
        # src[squeeze_and_excitation_net.py:N]: acc_a = torch.addmm(acc_a, x[tile_m, tile_n].T, grad_through_relu)
        load_6 = tl.load(x + (indices_2[:, None] * 256 + indices_0[None, :] * 1), None)
        permute_1 = tl.permute(load_6, [1, 0])
        acc_a = tl.dot(tl.cast(permute_1, tl.float16), tl.cast(v_8, tl.float16), acc=acc_a_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[squeeze_and_excitation_net.py:N]: grad_a[tile_n, tile_k] = acc_a
    v_9 = tl.cast(acc_a, tl.float16)
    tl.store(grad_a + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_9, None)

def squeeze_and_excitation_net_bwd_da(grad_out: Tensor, x: Tensor, b: Tensor, c: Tensor, d: Tensor, *, _launcher=_default_launcher):
    """
    Compute grad_a for the squeeze and excitation network.
    grad_a = x.T @ (grad_out * x * d * (1-d) @ b.T * (c>0))
    """
    # src[squeeze_and_excitation_net.py:N]: m, n = x.size()
    m, n = x.size()
    # src[squeeze_and_excitation_net.py:N]: k = c.size(1)
    k = c.size(1)
    # src[squeeze_and_excitation_net.py:N]: grad_a = torch.empty([n, k], dtype=x.dtype, device=x.device)
    grad_a = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[squeeze_and_excitation_net.py:N]: for tile_n, tile_k in hl.tile([n, k]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_3 = 256
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N]:     # Backprop through sigmoid: need full row for matmul with b.T
    # src[squeeze_and_excitation_net.py:N]:     grad_to_d = grad_out[tile_m, :] * x[tile_m, :]
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_n, tile_k in hl.tile([n, k]):
    # src[squeeze_and_excitation_net.py:N]:     acc_a = hl.zeros([tile_n, tile_k], dtype=torch.float32)
    # src[squeeze_and_excitation_net.py:N]:     for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _launcher(_helion_squeeze_and_excitation_net_bwd_da, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), grad_out, x, d, b, c, grad_a, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_3, _BLOCK_SIZE_2, num_warps=4, num_stages=2)
    # src[squeeze_and_excitation_net.py:N]: return grad_a
    return grad_a

--- assertExpectedJournal(TestExamples.test_squeeze_and_excitation_net_bwd_db)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_squeeze_and_excitation_net_bwd_db(grad_out, x, d, c, grad_b, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[squeeze_and_excitation_net.py:N]: for tile_k, tile_n in hl.tile([k, n]):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[squeeze_and_excitation_net.py:N]: acc = hl.zeros([tile_k, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N]:     grad_d = (
    # src[squeeze_and_excitation_net.py:N]:         grad_out[tile_m, tile_n]
    # src[squeeze_and_excitation_net.py:N-N]: ...
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[squeeze_and_excitation_net.py:N]: grad_out[tile_m, tile_n]
        load = tl.load(grad_out + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        # src[squeeze_and_excitation_net.py:N]: * x[tile_m, tile_n]
        load_1 = tl.load(x + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        # src[squeeze_and_excitation_net.py:N]: grad_out[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N]: * x[tile_m, tile_n]
        v_0 = load * load_1
        # src[squeeze_and_excitation_net.py:N]: * d[tile_m, tile_n]
        load_2 = tl.load(d + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        # src[squeeze_and_excitation_net.py:N]: grad_out[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N]: * x[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N]: * d[tile_m, tile_n]
        v_1 = v_0 * load_2
        # src[squeeze_and_excitation_net.py:N]: * (1.0 - d[tile_m, tile_n])
        load_3 = tl.load(d + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        v_2 = 1.0
        v_3 = v_2 - load_3
        # src[squeeze_and_excitation_net.py:N]: grad_out[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N]: * x[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N]: * d[tile_m, tile_n]
        # src[squeeze_and_excitation_net.py:N-N]: ...
        v_4 = v_1 * v_3
        # src[squeeze_and_excitation_net.py:N]: acc = torch.addmm(acc, c[tile_m, tile_k].T, grad_d)
        load_4 = tl.load(c + (indices_2[:, None] * 256 + indices_0[None, :] * 1), None)
        permute = tl.permute(load_4, [1, 0])
        acc = tl.dot(tl.cast(permute, tl.float16), tl.cast(v_4, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[squeeze_and_excitation_net.py:N]: grad_b[tile_k, tile_n] = acc
    v_5 = tl.cast(acc, tl.float16)
    tl.store(grad_b + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_5, None)

def squeeze_and_excitation_net_bwd_db(grad_out: Tensor, x: Tensor, d: Tensor, c: Tensor, *, _launcher=_default_launcher):
    """
    Compute grad_b by fusing grad_d computation inline.
    grad_b = c.T @ (grad_out * x * d * (1 - d))
    """
    # src[squeeze_and_excitation_net.py:N]: m, n = grad_out.size()
    m, n = grad_out.size()
    # src[squeeze_and_excitation_net.py:N]: k = c.size(1)
    k = c.size(1)
    # src[squeeze_and_excitation_net.py:N]: grad_b = torch.empty([k, n], dtype=grad_out.dtype, device=grad_out.device)
    grad_b = torch.empty([k, n], dtype=grad_out.dtype, device=grad_out.device)
    # src[squeeze_and_excitation_net.py:N]: for tile_k, tile_n in hl.tile([k, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N]:     grad_d = (
    # src[squeeze_and_excitation_net.py:N]:         grad_out[tile_m, tile_n]
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_k, tile_n in hl.tile([k, n]):
    # src[squeeze_and_excitation_net.py:N]:     acc = hl.zeros([tile_k, tile_n], dtype=torch.float32)
    # src[squeeze_and_excitation_net.py:N]:     for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _launcher(_helion_squeeze_and_excitation_net_bwd_db, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), grad_out, x, d, c, grad_b, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=2)
    # src[squeeze_and_excitation_net.py:N]: return grad_b
    return grad_b

--- assertExpectedJournal(TestExamples.test_squeeze_and_excitation_net_bwd_dx)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_squeeze_and_excitation_net_bwd_dx(grad_out, d, x, b, c, a, grad_x, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_3: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[squeeze_and_excitation_net.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_3 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    # src[squeeze_and_excitation_net.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[squeeze_and_excitation_net.py:N]: acc += grad_out[tile_m, tile_n] * d[tile_m, tile_n]
    load = tl.load(grad_out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    load_1 = tl.load(d + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    v_0 = load * load_1
    v_1 = tl.cast(v_0, tl.float32)
    v_2 = acc + v_1
    # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N]:     # Compute grad_to_d for the full row: shape [tile_m, n]
    # src[squeeze_and_excitation_net.py:N]:     grad_to_d = (
    # src[squeeze_and_excitation_net.py:N-N]: ...
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        v_2_copy = v_2
        v_2_copy_0 = v_2_copy
        # src[squeeze_and_excitation_net.py:N]: grad_out[tile_m, :] * x[tile_m, :] * d[tile_m, :] * (1.0 - d[tile_m, :])
        load_2 = tl.load(grad_out + (indices_0[:, None] * 256 + indices_3[None, :] * 1), None)
        load_3 = tl.load(x + (indices_0[:, None] * 256 + indices_3[None, :] * 1), None)
        v_3 = load_2 * load_3
        load_4 = tl.load(d + (indices_0[:, None] * 256 + indices_3[None, :] * 1), None)
        v_4 = v_3 * load_4
        load_5 = tl.load(d + (indices_0[:, None] * 256 + indices_3[None, :] * 1), None)
        v_5 = 1.0
        v_6 = v_5 - load_5
        v_7 = v_4 * v_6
        # src[squeeze_and_excitation_net.py:N]: grad_to_c = grad_to_d @ b[tile_k, :].T
        load_6 = tl.load(b + (indices_2[:, None] * 256 + indices_3[None, :] * 1), None)
        permute = tl.permute(load_6, [1, 0])
        grad_to_c = tl.cast(tl.dot(tl.cast(v_7, tl.float16), tl.cast(permute, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
        # src[squeeze_and_excitation_net.py:N]: grad_c_masked = grad_to_c * (c[tile_m, tile_k] > 0)
        load_7 = tl.load(c + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None)
        v_8 = 0.0
        v_9 = load_7 > v_8
        v_10 = tl.cast(v_9, tl.float16)
        v_11 = grad_to_c * v_10
        # src[squeeze_and_excitation_net.py:N]: acc = torch.addmm(acc, grad_c_masked, a[tile_n, tile_k].T)
        load_8 = tl.load(a + (indices_1[:, None] * 256 + indices_2[None, :] * 1), None)
        permute_1 = tl.permute(load_8, [1, 0])
        v_2 = tl.dot(tl.cast(v_11, tl.float16), tl.cast(permute_1, tl.float16), acc=v_2_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[squeeze_and_excitation_net.py:N]: grad_x[tile_m, tile_n] = acc
    v_12 = tl.cast(v_2, tl.float16)
    tl.store(grad_x + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_12, None)

def squeeze_and_excitation_net_bwd_dx(grad_out: Tensor, x: Tensor, a: Tensor, b: Tensor, c: Tensor, d: Tensor, *, _launcher=_default_launcher):
    """
    Compute grad_x for the squeeze and excitation network.
    grad_x = grad_out * d + (grad_out * x * d * (1-d) @ b.T * (c>0)) @ a.T

    The computation is structured to properly accumulate over the k dimension:
    1. First term: grad_out * d (element-wise, no reduction)
    2. Second term: chain rule through d->c->x path
       - For each output position (m, n), accumulate over k dimension
       - grad_c[m,k] = (grad_out * x * d * (1-d))[m,:] @ b[k,:].T * (c[m,k] > 0)
       - grad_x[m,n] += grad_c[m,k] @ a[n,k].T
    """
    # src[squeeze_and_excitation_net.py:N]: m, n = x.size()
    m, n = x.size()
    # src[squeeze_and_excitation_net.py:N]: k = a.size(1)
    k = a.size(1)
    # src[squeeze_and_excitation_net.py:N]: grad_x = torch.empty([m, n], dtype=x.dtype, device=x.device)
    grad_x = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[squeeze_and_excitation_net.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_3 = 256
    # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N]:     # Compute grad_to_d for the full row: shape [tile_m, n]
    # src[squeeze_and_excitation_net.py:N]:     grad_to_d = (
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[squeeze_and_excitation_net.py:N]:     # First term: grad_out * d (element-wise)
    # src[squeeze_and_excitation_net.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _launcher(_helion_squeeze_and_excitation_net_bwd_dx, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), grad_out, d, x, b, c, a, grad_x, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_3, _BLOCK_SIZE_2, num_warps=4, num_stages=2)
    # src[squeeze_and_excitation_net.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestExamples.test_squeeze_and_excitation_net_fwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_squeeze_and_excitation_net_fwd(x, a, c, b, d, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr):
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N]:     partial_xa = x[tile_m, :] @ a[:, tile_k]
    # src[squeeze_and_excitation_net.py:N]:     c[tile_m, tile_k] = torch.relu(partial_xa)
    for offset_1 in tl.range(0, 1024, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        # src[squeeze_and_excitation_net.py:N]: partial_xa = x[tile_m, :] @ a[:, tile_k]
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(a + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        partial_xa = tl.cast(tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
        # src[squeeze_and_excitation_net.py:N]: c[tile_m, tile_k] = torch.relu(partial_xa)
        v_0 = tl.full([], 0, tl.int32)
        v_1 = triton_helpers.maximum(v_0, partial_xa)
        tl.store(c + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_1, None)
    # src[squeeze_and_excitation_net.py:N]: for tile_n in hl.tile(n):
    # src[squeeze_and_excitation_net.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[squeeze_and_excitation_net.py:N]:     for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N-N]: ...
    for offset_3 in tl.range(0, 1024, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        # src[squeeze_and_excitation_net.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_3], 0.0, tl.float32)
        # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
        # src[squeeze_and_excitation_net.py:N]:     acc = torch.addmm(acc, c[tile_m, tile_k], b[tile_k, tile_n])
        for offset_4 in tl.range(0, 1024, _BLOCK_SIZE_4):
            indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
            acc_copy = acc
            acc_copy_0 = acc_copy
            # src[squeeze_and_excitation_net.py:N]: acc = torch.addmm(acc, c[tile_m, tile_k], b[tile_k, tile_n])
            load_2 = tl.load(c + (indices_0[:, None] * 1024 + indices_4[None, :] * 1), None)
            load_3 = tl.load(b + (indices_4[:, None] * 1024 + indices_3[None, :] * 1), None)
            acc = tl.dot(tl.cast(load_2, tl.float16), tl.cast(load_3, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
        # src[squeeze_and_excitation_net.py:N]: d[tile_m, tile_n] = torch.sigmoid(acc)
        v_2 = tl.sigmoid(tl.cast(acc, tl.float32))
        v_3 = tl.cast(v_2, tl.float16)
        tl.store(d + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), v_3, None)
        # src[squeeze_and_excitation_net.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] * d[tile_m, tile_n]
        load_4 = tl.load(x + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), None)
        load_5 = tl.load(d + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), None)
        v_4 = load_4 * load_5
        tl.store(out + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), v_4, None)

def squeeze_and_excitation_net_fwd(x: Tensor, a: Tensor, b: Tensor, *, _launcher=_default_launcher):
    """
    Performs torch.mul(x, torch.sigmoid(torch.relu((x @ a)) @ b))
    Args:
        x: 2D tensor of shape [m, n].
        a: 2D tensor of shape [n, k].
        b: 2D tensor of shape [k, n].
    Returns:
        out: Resulting matrix of shape [m, n].
        c = torch.relu(x @ a) of shape [m, k].
        d = torch.sigmoid(c @ b) of shape [m, n].
    """
    # src[squeeze_and_excitation_net.py:N]: m, n = x.size()
    m, n = x.size()
    # src[squeeze_and_excitation_net.py:N]: k = a.size(1)
    k = a.size(1)
    # src[squeeze_and_excitation_net.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[squeeze_and_excitation_net.py:N]: c = torch.empty([m, k], dtype=x.dtype, device=x.device)
    c = torch.empty([m, k], dtype=x.dtype, device=x.device)
    # src[squeeze_and_excitation_net.py:N]: d = torch.empty([m, n], dtype=x.dtype, device=x.device)
    d = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_2 = 1024
    # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N]:     partial_xa = x[tile_m, :] @ a[:, tile_k]
    # src[squeeze_and_excitation_net.py:N]:     c[tile_m, tile_k] = torch.relu(partial_xa)
    _BLOCK_SIZE_1 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_n in hl.tile(n):
    # src[squeeze_and_excitation_net.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[squeeze_and_excitation_net.py:N]:     for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N]:     acc = torch.addmm(acc, c[tile_m, tile_k], b[tile_k, tile_n])
    _BLOCK_SIZE_4 = 16
    # src[squeeze_and_excitation_net.py:N]: for tile_m in hl.tile(m):
    # src[squeeze_and_excitation_net.py:N]:     # Compute c = relu(x @ a) for this tile_m
    # src[squeeze_and_excitation_net.py:N]:     for tile_k in hl.tile(k):
    # src[squeeze_and_excitation_net.py:N-N]: ...
    _launcher(_helion_squeeze_and_excitation_net_fwd, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, a, c, b, d, out, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, _BLOCK_SIZE_4, num_warps=4, num_stages=2)
    # src[squeeze_and_excitation_net.py:N]: return out, c, d
    return (out, c, d)

--- assertExpectedJournal(TestExamples.test_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    # src[sum.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    # src[sum.py:N]: out[tile_m] = x[tile_m, :].sum(-1)
    sum_1_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, 512, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < 512
        load = tl.load(x + (indices_0[:, None] * 512 + rindex_1[None, :] * 1), mask_1[None, :], other=0)
        v_0 = sum_1_acc + load
        sum_1_acc = v_0
    sum_1 = tl.cast(tl.sum(sum_1_acc, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Sums a 2D tensor along the last dimension.

    Args:
        x: Input tensor of shape [M, N]

    Returns:
        Output tensor of shape [M] containing the sum of each row
    """
    # src[sum.py:N]: m, n = x.shape
    m, n = x.shape
    # src[sum.py:N]: out = torch.empty([m], dtype=x.dtype, device=x.device)
    out = torch.empty([m], dtype=x.dtype, device=x.device)
    # src[sum.py:N]: out[tile_m] = x[tile_m, :].sum(-1)
    _BLOCK_SIZE_0 = 1
    _REDUCTION_BLOCK_1 = 32768
    # src[sum.py:N]: for tile_m in hl.tile(m):
    # src[sum.py:N]:     out[tile_m] = x[tile_m, :].sum(-1)
    _launcher(_helion_sum_kernel, (512,), x, out, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    # src[sum.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_swiglu)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_swiglu_fwd(a_flat, b_flat, out_flat, _BLOCK_SIZE_0: tl.constexpr):
    # src[swiglu.py:N]: for tile_idx in hl.tile(total_elements):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[swiglu.py:N]: a_vals = a_flat[tile_idx].to(torch.float32)
    load = tl.load(a_flat + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    # src[swiglu.py:N]: b_vals = b_flat[tile_idx]
    b_vals = tl.load(b_flat + indices_0 * 1, None)
    # src[swiglu.py:N]: sigmoid_a = torch.sigmoid(a_vals)
    v_1 = tl.sigmoid(tl.cast(v_0, tl.float32))
    # src[swiglu.py:N]: silu_a = a_vals * sigmoid_a
    v_2 = v_0 * v_1
    # src[swiglu.py:N]: result = silu_a.to(b_vals.dtype) * b_vals
    v_3 = tl.cast(v_2, tl.float16)
    v_4 = v_3 * b_vals
    # src[swiglu.py:N]: out_flat[tile_idx] = result
    tl.store(out_flat + indices_0 * 1, v_4, None)

def swiglu_fwd(a: Tensor, b: Tensor, *, _launcher=_default_launcher):
    """
    Performs SwiGLU operation: SiLU(a) * b where SiLU is the Swish activation.

    SiLU(a) = a * sigmoid(a) = a / (1 + exp(-a))
    SwiGLU(a, b) = SiLU(a) * b

    Args:
        a (Tensor): Input tensor for SiLU activation of any shape.
        b (Tensor): Input tensor for multiplication, must have same shape as a.

    Returns:
        Tensor: Result of SwiGLU operation with same shape as inputs.
    """
    # src[swiglu.py:N]: assert a.shape == b.shape, (
    # src[swiglu.py:N]:     f"Input tensors must have same shape, got {a.shape} != {b.shape}"
    # src[swiglu.py:N]: )
    assert a.shape == b.shape, f'Input tensors must have same shape, got {a.shape} != {b.shape}'
    # src[swiglu.py:N]: out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    # src[swiglu.py:N]: total_elements = a.numel()
    total_elements = a.numel()
    # src[swiglu.py:N]: a_flat = a.view(-1)
    a_flat = a.view(-1)
    # src[swiglu.py:N]: b_flat = b.view(-1)
    b_flat = b.view(-1)
    # src[swiglu.py:N]: out_flat = out.view(-1)
    out_flat = out.view(-1)
    # src[swiglu.py:N]: for tile_idx in hl.tile(total_elements):
    _BLOCK_SIZE_0 = 16
    # src[swiglu.py:N]: for tile_idx in hl.tile(total_elements):
    # src[swiglu.py:N]:     # Load input values and convert to float32 for computation
    # src[swiglu.py:N]:     a_vals = a_flat[tile_idx].to(torch.float32)
    # src[swiglu.py:N-N]: ...
    _launcher(_helion_swiglu_fwd, (triton.cdiv(1048576, _BLOCK_SIZE_0),), a_flat, b_flat, out_flat, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    # src[swiglu.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_swiglu_bwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_swiglu_bwd(x1_flat, gout_flat, dx2_flat, x2_flat, dx1_flat, _BLOCK_SIZE_0: tl.constexpr):
    # src[swiglu.py:N]: for tile in hl.tile(x1.numel()):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[swiglu.py:N]: x1_vals = x1_flat[tile].to(torch.float32)
    load = tl.load(x1_flat + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    # src[swiglu.py:N]: gout_vals = gout_flat[tile].to(torch.float32)
    load_1 = tl.load(gout_flat + indices_0 * 1, None)
    v_1 = tl.cast(load_1, tl.float32)
    # src[swiglu.py:N]: dx2_vals = x1_vals * torch.sigmoid(x1_vals) * gout_vals
    v_2 = tl.sigmoid(tl.cast(v_0, tl.float32))
    v_3 = v_0 * v_2
    v_4 = v_3 * v_1
    # src[swiglu.py:N]: dx2_flat[tile] = dx2_vals.to(x2.dtype)
    v_5 = tl.cast(v_4, tl.bfloat16)
    tl.store(dx2_flat + indices_0 * 1, v_5, None)
    # src[swiglu.py:N]: x2_vals = x2_flat[tile].to(torch.float32)
    load_2 = tl.load(x2_flat + indices_0 * 1, None)
    v_6 = tl.cast(load_2, tl.float32)
    # src[swiglu.py:N]: x1_exp = torch.exp(x1_vals)
    v_7 = libdevice.exp(v_0)
    # src[swiglu.py:N]: x1_exp_plus1 = x1_exp + 1
    v_8 = 1.0
    v_9 = v_7 + v_8
    # src[swiglu.py:N]: dextra = x1_exp / x1_exp_plus1 + x1_vals * x1_exp / x1_exp_plus1 / x1_exp_plus1
    v_10 = v_7 / v_9
    v_11 = v_0 * v_7
    v_12 = v_11 / v_9
    v_13 = v_12 / v_9
    v_14 = v_10 + v_13
    # src[swiglu.py:N]: dx1_vals = gout_vals * x2_vals * dextra
    v_15 = v_1 * v_6
    v_16 = v_15 * v_14
    # src[swiglu.py:N]: dx1_flat[tile] = dx1_vals.to(x1.dtype)
    v_17 = tl.cast(v_16, tl.bfloat16)
    tl.store(dx1_flat + indices_0 * 1, v_17, None)

def swiglu_bwd(gout: Tensor, x1: Tensor, x2: Tensor, *, _launcher=_default_launcher):
    """
    Implement the backward formula for swiglu.
    """
    # src[swiglu.py:N]: dx1 = torch.empty_like(x1)
    dx1 = torch.empty_like(x1)
    # src[swiglu.py:N]: dx2 = torch.empty_like(x2)
    dx2 = torch.empty_like(x2)
    # src[swiglu.py:N]: gout_flat = gout.view(-1)
    gout_flat = gout.view(-1)
    # src[swiglu.py:N]: x1_flat = x1.view(-1)
    x1_flat = x1.view(-1)
    # src[swiglu.py:N]: x2_flat = x2.view(-1)
    x2_flat = x2.view(-1)
    # src[swiglu.py:N]: dx1_flat = dx1.view(-1)
    dx1_flat = dx1.view(-1)
    # src[swiglu.py:N]: dx2_flat = dx2.view(-1)
    dx2_flat = dx2.view(-1)
    # src[swiglu.py:N]: for tile in hl.tile(x1.numel()):
    _BLOCK_SIZE_0 = 32
    # src[swiglu.py:N]: for tile in hl.tile(x1.numel()):
    # src[swiglu.py:N]:     x1_vals = x1_flat[tile].to(torch.float32)
    # src[swiglu.py:N]:     gout_vals = gout_flat[tile].to(torch.float32)
    # src[swiglu.py:N-N]: ...
    _launcher(_helion_swiglu_bwd, (triton.cdiv(1024, _BLOCK_SIZE_0),), x1_flat, gout_flat, dx2_flat, x2_flat, dx1_flat, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[swiglu.py:N]: return dx1, dx2
    return (dx1, dx2)

--- assertExpectedJournal(TestExamples.test_template_via_closure0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    load_2 = tl.load(epilogue_closure_0 + indices_1[None, :] * 1, None)
    v_0 = tl.cast(load_2, tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = tl.cast(v_3, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_4, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _BLOCK_SIZE_2 = 16
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    # src[matmul.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure1)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    load_2 = tl.load(tl.make_block_ptr(epilogue_closure_0, [1, 1024], [1024, 1], [0, offset_1], [1, _BLOCK_SIZE_1], [1, 0]), boundary_check=[1], padding_option='zero')
    v_0 = tl.cast(load_2, tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = tl.cast(v_3, tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_4, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _BLOCK_SIZE_2 = 16
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    # src[matmul.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    v_0 = tl.full([], 0, tl.int32)
    v_1 = triton_helpers.maximum(v_0, acc)
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _BLOCK_SIZE_2 = 16
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    # src[matmul.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_welford)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_welford(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[welford.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[welford.py:N]: acc_cnt = torch.zeros_like(x[tile_m, 0], dtype=torch.float32)
    acc_cnt = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    # src[welford.py:N]: acc_mean = torch.zeros_like(acc_cnt)
    acc_mean = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    # src[welford.py:N]: acc_m2 = torch.zeros_like(acc_cnt)
    acc_m2 = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    # src[welford.py:N]: for tile_n in hl.tile(n):
    # src[welford.py:N]:     chunk = x[tile_m, tile_n]
    # src[welford.py:N]:     Tn = chunk.size(-1)
    # src[welford.py:N-N]: ...
    for offset_1 in tl.range(0, 1024, _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        acc_mean_copy = acc_mean
        acc_cnt_copy = acc_cnt
        acc_m2_copy = acc_m2
        acc_mean_copy_0 = acc_mean_copy
        acc_cnt_copy_0 = acc_cnt_copy
        acc_m2_copy_0 = acc_m2_copy
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        chunk = tl.load(x + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), None)
        # src[welford.py:N]: sum_x = torch.sum(chunk, dim=-1)
        sum_x = tl.cast(tl.sum(chunk, 1), tl.float32)
        # src[welford.py:N]: sum_x2 = torch.sum(chunk * chunk, dim=-1)
        v_0 = chunk * chunk
        sum_x2 = tl.cast(tl.sum(v_0, 1), tl.float32)
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        _BLOCK_SIZE_1_ = _BLOCK_SIZE_1
        # src[welford.py:N]: mean_c = sum_x / Tn
        v_1 = tl.cast(_BLOCK_SIZE_1_, tl.float32)
        v_2 = sum_x / v_1
        # src[welford.py:N]: m2_c = sum_x2 - (sum_x * sum_x) / Tn
        v_3 = sum_x * sum_x
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        _BLOCK_SIZE_1__1 = _BLOCK_SIZE_1
        # src[welford.py:N]: m2_c = sum_x2 - (sum_x * sum_x) / Tn
        v_4 = tl.cast(_BLOCK_SIZE_1__1, tl.float32)
        v_5 = v_3 / v_4
        v_6 = sum_x2 - v_5
        # src[welford.py:N]: delta = mean_c - acc_mean
        v_7 = v_2 - acc_mean_copy_0
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        _BLOCK_SIZE_1__2 = _BLOCK_SIZE_1
        # src[welford.py:N]: new_cnt = acc_cnt + Tn
        v_8 = tl.cast(_BLOCK_SIZE_1__2, tl.float32)
        acc_cnt = acc_cnt_copy_0 + v_8
        # src[welford.py:N]: new_mean = acc_mean + delta * (Tn / new_cnt)
        v_10 = tl.full([], 1, tl.int32)
        v_11 = v_10 / acc_cnt
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        _BLOCK_SIZE_1__3 = _BLOCK_SIZE_1
        # src[welford.py:N]: new_mean = acc_mean + delta * (Tn / new_cnt)
        v_12 = tl.cast(_BLOCK_SIZE_1__3, tl.float32)
        v_13 = v_11 * v_12
        v_14 = v_7 * v_13
        acc_mean = acc_mean_copy_0 + v_14
        # src[welford.py:N]: new_m2 = acc_m2 + m2_c + delta * delta * (acc_cnt * Tn / new_cnt)
        v_16 = acc_m2_copy_0 + v_6
        v_17 = v_7 * v_7
        # src[welford.py:N]: chunk = x[tile_m, tile_n]
        _BLOCK_SIZE_1__4 = _BLOCK_SIZE_1
        # src[welford.py:N]: new_m2 = acc_m2 + m2_c + delta * delta * (acc_cnt * Tn / new_cnt)
        v_18 = tl.cast(_BLOCK_SIZE_1__4, tl.float32)
        v_19 = acc_cnt_copy_0 * v_18
        v_20 = v_19 / acc_cnt
        v_21 = v_17 * v_20
        acc_m2 = v_16 + v_21
    # src[welford.py:N]: rstd_tile = torch.rsqrt(acc_m2 / acc_cnt + eps)
    v_23 = acc_m2 / acc_cnt
    v_24 = v_23 + eps
    v_25 = tl.rsqrt(v_24)
    # src[welford.py:N]: mean_col = acc_mean[:, None]
    mean_col = acc_mean[:, None]
    # src[welford.py:N]: rstd_col = rstd_tile[:, None]
    rstd_col = v_25[:, None]
    # src[welford.py:N]: for tile_n in hl.tile(n):
    # src[welford.py:N]:     xi_chuck = x[tile_m, tile_n]
    # src[welford.py:N]:     w_chuck = weight[tile_n][None, :]
    # src[welford.py:N-N]: ...
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mean_col_copy = mean_col
        rstd_col_copy = rstd_col
        mean_col_copy_0 = mean_col_copy
        rstd_col_copy_0 = rstd_col_copy
        # src[welford.py:N]: xi_chuck = x[tile_m, tile_n]
        xi_chuck = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        # src[welford.py:N]: w_chuck = weight[tile_n][None, :]
        load_1 = tl.load(weight + indices_2 * 1, None)
        w_chuck = load_1[None, :]
        # src[welford.py:N]: b_chuck = bias[tile_n][None, :]
        load_2 = tl.load(bias + indices_2 * 1, None)
        b_chuck = load_2[None, :]
        # src[welford.py:N]: y = (xi_chuck - mean_col) * rstd_col
        v_26 = xi_chuck - mean_col_copy_0
        v_27 = v_26 * rstd_col_copy_0
        # src[welford.py:N]: y = y * w_chuck + b_chuck
        v_28 = v_27 * w_chuck
        v_29 = v_28 + b_chuck
        # src[welford.py:N]: out[tile_m, tile_n] = y.to(x.dtype)
        tl.store(out + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), v_29, None)

def welford(weight: torch.Tensor, bias: torch.Tensor, x: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Applies LayerNorm using Welford's algorithm for mean/variance.
    Args:
        weight: weight tensor of shape [N]
        bias: bias tensor of shape [N]
        x: input tensor of shape [M, N]
    Returns:
        Output tensor of shape [M, N]
    """
    # src[welford.py:N]: m, n = x.size()
    m, n = x.size()
    # src[welford.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[welford.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    # src[welford.py:N]: for tile_n in hl.tile(n):
    # src[welford.py:N]:     chunk = x[tile_m, tile_n]
    # src[welford.py:N]:     Tn = chunk.size(-1)
    # src[welford.py:N-N]: ...
    _BLOCK_SIZE_1 = 16
    # src[welford.py:N]: for tile_n in hl.tile(n):
    # src[welford.py:N]:     xi_chuck = x[tile_m, tile_n]
    # src[welford.py:N]:     w_chuck = weight[tile_n][None, :]
    # src[welford.py:N-N]: ...
    _BLOCK_SIZE_2 = 16
    # src[welford.py:N]: for tile_m in hl.tile(m):
    # src[welford.py:N]:     acc_cnt = torch.zeros_like(x[tile_m, 0], dtype=torch.float32)
    # src[welford.py:N]:     acc_mean = torch.zeros_like(acc_cnt)
    # src[welford.py:N-N]: ...
    _launcher(_helion_welford, (triton.cdiv(128, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[welford.py:N]: return out
    return out
