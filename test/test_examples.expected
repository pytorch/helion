This file is automatically generated by assertExpectedJournal calls in test_examples.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestExamples.test_matmul_addmm_epilogue_subtiling_subtile_size_2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(64)
_BLOCK_SIZE_1 = tl.constexpr(64)
_BLOCK_SIZE_2 = tl.constexpr(32)

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _SHAPE_DIM: tl.constexpr, _SHAPE_DIM_1: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    load_2 = tl.load(epilogue_closure_0 + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), None)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    reshape_default = tl.reshape(acc, [_BLOCK_SIZE_0, 2, _SHAPE_DIM])
    permute_default = tl.permute(reshape_default, [0, 2, 1])
    split = tl.split(permute_default)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    reshape_default_1 = tl.reshape(load_2, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_1])
    permute_default_1 = tl.permute(reshape_default_1, [0, 2, 1])
    split_1 = tl.split(permute_default_1)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_1 = split[0]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_3 = split_1[0]
    v_0 = tl.cast(getitem_3, tl.float32)
    v_1 = getitem_1 + v_0
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 0 * (_BLOCK_SIZE_1 // 2) + tl.arange(0, _BLOCK_SIZE_1 // 2).to(tl.int32))[None, :] * 1), v_2, (offset_1 + 0 * (_BLOCK_SIZE_1 // 2) + tl.arange(0, _BLOCK_SIZE_1 // 2) < 1024)[None, :])
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_2 = split[1]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_4 = split_1[1]
    v_3 = tl.cast(getitem_4, tl.float32)
    v_4 = getitem_2 + v_3
    v_5 = tl.cast(v_4, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 1 * (_BLOCK_SIZE_1 // 2) + tl.arange(0, _BLOCK_SIZE_1 // 2).to(tl.int32))[None, :] * 1), v_5, (offset_1 + 1 * (_BLOCK_SIZE_1 // 2) + tl.arange(0, _BLOCK_SIZE_1 // 2) < 1024)[None, :])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _SHAPE_DIM = _BLOCK_SIZE_1 // 2
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    _SHAPE_DIM_1 = _BLOCK_SIZE_1 // 2
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _SHAPE_DIM, _SHAPE_DIM_1, num_warps=4, num_stages=3)
    # src[matmul.py:N]: return out
    return out

--- assertExpectedJournal(TestExamples.test_matmul_addmm_epilogue_subtiling_subtile_size_4)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(64)
_BLOCK_SIZE_1 = tl.constexpr(64)
_BLOCK_SIZE_2 = tl.constexpr(32)

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _SHAPE_DIM: tl.constexpr, _SHAPE_DIM_1: tl.constexpr, _SHAPE_DIM_2: tl.constexpr, _SHAPE_DIM_3: tl.constexpr, _SHAPE_DIM_4: tl.constexpr, _SHAPE_DIM_5: tl.constexpr):
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[matmul.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[matmul.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    load_2 = tl.load(epilogue_closure_0 + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), None)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    reshape_default = tl.reshape(acc, [_BLOCK_SIZE_0, 2, _SHAPE_DIM])
    permute_default = tl.permute(reshape_default, [0, 2, 1])
    split = tl.split(permute_default)
    getitem_1 = split[0]
    getitem_2 = split[1]
    reshape_default_1 = tl.reshape(getitem_1, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_1])
    permute_default_1 = tl.permute(reshape_default_1, [0, 2, 1])
    split_1 = tl.split(permute_default_1)
    reshape_default_2 = tl.reshape(getitem_2, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_2])
    permute_default_2 = tl.permute(reshape_default_2, [0, 2, 1])
    split_2 = tl.split(permute_default_2)
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    reshape_default_3 = tl.reshape(load_2, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_3])
    permute_default_3 = tl.permute(reshape_default_3, [0, 2, 1])
    split_3 = tl.split(permute_default_3)
    getitem_7 = split_3[0]
    getitem_8 = split_3[1]
    reshape_default_4 = tl.reshape(getitem_7, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_4])
    permute_default_4 = tl.permute(reshape_default_4, [0, 2, 1])
    split_4 = tl.split(permute_default_4)
    reshape_default_5 = tl.reshape(getitem_8, [_BLOCK_SIZE_0, 2, _SHAPE_DIM_5])
    permute_default_5 = tl.permute(reshape_default_5, [0, 2, 1])
    split_5 = tl.split(permute_default_5)
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_3 = split_1[0]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_9 = split_4[0]
    v_0 = tl.cast(getitem_9, tl.float32)
    v_1 = getitem_3 + v_0
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 0 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4).to(tl.int32))[None, :] * 1), v_2, (offset_1 + 0 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4) < 1024)[None, :])
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_4 = split_1[1]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_10 = split_4[1]
    v_3 = tl.cast(getitem_10, tl.float32)
    v_4 = getitem_4 + v_3
    v_5 = tl.cast(v_4, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 1 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4).to(tl.int32))[None, :] * 1), v_5, (offset_1 + 1 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4) < 1024)[None, :])
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_5 = split_2[0]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_11 = split_5[0]
    v_6 = tl.cast(getitem_11, tl.float32)
    v_7 = getitem_5 + v_6
    v_8 = tl.cast(v_7, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 2 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4).to(tl.int32))[None, :] * 1), v_8, (offset_1 + 2 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4) < 1024)[None, :])
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    getitem_6 = split_2[1]
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    getitem_12 = split_5[1]
    v_9 = tl.cast(getitem_12, tl.float32)
    v_10 = getitem_6 + v_9
    v_11 = tl.cast(v_10, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + (offset_1 + 3 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4).to(tl.int32))[None, :] * 1), v_11, (offset_1 + 3 * (_BLOCK_SIZE_1 // 4) + tl.arange(0, _BLOCK_SIZE_1 // 4) < 1024)[None, :])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    # src[matmul.py:N]: m, k = x.size()
    m, k = x.size()
    # src[matmul.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[matmul.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[matmul.py:N]: out = torch.empty(
    # src[matmul.py:N]:     [m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device
    # src[matmul.py:N]: )
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    # src[matmul.py:N]: for tile_k in hl.tile(k):
    # src[matmul.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _SHAPE_DIM = _BLOCK_SIZE_1 // 2
    _SHAPE_DIM_1 = _BLOCK_SIZE_1 // 4
    _SHAPE_DIM_2 = _BLOCK_SIZE_1 // 4
    # src[matmul.py:N]: out[tile_m, tile_n] = epilogue(acc, (tile_m, tile_n))
    _SHAPE_DIM_3 = _BLOCK_SIZE_1 // 2
    _SHAPE_DIM_4 = _BLOCK_SIZE_1 // 4
    _SHAPE_DIM_5 = _BLOCK_SIZE_1 // 4
    # src[matmul.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[matmul.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[matmul.py:N]:     for tile_k in hl.tile(k):
    # src[matmul.py:N-N]: ...
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _SHAPE_DIM, _SHAPE_DIM_1, _SHAPE_DIM_2, _SHAPE_DIM_3, _SHAPE_DIM_4, _SHAPE_DIM_5, num_warps=4, num_stages=3)
    # src[matmul.py:N]: return out
    return out
