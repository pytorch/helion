This file is automatically generated by assertExpectedJournal calls in test_tensor_descriptor.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestTensorDescriptor.test_attention_td_dynamic)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, q_in_size_1, k_view_stride_0, k_view_stride_1, k_view_stride_2, out_stride_0, out_stride_1, out_stride_2, q_view_stride_0, q_view_stride_1, q_view_stride_2, v_view_stride_0, v_view_stride_1, v_view_stride_2, m_dim, n_dim, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    num_blocks_0 = q_in_size_1
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m_dim
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
    l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
    # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
    q = tl.load(q_view + (indices_0[:, None, None] * q_view_stride_0 + indices_1[None, :, None] * q_view_stride_1 + indices_4[None, None, :] * q_view_stride_2), mask_1[None, :, None], other=0)
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    for offset_2 in tl.range(0, n_dim.to(tl.int32), _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_2 < n_dim
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
        k = tl.load(k_view + (indices_0[:, None, None] * k_view_stride_0 + indices_4[None, :, None] * k_view_stride_1 + indices_2[None, None, :] * k_view_stride_2), mask_3[None, None, :], other=0)
        # src[attention.py:N]: qk = torch.bmm(q, k)
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float32), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float32), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        _mask_to_2 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), qk, tl.full([], float('-inf'), tl.float32))
        amax = tl.cast(tl.max(_mask_to_2, 2), tl.float32)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        # src[attention.py:N]: p = torch.exp2(qk)
        v_6 = libdevice.exp2(v_5)
        # src[attention.py:N]: l_ij = torch.sum(p, -1)
        _mask_to_3 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), v_6, tl.full([], 0, tl.float32))
        l_ij = tl.cast(tl.sum(_mask_to_3, 2), tl.float32)
        # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        # src[attention.py:N]: l_i = l_i * alpha + l_ij
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        # src[attention.py:N]: acc = acc * alpha[:, :, None]
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
        v = tl.load(v_view + (indices_0[:, None, None] * v_view_stride_0 + indices_2[None, :, None] * v_view_stride_1 + indices_4[None, None, :] * v_view_stride_2), mask_3[None, :, None], other=0)
        # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(_mask_to_3, tl.float32), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float32), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64])
        # src[attention.py:N]: m_i = m_ij
        m_i = v_2
    # src[attention.py:N]: acc = acc / l_i[:, :, None]
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
    tl.store(out + (indices_0[:, None, None] * out_stride_0 + indices_1[None, :, None] * out_stride_1 + indices_4[None, None, :] * out_stride_2), v_12, mask_1[None, :, None])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_2 = 64
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (q_in.size(1) * triton.cdiv(m_dim, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, q_in.size(1), k_view.stride(0), k_view.stride(1), k_view.stride(2), out.stride(0), out.stride(1), out.stride(2), q_view.stride(0), q_view.stride(1), q_view.stride(2), v_view.stride(0), v_view.stride(1), v_view.stride(2), m_dim, n_dim, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())

--- assertExpectedJournal(TestTensorDescriptor.test_attention_tensor_descriptor)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    num_blocks_0 = 64
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    m_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    # src[attention.py:N]: l_i = torch.full_like(m_i, 1.0)
    l_i = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 1.0, tl.float32)
    # src[attention.py:N]: acc = hl.zeros([tile_b, tile_m, head_dim], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    # src[attention.py:N]: q = q_view[tile_b, tile_m, :]
    q = tl.load(q_view + (indices_0[:, None, None] * 65536 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        # src[attention.py:N]: k = k_view[tile_b, :, tile_n]
        k = tl.load(k_view + (indices_0[:, None, None] * 32768 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        # src[attention.py:N]: qk = torch.bmm(q, k)
        qk = tl.cast(tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float16), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float16), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.float16)
        # src[attention.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        amax = tl.cast(tl.max(qk, 2), tl.float16)
        v_0 = 0.18033688
        v_1 = tl.cast(amax * v_0, tl.float16)
        v_2 = tl.cast(v_1, tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        # src[attention.py:N]: qk = qk * qk_scale - m_ij[:, :, None]
        v_4 = 0.18033688
        v_5 = tl.cast(qk * v_4, tl.float16)
        subscript = v_3[:, :, None]
        v_6 = tl.cast(v_5, tl.float32)
        v_7 = v_6 - subscript
        # src[attention.py:N]: p = torch.exp2(qk)
        v_8 = libdevice.exp2(v_7)
        # src[attention.py:N]: l_ij = torch.sum(p, -1)
        l_ij = tl.cast(tl.sum(v_8, 2), tl.float32)
        # src[attention.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        # src[attention.py:N]: l_i = l_i * alpha + l_ij
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        # src[attention.py:N]: acc = acc * alpha[:, :, None]
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        # src[attention.py:N]: v = v_view[tile_b, tile_n, :]
        v = tl.load(v_view + (indices_0[:, None, None] * 32768 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        # src[attention.py:N]: p = p.to(v.dtype)
        v_14 = tl.cast(v_8, tl.float16)
        # src[attention.py:N]: acc = torch.baddbmm(acc, p, v)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(v_14, tl.float16), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float16), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, 64])
        # src[attention.py:N]: m_i = m_ij
        m_i = v_3
    # src[attention.py:N]: acc = acc / l_i[:, :, None]
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    # src[attention.py:N]: out[tile_b, tile_m, :] = acc.to(out.dtype)
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None, None] * 65536 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    # src[attention.py:N]: m_dim = q_in.size(-2)
    m_dim = q_in.size(-2)
    # src[attention.py:N]: n_dim = k_in.size(-2)
    n_dim = k_in.size(-2)
    # src[attention.py:N]: assert n_dim == v_in.size(-2)
    assert n_dim == v_in.size(-2)
    # src[attention.py:N]: head_dim = hl.specialize(q_in.size(-1))
    head_dim = 64
    # src[attention.py:N]: assert head_dim == k_in.size(-1) == v_in.size(-1)
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    # src[attention.py:N]: q_view = q_in.reshape([-1, m_dim, head_dim])
    q_view = q_in.reshape([-1, m_dim, head_dim])
    # src[attention.py:N]: v_view = v_in.reshape([-1, n_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    # src[attention.py:N]: k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    # src[attention.py:N]: out = torch.empty_like(q_view)
    out = torch.empty_like(q_view)
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    _BLOCK_SIZE_1 = 128
    _RDIM_SIZE_2 = 64
    # src[attention.py:N]: m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    _BLOCK_SIZE_0 = 1
    # src[attention.py:N]: for tile_n in hl.tile(v_view.size(1)):
    # src[attention.py:N]:     k = k_view[tile_b, :, tile_n]
    # src[attention.py:N]:     qk = torch.bmm(q, k)
    # src[attention.py:N-N]: ...
    _BLOCK_SIZE_3 = 64
    # src[attention.py:N]: for tile_b, tile_m in hl.tile([q_view.size(0), m_dim]):
    # src[attention.py:N]:     m_i = hl.full([tile_b, tile_m], float("-inf"), dtype=torch.float32)
    # src[attention.py:N]:     l_i = torch.full_like(m_i, 1.0)
    # src[attention.py:N-N]: ...
    _launcher(_helion_attention, (64 * triton.cdiv(1024, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_3, num_warps=4, num_stages=1)
    # src[attention.py:N]: return out.view(q_in.size())
    return out.view(q_in.size())
