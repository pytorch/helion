This file is automatically generated by assertExpectedJournal calls in test_builtin_min_max.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestBuiltinMinMax.test_gdn_kernel_matches_reference)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_helion_gdn_fwd_h_kernel(g_c, out):
    # src[test_builtin_min_max.py:N]: for chunk in hl.grid(nchunks):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[test_builtin_min_max.py:N]: last_idx = min((chunk + 1) * chunk_size, seqlen) - 1
    sub = -1 + (6 * (6 <= 2 + 2 * offset_0) + (2 + 2 * offset_0) * (2 + 2 * offset_0 < 6))
    # src[test_builtin_min_max.py:N]: out[chunk] = g_c[last_idx // chunk_size, last_idx % chunk_size]
    floordiv = triton_helpers.div_floor_integer(-1 + (6 * (6 <= 2 + 2 * offset_0) + (2 + 2 * offset_0) * (2 + 2 * offset_0 < 6)), 2)
    load = tl.load(g_c + (floordiv * 2 + 1 * 1), None)
    tl.store(out + offset_0 * 1, load, None)

def helion_gdn_fwd_h_kernel(g_c, *, _launcher=_default_launcher):
    # src[test_builtin_min_max.py:N]: nchunks, chunk_size = g_c.shape
    nchunks, chunk_size = g_c.shape
    # src[test_builtin_min_max.py:N]: out = torch.zeros(nchunks, dtype=g_c.dtype, device=g_c.device)
    out = torch.zeros(nchunks, dtype=g_c.dtype, device=g_c.device)
    # src[test_builtin_min_max.py:N]: for chunk in hl.grid(nchunks):
    # src[test_builtin_min_max.py:N]:     last_idx = min((chunk + 1) * chunk_size, seqlen) - 1
    # src[test_builtin_min_max.py:N]:     out[chunk] = g_c[last_idx // chunk_size, last_idx % chunk_size]
    _launcher(_helion_helion_gdn_fwd_h_kernel, (3,), g_c, out, num_warps=4, num_stages=1)
    # src[test_builtin_min_max.py:N]: return out
    return out

--- assertExpectedJournal(TestBuiltinMinMax.test_max_kernel_matches_reference)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_helion_max_kernel(x_c, out):
    # src[test_builtin_min_max.py:N]: for chunk in hl.grid(nchunks):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    # src[test_builtin_min_max.py:N]: out[chunk] = x_c[last_idx // chunk_size, last_idx % chunk_size]
    floordiv = (5 * (5 >= 2 * offset_0) + 2 * offset_0 * (2 * offset_0 > 5)) // 2
    mod = (5 * (5 >= 2 * offset_0) + 2 * offset_0 * (2 * offset_0 > 5)) % 2
    load = tl.load(x_c + (floordiv * 2 + mod * 1), None)
    tl.store(out + offset_0 * 1, load, None)

def helion_max_kernel(x_c, *, _launcher=_default_launcher):
    # src[test_builtin_min_max.py:N]: nchunks, chunk_size = x_c.shape
    nchunks, chunk_size = x_c.shape
    # src[test_builtin_min_max.py:N]: out = torch.zeros(nchunks, dtype=x_c.dtype, device=x_c.device)
    out = torch.zeros(nchunks, dtype=x_c.dtype, device=x_c.device)
    # src[test_builtin_min_max.py:N]: for chunk in hl.grid(nchunks):
    # src[test_builtin_min_max.py:N]:     first_idx = chunk * chunk_size
    # src[test_builtin_min_max.py:N]:     last_idx = max(first_idx, seqlen - 1)
    # src[test_builtin_min_max.py:N-N]: ...
    _launcher(_helion_helion_max_kernel, (3,), x_c, out, num_warps=4, num_stages=1)
    # src[test_builtin_min_max.py:N]: return out
    return out
