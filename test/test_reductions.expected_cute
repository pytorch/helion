This file is automatically generated by assertExpectedJournal calls in test_reductions.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestReductions.test_argmax_on_tile_after_matmul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_argmax(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_reductions.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[test_reductions.py:N]: for tile_k in hl.tile(k):
    # src[test_reductions.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 64, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_reductions.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 64 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 64 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[test_reductions.py:N]: out[tile_m] = acc.argmax(dim=1)
    argmax = tl.cast(triton_helpers.max_with_index(acc, tl.broadcast_to(indices_1[None, :], [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), 1)[1].to(tl.int64), tl.int64)
    tl.store(out + indices_0 * 1, argmax, None)

def matmul_argmax(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, k = x.size()
    m, k = x.size()
    # src[test_reductions.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[test_reductions.py:N]: assert k == k2, f"size mismatch {k} != {k2}"
    assert k == k2, f'size mismatch {k} != {k2}'
    # src[test_reductions.py:N]: out = torch.empty([m], dtype=torch.int64, device=x.device)
    out = torch.empty([m], dtype=torch.int64, device=x.device)
    # src[test_reductions.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_reductions.py:N]: for tile_k in hl.tile(k):
    # src[test_reductions.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    _BLOCK_SIZE_2 = 16
    # src[test_reductions.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_reductions.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[test_reductions.py:N]:     for tile_k in hl.tile(k):
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_matmul_argmax, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_argmin_argmax)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 16

@cute.kernel
def _helion_reduce_kernel(x, out, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 512
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    argmax_acc = cutlass.Float32(float('-inf'))
    argmax_acc_index = cutlass.Int32(2147483647)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(512), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float32(0)
        _mask_to = load if mask_0 else cutlass.Float32(float('-inf'))
        argmax_acc, argmax_acc_index = (_mask_to, rindex_1) if (_mask_to > argmax_acc) | (_mask_to == argmax_acc) & (rindex_1 < argmax_acc_index) else (argmax_acc, argmax_acc_index)
    argmax = cutlass.Int64(cutlass.Int64(cute.arch.warp_reduction(argmax_acc_index if argmax_acc == cute.arch.warp_reduction_max(argmax_acc, threads_in_group=32) else cutlass.Int32(2147483647), lambda a, b: a if a < b else b, threads_in_group=32)))
    out.__setitem__((indices_0,), argmax) if mask_0 else None

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 16
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    _REDUCTION_BLOCK_1 = 32
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, ((512 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, _REDUCTION_BLOCK_1, block=(32, 16, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_argmin_argmax_looped)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 1

@cute.kernel
def _helion_reduce_kernel(x, out, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 512
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    argmax_acc = cutlass.Float32(float('-inf'))
    argmax_acc_index = cutlass.Int32(2147483647)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(512), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float32(0)
        _mask_to = load if mask_0 else cutlass.Float32(float('-inf'))
        argmax_acc, argmax_acc_index = (_mask_to, rindex_1) if (_mask_to > argmax_acc) | (_mask_to == argmax_acc) & (rindex_1 < argmax_acc_index) else (argmax_acc, argmax_acc_index)
    argmax = cutlass.Int64(cutlass.Int64(cute.arch.warp_reduction(argmax_acc_index if argmax_acc == cute.arch.warp_reduction_max(argmax_acc, threads_in_group=16) else cutlass.Int32(2147483647), lambda a, b: a if a < b else b, threads_in_group=16)))
    out.__setitem__((indices_0,), argmax) if mask_0 else None

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 1
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    _REDUCTION_BLOCK_1 = 16
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, ((512 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, _REDUCTION_BLOCK_1, block=(16, 1, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_broken_layernorm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    acc = tl.load(x + (indices_0[:, None] * 2 + indices_1[None, :] * 1), None)
    # src[test_reductions.py:N]: mean = hl.full([n], 0.0, acc.dtype)
    mean = tl.full([2], 0.0, tl.float32)
    # src[test_reductions.py:N]: count = hl.arange(0, acc.shape[1], 1)
    count = tl.arange(0, 2)
    # src[test_reductions.py:N]: delta = acc - mean
    v_0 = mean[None, :]
    v_1 = acc - v_0
    # src[test_reductions.py:N]: mean = delta / count[None, :]
    subscript = count[None, :]
    v_2 = tl.cast(subscript, tl.float32)
    v_3 = v_1 / v_2
    # src[test_reductions.py:N]: delta2 = acc - mean.sum(-1)[:, None]
    sum_1 = tl.cast(tl.sum(v_3, 1), tl.float32)
    subscript_1 = sum_1[:, None]
    v_4 = acc - subscript_1
    # src[test_reductions.py:N]: m2 = delta * delta2
    v_5 = v_1 * v_4
    # src[test_reductions.py:N]: var = m2 / n
    v_6 = 0.5
    v_7 = v_5 * v_6
    # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
    v_8 = acc - v_3
    v_9 = v_7 + eps
    v_10 = tl.rsqrt(v_9)
    v_11 = v_8 * v_10
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_12 = load_1[None, :]
    v_13 = v_11 * v_12
    # src[test_reductions.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, None)
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    # src[test_reductions.py:N]:     bias[:].to(torch.float32)
    # src[test_reductions.py:N]: )
    v_14 = load_2[None, :]
    v_15 = v_13 + v_14
    # src[test_reductions.py:N]: out[tile_m, :] = acc
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * 2 + indices_1[None, :] * 1), v_16, None)

def layer_norm_fwd(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 2
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     mean = hl.full([n], 0.0, acc.dtype)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(2, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_fp16_math_ops_fp32_fallback)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

@cute.kernel
def _helion_rsqrt_fp16_kernel(x, result, _BLOCK_SIZE_0: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[0])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[0]) < _BLOCK_SIZE_0 and offsets_0 < 32
    # src[test_reductions.py:N]: result[tile] = torch.rsqrt(x[tile])
    load = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_0 = cutlass.Float32(load)
    v_1 = cute.math.sqrt(v_0)
    v_2 = 1
    v_3 = v_2 / v_1
    v_4 = cutlass.Float16(v_3)
    result.__setitem__((indices_0,), v_4) if mask_0 else None

def rsqrt_fp16_kernel(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: result = torch.empty_like(x)
    result = torch.empty_like(x)
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_reductions.py:N]:     # This should now work via fp32 fallback
    # src[test_reductions.py:N]:     result[tile] = torch.rsqrt(x[tile])
    _launcher(_helion_rsqrt_fp16_kernel, ((32 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, result, _BLOCK_SIZE_0, block=(32, 1, 1))
    # src[test_reductions.py:N]: return result
    return result

--- assertExpectedJournal(TestReductions.test_fp16_math_ops_fp32_fallback)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

@cute.kernel
def _helion_multi_math_ops_fp16_kernel(x, result, _BLOCK_SIZE_0: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[0])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[0]) < _BLOCK_SIZE_0 and offsets_0 < 16
    # src[test_reductions.py:N]: result[tile, 0] = torch.rsqrt(x[tile])
    load = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_0 = cutlass.Float32(load)
    v_1 = cute.math.sqrt(v_0)
    v_2 = 1
    v_3 = v_2 / v_1
    v_4 = cutlass.Float16(v_3)
    result.__setitem__((indices_0, 0), v_4) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 1] = torch.sqrt(x[tile])
    load_1 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_5 = cutlass.Float32(load_1)
    v_6 = cute.math.sqrt(v_5)
    v_7 = cutlass.Float16(v_6)
    result.__setitem__((indices_0, 1), v_7) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 2] = torch.sin(x[tile])
    load_2 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_8 = cutlass.Float32(load_2)
    v_9 = cute.math.sin(v_8)
    v_10 = cutlass.Float16(v_9)
    result.__setitem__((indices_0, 2), v_10) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 3] = torch.cos(x[tile])
    load_3 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_11 = cutlass.Float32(load_3)
    v_12 = cute.math.cos(v_11)
    v_13 = cutlass.Float16(v_12)
    result.__setitem__((indices_0, 3), v_13) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 4] = torch.log(x[tile])
    load_4 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_14 = cutlass.Float32(load_4)
    v_15 = cute.math.log(v_14)
    v_16 = cutlass.Float16(v_15)
    result.__setitem__((indices_0, 4), v_16) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 5] = torch.tanh(x[tile])
    load_5 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_17 = cutlass.Float32(load_5)
    v_18 = cute.math.tanh(v_17)
    v_19 = cutlass.Float16(v_18)
    result.__setitem__((indices_0, 5), v_19) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 6] = torch.log1p(x[tile])
    load_6 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_20 = cutlass.Float32(load_6)
    v_21 = 1
    v_22 = v_20 + v_21
    v_23 = cute.math.log(v_22)
    v_24 = cutlass.Float16(v_23)
    result.__setitem__((indices_0, 6), v_24) if mask_0 else None
    # src[test_reductions.py:N]: result[tile, 7] = torch.exp(x[tile])
    load_7 = x[indices_0] if mask_0 else cutlass.Float16(0)
    v_25 = cutlass.Float32(load_7)
    v_26 = cute.math.exp2(cutlass.Float32(v_25) * 1.4426950408889634)
    v_27 = cutlass.Float16(v_26)
    result.__setitem__((indices_0, 7), v_27) if mask_0 else None

def multi_math_ops_fp16_kernel(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: result = torch.empty([x.size(0), 8], dtype=x.dtype, device=x.device)
    result = torch.empty([x.size(0), 8], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 16
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_reductions.py:N]:     # Test multiple operations that have confirmed fallbacks
    # src[test_reductions.py:N]:     result[tile, 0] = torch.rsqrt(x[tile])
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_multi_math_ops_fp16_kernel, ((16 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, result, _BLOCK_SIZE_0, block=(16, 1, 1))
    # src[test_reductions.py:N]: return result
    return result

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

@cute.kernel
def _helion_layer_norm_fwd_repro(x, weight, bias, out, eps, _BLOCK_SIZE_0: cutlass.Constexpr, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 32
    # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    var_mean_extra_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        x_part = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_0 = cutlass.Float32(x_part)
        var_mean_extra_acc = var_mean_extra_acc + v_0
    var_mean_extra = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_acc, threads_in_group=32))
    v_1 = 64
    v_2 = cutlass.Float32(v_1)
    v_3 = var_mean_extra / v_2
    _mask_to_1 = v_3 if mask_0 else cutlass.Float32(0)
    var_mean_extra_2_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        _mask_to_1_copy = _mask_to_1
        x_part_1 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_4 = cutlass.Float32(x_part_1)
        v_5 = v_4 - _mask_to_1_copy
        v_6 = v_5 * v_5
        var_mean_extra_2_acc = var_mean_extra_2_acc + v_6
    var_mean_extra_2 = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_2_acc, threads_in_group=32))
    v_7 = 64
    v_8 = cutlass.Float32(v_7)
    v_9 = var_mean_extra_2 / v_8
    v_10 = cutlass.Float16(v_9)
    v_11 = cutlass.Float16(v_3)
    # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
    v_12 = cutlass.Float32(v_10)
    v_13 = v_12 + eps
    v_14 = cute.math.sqrt(v_13)
    v_15 = 1
    v_16 = v_15 / v_14
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        v_11_copy = v_11
        v_16_copy = v_16
        x_part_2 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
        v_17 = x_part_2 - v_11_copy
        v_18 = cutlass.Float32(v_17)
        v_19 = v_18 * v_16_copy
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        load_1 = weight[rindex_1]
        v_20 = cutlass.Float32(load_1)
        v_21 = v_19 * v_20
        # src[test_reductions.py:N]: bias[:].to(torch.float32)
        load_2 = bias[rindex_1]
        v_22 = cutlass.Float32(load_2)
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        # src[test_reductions.py:N]:     bias[:].to(torch.float32)
        # src[test_reductions.py:N]: )
        v_23 = v_21 + v_22
        v_24 = cutlass.Float16(v_23)
        out.__setitem__((indices_0, rindex_1), v_24) if mask_0 else None

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    _REDUCTION_BLOCK_1 = 32
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     x_part = x[tile_m, :]
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_repro, ((32 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, block=(32, 32, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

@cute.kernel
def _helion_layer_norm_fwd_repro(x, weight, bias, out, eps, _BLOCK_SIZE_0: cutlass.Constexpr, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 32
    # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    var_mean_extra_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        x_part = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_0 = cutlass.Float32(x_part)
        var_mean_extra_acc = var_mean_extra_acc + v_0
    var_mean_extra = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_acc, threads_in_group=8))
    v_1 = 64
    v_2 = cutlass.Float32(v_1)
    v_3 = var_mean_extra / v_2
    _mask_to_1 = v_3 if mask_0 else cutlass.Float32(0)
    var_mean_extra_2_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        _mask_to_1_copy = _mask_to_1
        x_part_1 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_4 = cutlass.Float32(x_part_1)
        v_5 = v_4 - _mask_to_1_copy
        v_6 = v_5 * v_5
        var_mean_extra_2_acc = var_mean_extra_2_acc + v_6
    var_mean_extra_2 = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_2_acc, threads_in_group=8))
    v_7 = 64
    v_8 = cutlass.Float32(v_7)
    v_9 = var_mean_extra_2 / v_8
    v_10 = cutlass.Float16(v_9)
    v_11 = cutlass.Float16(v_3)
    # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
    v_12 = cutlass.Float32(v_10)
    v_13 = v_12 + eps
    v_14 = cute.math.sqrt(v_13)
    v_15 = 1
    v_16 = v_15 / v_14
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        v_11_copy = v_11
        v_16_copy = v_16
        x_part_2 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
        v_17 = x_part_2 - v_11_copy
        v_18 = cutlass.Float32(v_17)
        v_19 = v_18 * v_16_copy
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        load_1 = weight[rindex_1]
        v_20 = cutlass.Float32(load_1)
        v_21 = v_19 * v_20
        # src[test_reductions.py:N]: bias[:].to(torch.float32)
        load_2 = bias[rindex_1]
        v_22 = cutlass.Float32(load_2)
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        # src[test_reductions.py:N]:     bias[:].to(torch.float32)
        # src[test_reductions.py:N]: )
        v_23 = v_21 + v_22
        v_24 = cutlass.Float16(v_23)
        out.__setitem__((indices_0, rindex_1), v_24) if mask_0 else None

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    _REDUCTION_BLOCK_1 = 8
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     x_part = x[tile_m, :]
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_repro, ((32 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, block=(8, 32, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_layer_norm_nonpow2_reduction)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd_nonpow2(x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 1536
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 1536 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    # src[test_reductions.py:N]: mean_val = torch.sum(acc, dim=-1) / n
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = 0.0006510416666666666
    v_2 = sum_1 * v_1
    # src[test_reductions.py:N]: centered = acc - mean_val[:, None]
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    # src[test_reductions.py:N]: var_val = torch.sum(centered * centered, dim=-1) / n
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
    v_5 = 0.0006510416666666666
    v_6 = sum_2 * v_5
    # src[test_reductions.py:N]: rstd_val = torch.rsqrt(var_val + eps)
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    # src[test_reductions.py:N]: normalized = centered * rstd_val[:, None]
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, mask_1, other=0)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    # src[test_reductions.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, mask_1, other=0)
    v_13 = tl.cast(load_2, tl.float32)
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    # src[test_reductions.py:N]:     bias[:].to(torch.float32)
    # src[test_reductions.py:N]: )
    v_14 = v_13[None, :]
    v_15 = v_12 + v_14
    # src[test_reductions.py:N]: out[tile_m, :] = acc.to(x.dtype)
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * 1536 + indices_1[None, :] * 1), v_16, mask_1[None, :])
    # src[test_reductions.py:N]: mean[tile_m] = mean_val
    tl.store(mean + indices_0 * 1, v_2, None)
    # src[test_reductions.py:N]: rstd[tile_m] = rstd_val
    tl.store(rstd + indices_0 * 1, v_8, None)

def layer_norm_fwd_nonpow2(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: mean = torch.empty([m], dtype=torch.float32, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 2048
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     # Compute mean
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_nonpow2, (triton.cdiv(4096, _BLOCK_SIZE_0),), x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=4)
    # src[test_reductions.py:N]: return out, mean, rstd
    return (out, mean, rstd)

--- assertExpectedJournal(TestReductions.test_mean)
def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32):
    n, _m = 
    # Call: SequenceType((LiteralType(512), LiteralType(512))) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
    # Attribute: TensorAttributeType AttributeOrigin(value=ArgumentOrigin(name='x'), key='size')
    # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x.size()
    out = 
    # Call: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
    # Attribute: CallableType(_VariableFunctionsClass.empty) AttributeOrigin(value=GlobalOrigin(name='torch'), key='empty')
    # Name: PythonModuleType(torch) GlobalOrigin(name='torch')
torch.empty(
    # List: SequenceType([LiteralType(512)]) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
[
    # Name: LiteralType(512) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:N>), key=0)
n], dtype=
    # Name: LiteralType(torch.float32) ArgumentOrigin(name='out_dtype')
out_dtype, device=
    # Attribute: LiteralType(device=DEVICE) AttributeOrigin(value=ArgumentOrigin(name='x'), key='device')
    # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x.device)
    # For: loop_type=GRID

    for tile_n in 
    # Call: IterType(TileIndexType(0)) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
    # Attribute: CallableType(tile) AttributeOrigin(value=GlobalOrigin(name='hl'), key='tile')
    # Name: PythonModuleType(helion.language) GlobalOrigin(name='hl')
hl.tile(
    # Name: LiteralType(512) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:N>), key=0)
n):
        
        # Subscript: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
        # Name: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
out[
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
tile_n] = 
        # Call: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
        # Name: CallableType(_VariableFunctionsClass.mean) ArgumentOrigin(name='fn')
fn(
        # Subscript: TensorType([block_size_0, rdim_1], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
        # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x[
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
tile_n, 
        # Slice: SliceType(LiteralType(None):LiteralType(None):LiteralType(None)) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
:], dim=
        # UnaryOp: LiteralType(-1) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
-
        # Constant: LiteralType(1) DeviceOrigin(location=<SourceLocation test_reductions.py:N>)
1)
    return 
    # Name: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:N>)
out

def root_graph_0():
    # File: .../test_reductions.py:N in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[512, 512]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None, None);  x = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [mean_extra]);  mean_extra = None
    out: "f32[512]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

def reduction_loop_1():
    # File: .../test_reductions.py:N in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[512, 512]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None, None);  x = block_size_0 = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    return [mean_extra]

def root_graph_2():
    # File: .../test_reductions.py:N in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    _get_symnode = helion_language__tracing_ops__get_symnode('rdim1')
    _for_loop = helion_language__tracing_ops__for_loop(1, [0], [_get_symnode], []);  _get_symnode = None
    getitem: "f32[u0]" = _for_loop[0];  _for_loop = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [getitem]);  getitem = None
    out: "f32[512]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

--- assertExpectedJournal(TestReductions.test_mean)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 8

@cute.kernel
def _helion_reduce_kernel(x, out, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 512
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    mean_extra_acc = cutlass.Float32(0)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(512), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float32(0)
        mean_extra_acc = mean_extra_acc + load
    mean_extra = cutlass.Float32(cute.arch.warp_reduction_sum(mean_extra_acc, threads_in_group=32))
    v_0 = 512
    v_1 = cutlass.Float32(v_0)
    v_2 = mean_extra / v_1
    out.__setitem__((indices_0,), v_2) if mask_0 else None

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 8
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    _REDUCTION_BLOCK_1 = 32
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, ((512 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, _REDUCTION_BLOCK_1, block=(32, 8, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_reduction_loops_integer_values)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

@cute.kernel
def _helion_layer_norm_reduction(x, weight, bias, out, eps, _BLOCK_SIZE_0: cutlass.Constexpr, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 32
    # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
    var_mean_extra_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        v_0 = cutlass.Float32(load)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
        var_mean_extra_acc = var_mean_extra_acc + v_0
    var_mean_extra = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_acc, threads_in_group=4))
    v_1 = 64
    v_2 = cutlass.Float32(v_1)
    v_3 = var_mean_extra / v_2
    _mask_to_1 = v_3 if mask_0 else cutlass.Float32(0)
    var_mean_extra_2_acc = cutlass.Float32(0)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        _mask_to_1_copy = _mask_to_1
        load_1 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        v_4 = cutlass.Float32(load_1)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
        v_5 = v_4 - _mask_to_1_copy
        v_6 = v_5 * v_5
        var_mean_extra_2_acc = var_mean_extra_2_acc + v_6
    var_mean_extra_2 = cutlass.Float32(cute.arch.warp_reduction_sum(var_mean_extra_2_acc, threads_in_group=4))
    v_7 = 64
    v_8 = cutlass.Float32(v_7)
    v_9 = var_mean_extra_2 / v_8
    # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
    v_10 = v_9 + eps
    v_11 = cute.math.sqrt(v_10)
    v_12 = 1
    v_13 = v_12 / v_11
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(64), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        v_3_copy = v_3
        v_13_copy = v_13
        load_2 = x[indices_0, rindex_1] if mask_0 else cutlass.Float16(0)
        v_14 = cutlass.Float32(load_2)
        # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
        v_15 = v_14 - v_3_copy
        v_16 = v_15 * v_13_copy
        # src[test_reductions.py:N]: result = normalized * (weight[:].to(torch.float32)) + (
        load_3 = weight[rindex_1]
        v_17 = cutlass.Float32(load_3)
        v_18 = v_16 * v_17
        # src[test_reductions.py:N]: bias[:].to(torch.float32)
        load_4 = bias[rindex_1]
        v_19 = cutlass.Float32(load_4)
        # src[test_reductions.py:N]: result = normalized * (weight[:].to(torch.float32)) + (
        # src[test_reductions.py:N]:     bias[:].to(torch.float32)
        # src[test_reductions.py:N]: )
        v_20 = v_18 + v_19
        # src[test_reductions.py:N]: out[tile_m, :] = result
        v_21 = cutlass.Float16(v_20)
        out.__setitem__((indices_0, rindex_1), v_21) if mask_0 else None

def layer_norm_reduction(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    _REDUCTION_BLOCK_1 = 4
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_reduction, ((32 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, block=(4, 32, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_size1_reduction_keepdim_sum)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 8
_BLOCK_SIZE_1 = 128

@cute.kernel
def _helion_keepdim_sum(x, out, T, D):
    # src[test_reductions.py:N]: for tile_t, tile_d in hl.tile([T, D]):
    num_blocks_0 = (T + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0
    pid_0 = cute.arch.block_idx()[0] % num_blocks_0
    pid_1 = cute.arch.block_idx()[0] // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = offset_0 + cutlass.Int32(cute.arch.thread_idx()[0])
    mask_0 = indices_0 < T
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = offset_1 + cutlass.Int32(cute.arch.thread_idx()[1])
    mask_1 = indices_1 < D
    # src[test_reductions.py:N]: val = x[tile_t, tile_d].float()  # [T_tile, D_tile]
    load = x[indices_0, indices_1] if mask_0 and mask_1 else cutlass.BFloat16(0)
    v_0 = cutlass.Float32(load)
    # src[test_reductions.py:N]: partial = val.sum(0, keepdim=True)  # [1, D_tile]
    partial = cutlass.Float32(cute.arch.warp_reduction_sum(v_0, threads_in_group=8))
    # src[test_reductions.py:N]: hl.store(out, [tile_d.index], result)
    out.__setitem__((indices_1,), partial) if mask_1 else None

def keepdim_sum(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: T, D = x.shape
    T, D = x.shape
    # src[test_reductions.py:N]: out = torch.empty(D, dtype=torch.float32, device=x.device)
    out = torch.empty(D, dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: for tile_t, tile_d in hl.tile([T, D]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 128
    # src[test_reductions.py:N]: for tile_t, tile_d in hl.tile([T, D]):
    # src[test_reductions.py:N]:     val = x[tile_t, tile_d].float()  # [T_tile, D_tile]
    # src[test_reductions.py:N]:     partial = val.sum(0, keepdim=True)  # [1, D_tile]
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_keepdim_sum, ((T + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0 * ((D + _BLOCK_SIZE_1 - 1) // _BLOCK_SIZE_1),), x, out, T, D, block=(8, 128, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_size1_reduction_unsqueeze_sum)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 128

@cute.kernel
def _helion_unsqueeze_sum(x, out, D):
    # src[test_reductions.py:N]: for (tile_d,) in hl.tile([D]):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[0])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[0]) < _BLOCK_SIZE_0 and offsets_0 < D
    # src[test_reductions.py:N]: val = x[tile_d].float()  # [D_tile]
    load = x[indices_0] if mask_0 else cutlass.BFloat16(0)
    v_0 = cutlass.Float32(load)
    # src[test_reductions.py:N]: hl.store(out, [tile_d.index], reduced)
    out.__setitem__((indices_0,), v_0) if mask_0 else None

def unsqueeze_sum(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: (D,) = x.shape
    D, = x.shape
    # src[test_reductions.py:N]: out = torch.empty(D, dtype=torch.float32, device=x.device)
    out = torch.empty(D, dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: for (tile_d,) in hl.tile([D]):
    _BLOCK_SIZE_0 = 128
    # src[test_reductions.py:N]: for (tile_d,) in hl.tile([D]):
    # src[test_reductions.py:N]:     val = x[tile_d].float()  # [D_tile]
    # src[test_reductions.py:N]:     val2 = val.unsqueeze(0)  # [1, D_tile]
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_unsqueeze_sum, ((D + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, D, block=(128, 1, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 1

@cute.kernel
def _helion_sum_kernel(x, out, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 512
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    sum_1_acc = cutlass.Float32(0)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(512), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float32(0)
        sum_1_acc = sum_1_acc + load
    sum_1 = cutlass.Float32(cute.arch.warp_reduction_sum(sum_1_acc, threads_in_group=32))
    out.__setitem__((indices_0,), sum_1) if mask_0 else None

def sum_kernel(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 1
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    _REDUCTION_BLOCK_1 = 32
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = x[tile_n, :].sum(-1)
    _launcher(_helion_sum_kernel, ((512 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, _REDUCTION_BLOCK_1, block=(32, 1, 1))
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum_keepdims)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_reductions.py:N]: def sum_kernel_keepdims(x: torch.Tensor) -> torch.Tensor:
# src[test_reductions.py:N]:     _n, m = x.size()
# src[test_reductions.py:N]:     out = torch.empty(
# src[test_reductions.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_sum_kernel_keepdims(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    x_desc = tl.make_tensor_descriptor(x, [512, 512], [512, 1], [_RDIM_SIZE_1, _BLOCK_SIZE_0])
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    load = x_desc.load([0, offset_0])
    sum_1 = tl.cast(tl.reshape(tl.sum(load, 0), [1, _BLOCK_SIZE_0]), tl.float32)
    tl.store(out + indices_0[None, :] * 1, sum_1, None)

def sum_kernel_keepdims(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: _n, m = x.size()
    _n, m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [1, m],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([1, m], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = 512
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    _launcher(_helion_sum_kernel_keepdims, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum_looped)
from __future__ import annotations

import torch
import cutlass
import cutlass.cute as cute
from helion.runtime import default_cute_launcher as _default_cute_launcher

_BLOCK_SIZE_0 = 2

@cute.kernel
def _helion_sum_kernel(x, out, _REDUCTION_BLOCK_1: cutlass.Constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_flat = cute.arch.block_idx()[0]
    offsets_0 = pid_flat * _BLOCK_SIZE_0 + cutlass.Int32(cute.arch.thread_idx()[1])
    indices_0 = offsets_0
    mask_0 = cutlass.Int32(cute.arch.thread_idx()[1]) < _BLOCK_SIZE_0 and offsets_0 < 512
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    sum_1_acc = cutlass.Float32(0)
    for roffset_1 in range(cutlass.Int32(0), cutlass.Int32(512), cutlass.Int32(_REDUCTION_BLOCK_1)):
        rindex_1 = roffset_1 + cutlass.Int32(cute.arch.thread_idx()[0])
        load = x[indices_0, rindex_1] if mask_0 else cutlass.Float32(0)
        sum_1_acc = sum_1_acc + load
    sum_1 = cutlass.Float32(cute.arch.warp_reduction_sum(sum_1_acc, threads_in_group=32))
    out.__setitem__((indices_0,), sum_1) if mask_0 else None

def sum_kernel(x: torch.Tensor, *, _launcher=_default_cute_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 2
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    _REDUCTION_BLOCK_1 = 32
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = x[tile_n, :].sum(-1)
    _launcher(_helion_sum_kernel, ((512 + _BLOCK_SIZE_0 - 1) // _BLOCK_SIZE_0,), x, out, _REDUCTION_BLOCK_1, block=(32, 2, 1))
    # src[test_reductions.py:N]: return out
    return out
