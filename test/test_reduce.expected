This file is automatically generated by assertExpectedJournal calls in test_reduce.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestReduce.test_reduce_argmax_negative_values)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def argmax_combine_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_2 > param_0
    v_1 = tl.where(v_0, param_2, param_0)
    v_2 = tl.where(v_0, param_3, param_1)
    return (v_1, v_2)

@triton.jit
def _helion_test_argmax_negative_kernel(values, indices, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_values = tl.load(values + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    row_indices = tl.load(indices + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_0[:, None], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), row_values, tl.full([], float('-inf'), tl.float32))
    max_index = tl.reduce((_mask_to, row_indices), 1, argmax_combine_fn_0)[1]
    tl.store(result + indices_0 * 1, max_index, mask_0)

def test_argmax_negative_kernel(values: torch.Tensor, indices: torch.Tensor, *, _launcher=_default_launcher):
    batch_size = values.size(0)
    result = torch.empty([batch_size], dtype=torch.int64, device=values.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_argmax_negative_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), values, indices, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_argmax_unpacked_format)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def argmax_combine_unpacked_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_2 > param_0
    v_1 = tl.where(v_0, param_2, param_0)
    v_2 = tl.where(v_0, param_3, param_1)
    return (v_1, v_2)

@triton.jit
def _helion_test_argmax_unpacked_kernel(values, indices, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_values = tl.load(values + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    row_indices = tl.load(indices + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    max_index = tl.reduce((row_values, row_indices), 1, argmax_combine_unpacked_fn_0)[1]
    tl.store(result + indices_0 * 1, max_index, mask_0)

def test_argmax_unpacked_kernel(values: torch.Tensor, indices: torch.Tensor, *, _launcher=_default_launcher):
    batch_size = values.size(0)
    result = torch.empty([batch_size], dtype=torch.int64, device=values.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_argmax_unpacked_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), values, indices, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_basic_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def add_combine_fn_0(param_0, param_1):
    v_0 = param_0 + param_1
    return v_0

@triton.jit
def _helion_test_reduce_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    _reduce = tl.reduce(row_data, 1, add_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, mask_0)

def test_reduce_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_code_generation)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def add_combine_fn_0(param_0, param_1):
    v_0 = param_0 + param_1
    return v_0

@triton.jit
def _helion_test_reduce_codegen_kernel(x, result, _RDIM_SIZE_1: tl.constexpr):
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 3
    row_data = tl.load(x + indices_1[None, :] * 1, mask_1[None, :], other=0)
    _reduce = tl.reduce(row_data, 1, add_combine_fn_0)
    tl.store(result + tl.zeros([1], tl.int32), _reduce, None)

def test_reduce_codegen_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_codegen_kernel, (1,), x, result, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_different_dtypes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def add_combine_fn_0(param_0, param_1):
    v_0 = param_0 + param_1
    return v_0

@triton.jit
def _helion_test_reduce_int_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), None)
    _reduce = tl.reduce(row_data, 1, add_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, None)

def test_reduce_int_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_int_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_jit_combine_fn)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def jit_add_combine_fn_0(param_0, param_1):
    v_0 = param_0 + param_1
    return v_0

@triton.jit
def _helion_test_reduce_jit_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), None)
    _reduce = tl.reduce(row_data, 1, jit_add_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, None)

def test_reduce_jit_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_jit_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_max)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def max_combine_fn_0(param_0, param_1):
    v_0 = triton_helpers.maximum(param_0, param_1)
    return v_0

@triton.jit
def _helion_test_reduce_max_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    _reduce = tl.reduce(row_data, 1, max_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, mask_0)

def test_reduce_max_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_max_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_min)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def min_combine_fn_0(param_0, param_1):
    v_0 = triton_helpers.minimum(param_0, param_1)
    return v_0

@triton.jit
def _helion_test_reduce_min_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    _reduce = tl.reduce(row_data, 1, min_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, mask_0)

def test_reduce_min_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_min_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_product)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def mul_combine_fn_0(param_0, param_1):
    v_0 = param_0 * param_1
    return v_0

@triton.jit
def _helion_test_reduce_product_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 3
    row_data = tl.load(x + (indices_0[:, None] * 3 + indices_1[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], row_data, tl.full([], 1.0, tl.float32))
    _reduce = tl.reduce(_mask_to, 1, mul_combine_fn_0)
    tl.store(result + indices_0 * 1, _reduce, mask_0)

def test_reduce_product_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_product_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_tuple_input)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def tuple_add_combine_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_0 + param_2
    v_1 = param_1 + param_3
    return (v_0, v_1)

@triton.jit
def _helion_test_reduce_tuple_kernel(x, y, result_x, result_y, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 3
    row_x = tl.load(x + (indices_0[:, None] * 3 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    row_y = tl.load(y + (indices_0[:, None] * 3 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    getitem = tl.reduce((row_x, row_y), 1, tuple_add_combine_fn_0)[0]
    getitem_1 = tl.reduce((row_x, row_y), 1, tuple_add_combine_fn_0)[1]
    tl.store(result_x + indices_0 * 1, getitem, None)
    tl.store(result_y + indices_0 * 1, getitem_1, None)

def test_reduce_tuple_kernel(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    result_x = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    result_y = torch.empty([y.size(0)], dtype=y.dtype, device=y.device)
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_tuple_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, y, result_x, result_y, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return (result_x, result_y)

--- assertExpectedJournal(TestReduce.test_reduce_tuple_unpacked_format)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def tuple_add_combine_unpacked_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_0 + param_2
    v_1 = param_1 + param_3
    return (v_0, v_1)

@triton.jit
def _helion_test_reduce_tuple_unpacked_kernel(x, y, result_x, result_y, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 3
    row_x = tl.load(x + (indices_0[:, None] * 3 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    row_y = tl.load(y + (indices_0[:, None] * 3 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    getitem = tl.reduce((row_x, row_y), 1, tuple_add_combine_unpacked_fn_0)[0]
    getitem_1 = tl.reduce((row_x, row_y), 1, tuple_add_combine_unpacked_fn_0)[1]
    tl.store(result_x + indices_0 * 1, getitem, None)
    tl.store(result_y + indices_0 * 1, getitem_1, None)

def test_reduce_tuple_unpacked_kernel(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    result_x = torch.empty([x.size(0)], dtype=x.dtype, device=x.device)
    result_y = torch.empty([y.size(0)], dtype=y.dtype, device=y.device)
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_tuple_unpacked_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, y, result_x, result_y, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return (result_x, result_y)

--- assertExpectedJournal(TestReduce.test_reduce_tuple_unpacking_oneline)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def argmax_combine_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_2 > param_0
    v_1 = tl.where(v_0, param_2, param_0)
    v_2 = tl.where(v_0, param_3, param_1)
    return (v_1, v_2)

@triton.jit
def _helion_test_tuple_oneline_kernel(values, indices, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_values = tl.load(values + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    row_indices = tl.load(indices + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    max_index = tl.reduce((row_values, row_indices), 1, argmax_combine_fn_0)[1]
    tl.store(result + indices_0 * 1, max_index, mask_0)

def test_tuple_oneline_kernel(values: torch.Tensor, indices: torch.Tensor, *, _launcher=_default_launcher):
    batch_size = values.size(0)
    result = torch.empty([batch_size], dtype=torch.int64, device=values.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_tuple_oneline_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), values, indices, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_tuple_unpacking_twoline)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def argmax_combine_fn_0(param_0, param_1, param_2, param_3):
    v_0 = param_2 > param_0
    v_1 = tl.where(v_0, param_2, param_0)
    v_2 = tl.where(v_0, param_3, param_1)
    return (v_1, v_2)

@triton.jit
def _helion_test_tuple_twoline_kernel(values, indices, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_values = tl.load(values + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    row_indices = tl.load(indices + (indices_0[:, None] * 4 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    max_index = tl.reduce((row_values, row_indices), 1, argmax_combine_fn_0)[1]
    tl.store(result + indices_0 * 1, max_index, mask_0)

def test_tuple_twoline_kernel(values: torch.Tensor, indices: torch.Tensor, *, _launcher=_default_launcher):
    batch_size = values.size(0)
    result = torch.empty([batch_size], dtype=torch.int64, device=values.device)
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_tuple_twoline_kernel, (triton.cdiv(3, _BLOCK_SIZE_0),), values, indices, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result

--- assertExpectedJournal(TestReduce.test_reduce_with_keep_dims)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import test.test_reduce as _source_module

@triton.jit
def add_combine_fn_0(param_0, param_1):
    v_0 = param_0 + param_1
    return v_0

@triton.jit
def _helion_test_reduce_keep_dims_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    row_data = tl.load(x + (indices_0[:, None] * 4 + indices_1[None, :] * 1), None)
    _reduce = tl.reduce(row_data, 1, add_combine_fn_0, keep_dims=True)
    tl.store(result + indices_0[:, None] * 1, _reduce, None)

def test_reduce_keep_dims_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    result = torch.empty([x.size(0), 1], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 4
    _launcher(_helion_test_reduce_keep_dims_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=2)
    return result
