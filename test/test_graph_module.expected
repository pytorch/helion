This file is automatically generated by assertExpectedJournal calls in test_graph_module.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestGraphModule.test_graph_module_arg)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_apply_graph_module(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 1000
    # src[test_graph_module.py:N]: out[tile] = func_m(x[tile])
    load = tl.load(x + indices_0 * 1, mask_0, other=0)
    v_0 = 1.0
    v_1 = load + v_0
    v_2 = tl_math.sin(v_1)
    tl.store(out + indices_0 * 1, v_2, mask_0)

def apply_graph_module(func_m, x, *, _launcher=_default_launcher):
    """Kernel that applies a GraphModule function to tensor elements."""
    # src[test_graph_module.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    _BLOCK_SIZE_0 = 32
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    # src[test_graph_module.py:N]:     out[tile] = func_m(x[tile])
    _launcher(_helion_apply_graph_module, (triton.cdiv(1000, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_graph_module.py:N]: return out
    return out

--- assertExpectedJournal(TestGraphModule.test_graph_module_with_multiple_ops)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_apply_graph_module(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_graph_module.py:N]: out[tile] = func_m(x[tile])
    load = tl.load(x + indices_0 * 1, None)
    v_0 = 2.0
    v_1 = load * v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = 1.0
    v_5 = v_3 + v_4
    v_6 = tl_math.cos(v_5)
    tl.store(out + indices_0 * 1, v_6, None)

def apply_graph_module(func_m, x, *, _launcher=_default_launcher):
    """Kernel that applies a GraphModule function to tensor elements."""
    # src[test_graph_module.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    _BLOCK_SIZE_0 = 32
    # src[test_graph_module.py:N]: for tile in hl.tile(out.size()):
    # src[test_graph_module.py:N]:     out[tile] = func_m(x[tile])
    _launcher(_helion_apply_graph_module, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_graph_module.py:N]: return out
    return out
