This file is automatically generated by assertExpectedJournal calls in test_topk.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestTopK.test_topk_1d_tensor)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_1d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 5
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n], k, dim=-1, largest=True)
    load = tl.load(x + indices_0 * 1, None)
    idx = tl.arange(0, _BLOCK_SIZE_0).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[:] = v
    tl.store(values + indices_1 * 1, out_val, mask_1)
    # src[test_topk.py:N]: indices[:] = idx
    tl.store(indices + indices_1 * 1, out_idx, mask_1)

def topk_1d_kernel(x: torch.Tensor, k: int, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 5
    # src[test_topk.py:N]: (n,) = x.size()
    n, = x.size()
    # src[test_topk.py:N]: values = torch.empty([k], dtype=x.dtype, device=x.device)
    values = torch.empty([k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([k], dtype=torch.int64, device=x.device)
    indices = torch.empty([k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 64
    _RDIM_SIZE_1 = 8
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n], k, dim=-1, largest=True)
    # src[test_topk.py:N]:     values[:] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_1d_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_3d_tensor)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_3d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _RDIM_SIZE_3: tl.constexpr):
    # src[test_topk.py:N]: for tile_b, tile_n in hl.tile([b, n]):
    num_blocks_0 = tl.cdiv(4, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 4
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < 8
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    indices_3 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    mask_3 = indices_3 < 5
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_b, tile_n, :], k, dim=-1, largest=True)
    load = tl.load(x + (indices_0[:, None, None] * 256 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), mask_0[:, None, None] & mask_1[None, :, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_2).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_b, tile_n, :] = v
    tl.store(values + (indices_0[:, None, None] * 40 + indices_1[None, :, None] * 5 + indices_3[None, None, :] * 1), out_val, mask_0[:, None, None] & mask_1[None, :, None] & mask_3[None, None, :])
    # src[test_topk.py:N]: indices[tile_b, tile_n, :] = idx
    tl.store(indices + (indices_0[:, None, None] * 40 + indices_1[None, :, None] * 5 + indices_3[None, None, :] * 1), out_idx, mask_0[:, None, None] & mask_1[None, :, None] & mask_3[None, None, :])

def topk_3d_kernel(x: torch.Tensor, k: int, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 5
    # src[test_topk.py:N]: b, n, m = x.size()
    b, n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([b, n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([b, n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([b, n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([b, n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_b, tile_n in hl.tile([b, n]):
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 32
    _RDIM_SIZE_3 = 8
    # src[test_topk.py:N]: for tile_b, tile_n in hl.tile([b, n]):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_b, tile_n, :], k, dim=-1, largest=True)
    # src[test_topk.py:N]:     values[tile_b, tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_3d_kernel, (triton.cdiv(4, _BLOCK_SIZE_0) * triton.cdiv(8, _BLOCK_SIZE_1),), x, values, indices, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_2, _RDIM_SIZE_3, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_4d_tensor)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_4d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _RDIM_SIZE_3: tl.constexpr, _RDIM_SIZE_4: tl.constexpr):
    # src[test_topk.py:N]: for tile_a, tile_b, tile_n in hl.tile([a, b, n]):
    num_blocks_0 = tl.cdiv(2, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(2, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 2
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < 2
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < 4
    indices_3 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_4).to(tl.int32)
    mask_4 = indices_4 < 5
    # src[test_topk.py:N]: x[tile_a, tile_b, tile_n, :], k, dim=-1, largest=True
    load = tl.load(x + (indices_0[:, None, None, None] * 256 + indices_1[None, :, None, None] * 128 + indices_2[None, None, :, None] * 32 + indices_3[None, None, None, :] * 1), mask_0[:, None, None, None] & mask_1[None, :, None, None] & mask_2[None, None, :, None], other=0)
    # src[test_topk.py:N]: v, idx = torch.topk(
    # src[test_topk.py:N]:     x[tile_a, tile_b, tile_n, :], k, dim=-1, largest=True
    # src[test_topk.py:N]: )
    idx = tl.arange(0, _RDIM_SIZE_3).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_a, tile_b, tile_n, :] = v
    tl.store(values + (indices_0[:, None, None, None] * 40 + indices_1[None, :, None, None] * 20 + indices_2[None, None, :, None] * 5 + indices_4[None, None, None, :] * 1), out_val, mask_0[:, None, None, None] & mask_1[None, :, None, None] & mask_2[None, None, :, None] & mask_4[None, None, None, :])
    # src[test_topk.py:N]: indices[tile_a, tile_b, tile_n, :] = idx
    tl.store(indices + (indices_0[:, None, None, None] * 40 + indices_1[None, :, None, None] * 20 + indices_2[None, None, :, None] * 5 + indices_4[None, None, None, :] * 1), out_idx, mask_0[:, None, None, None] & mask_1[None, :, None, None] & mask_2[None, None, :, None] & mask_4[None, None, None, :])

def topk_4d_kernel(x: torch.Tensor, k: int, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 5
    # src[test_topk.py:N]: a, b, n, m = x.size()
    a, b, n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([a, b, n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([a, b, n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([a, b, n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([a, b, n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_a, tile_b, tile_n in hl.tile([a, b, n]):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 8
    _RDIM_SIZE_3 = 32
    _RDIM_SIZE_4 = 8
    # src[test_topk.py:N]: for tile_a, tile_b, tile_n in hl.tile([a, b, n]):
    # src[test_topk.py:N]:     v, idx = torch.topk(
    # src[test_topk.py:N]:         x[tile_a, tile_b, tile_n, :], k, dim=-1, largest=True
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_4d_kernel, (triton.cdiv(2, _BLOCK_SIZE_0) * triton.cdiv(2, _BLOCK_SIZE_1) * triton.cdiv(4, _BLOCK_SIZE_2),), x, values, indices, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _RDIM_SIZE_3, _RDIM_SIZE_4, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_bfloat16)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 10
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.float32).to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    out_val = out_val.to(tl.bfloat16)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_val, mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_idx, mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 10
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_float16)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 10
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.float32).to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    out_val = out_val.to(tl.float16)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_val, mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_idx, mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 10
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_float32_largest)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 10
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_val, mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_idx, mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 10
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_float32_smallest)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 10
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    sortable = sortable ^ 4294967295
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_bits = out_bits ^ 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_val, mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_idx, mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 10
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_int32_largest)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 8
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 5
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 32 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    sortable = load.to(tl.int64)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = out_bits.to(tl.int32)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 5 + indices_2[None, :] * 1), out_val, mask_0[:, None] & mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 5 + indices_2[None, :] * 1), out_idx, mask_0[:, None] & mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 5
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 32
    _RDIM_SIZE_2 = 8
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(8, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_int32_smallest)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 8
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 5
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 32 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    sortable = load.to(tl.int64)
    sortable = sortable ^ 4294967295
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_bits = out_bits ^ 4294967295
    out_val = out_bits.to(tl.int32)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 5 + indices_2[None, :] * 1), out_val, mask_0[:, None] & mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 5 + indices_2[None, :] * 1), out_idx, mask_0[:, None] & mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 5
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 32
    _RDIM_SIZE_2 = 8
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(8, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_k1)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 16
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 1)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + indices_0[:, None] * 1, out_val, mask_0[:, None])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + indices_0[:, None] * 1, out_idx, mask_0[:, None])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 1
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(16, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_k16)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 16
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 16 + indices_2[None, :] * 1), out_val, mask_0[:, None])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 16 + indices_2[None, :] * 1), out_idx, mask_0[:, None])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 16
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(16, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_k7)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 16
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 7
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 8)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 7 + indices_2[None, :] * 1), out_val, mask_0[:, None] & mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 7 + indices_2[None, :] * 1), out_idx, mask_0[:, None] & mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 7
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 8
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(16, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_known_values_float32)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 4
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 3
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 16 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 4)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 3 + indices_2[None, :] * 1), out_val, mask_0[:, None] & mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 3 + indices_2[None, :] * 1), out_idx, mask_0[:, None] & mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 3
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 16
    _RDIM_SIZE_2 = 4
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(4, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_known_values_int32)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_2d_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 4
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 3
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    load = tl.load(x + (indices_0[:, None] * 16 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    sortable = load.to(tl.int64)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 4)
    result = tl.sort(result, descending=True)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = out_bits.to(tl.int32)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 3 + indices_2[None, :] * 1), out_val, mask_0[:, None] & mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 3 + indices_2[None, :] * 1), out_idx, mask_0[:, None] & mask_2[None, :])

def topk_2d_kernel(x: torch.Tensor, k: int, largest: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 3
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 16
    _RDIM_SIZE_2 = 4
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, largest=largest)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_2d_kernel, (triton.cdiv(4, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)

--- assertExpectedJournal(TestTopK.test_topk_sorted_false)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_topk_unsorted_kernel(x, values, indices, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < 10
    # src[test_topk.py:N]: v, idx = torch.topk(x[tile_n, :], k, dim=-1, sorted=False)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    idx = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    bits = load.to(tl.int32, bitcast=True).to(tl.int64) & 4294967295
    sortable = tl.where(bits >> 31 == 0, bits, bits ^ 2147483647)
    packed = sortable << 32 | idx
    result = tl.topk(packed, 16)
    out_idx = (result & 4294967295).to(tl.int64)
    out_bits = result >> 32 & 4294967295
    out_val = tl.where(out_bits >> 31 == 0, out_bits, out_bits ^ 2147483647).to(tl.int32).to(tl.float32, bitcast=True)
    # src[test_topk.py:N]: values[tile_n, :] = v
    tl.store(values + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_val, mask_2[None, :])
    # src[test_topk.py:N]: indices[tile_n, :] = idx
    tl.store(indices + (indices_0[:, None] * 10 + indices_2[None, :] * 1), out_idx, mask_2[None, :])

def topk_unsorted_kernel(x: torch.Tensor, k: int, *, _launcher=_default_launcher):
    # src[test_topk.py:N]: k = hl.specialize(k)
    k = 10
    # src[test_topk.py:N]: n, m = x.size()
    n, m = x.size()
    # src[test_topk.py:N]: values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    values = torch.empty([n, k], dtype=x.dtype, device=x.device)
    # src[test_topk.py:N]: indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    indices = torch.empty([n, k], dtype=torch.int64, device=x.device)
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _RDIM_SIZE_2 = 16
    # src[test_topk.py:N]: for tile_n in hl.tile(n):
    # src[test_topk.py:N]:     v, idx = torch.topk(x[tile_n, :], k, dim=-1, sorted=False)
    # src[test_topk.py:N]:     values[tile_n, :] = v
    # src[test_topk.py:N-N]: ...
    _launcher(_helion_topk_unsorted_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, values, indices, _BLOCK_SIZE_0, _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_topk.py:N]: return values, indices
    return (values, indices)
