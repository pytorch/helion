This file is automatically generated by assertExpectedJournal calls in test_autodiff.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestAutodiff.test_add)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        grad_x[tile] = grad_out_tile
        grad_y[tile] = grad_out_tile
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, grad_x, grad_y, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_6638b8a1210a.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_6638b8a1210a.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_6638b8a1210a.py:N]: grad_x[tile] = grad_out_tile
    tl.store(grad_x + indices_0 * 1, grad_out_tile, None)
    # src[helion_bwd_6638b8a1210a.py:N]: grad_y[tile] = grad_out_tile
    tl.store(grad_y + indices_0 * 1, grad_out_tile, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_6638b8a1210a.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_6638b8a1210a.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_6638b8a1210a.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_6638b8a1210a.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_6638b8a1210a.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_6638b8a1210a.py:N]:     x_tile = x[tile]
    # src[helion_bwd_6638b8a1210a.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, grad_x, grad_y, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_6638b8a1210a.py:N]: return (grad_x, grad_y)
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_exp)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        exp_val = torch.exp(x_tile)
        mul_val = torch.mul(grad_out_tile, exp_val)
        grad_x[tile] = mul_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_exp)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_2efef9e1e860.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_2efef9e1e860.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_2efef9e1e860.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_2efef9e1e860.py:N]: exp_val = torch.exp(x_tile)
    v_0 = libdevice.exp(x_tile)
    # src[helion_bwd_2efef9e1e860.py:N]: mul_val = torch.mul(grad_out_tile, exp_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_2efef9e1e860.py:N]: grad_x[tile] = mul_val
    tl.store(grad_x + indices_0 * 1, v_1, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_2efef9e1e860.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_2efef9e1e860.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_2efef9e1e860.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_2efef9e1e860.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_2efef9e1e860.py:N]:     x_tile = x[tile]
    # src[helion_bwd_2efef9e1e860.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_2efef9e1e860.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_exp_sin)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        sin_val = torch.sin(x_tile)
        exp_val = torch.exp(sin_val)
        mul_val = torch.mul(grad_out_tile, exp_val)
        cos_val = torch.cos(x_tile)
        mul_1_val = torch.mul(mul_val, cos_val)
        grad_x[tile] = mul_1_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_exp_sin)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_64115e9471c9.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_64115e9471c9.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_64115e9471c9.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_64115e9471c9.py:N]: sin_val = torch.sin(x_tile)
    v_0 = tl_math.sin(x_tile)
    # src[helion_bwd_64115e9471c9.py:N]: exp_val = torch.exp(sin_val)
    v_1 = libdevice.exp(v_0)
    # src[helion_bwd_64115e9471c9.py:N]: mul_val = torch.mul(grad_out_tile, exp_val)
    v_2 = grad_out_tile * v_1
    # src[helion_bwd_64115e9471c9.py:N]: cos_val = torch.cos(x_tile)
    v_3 = tl_math.cos(x_tile)
    # src[helion_bwd_64115e9471c9.py:N]: mul_1_val = torch.mul(mul_val, cos_val)
    v_4 = v_2 * v_3
    # src[helion_bwd_64115e9471c9.py:N]: grad_x[tile] = mul_1_val
    tl.store(grad_x + indices_0 * 1, v_4, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_64115e9471c9.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_64115e9471c9.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_64115e9471c9.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_64115e9471c9.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_64115e9471c9.py:N]:     x_tile = x[tile]
    # src[helion_bwd_64115e9471c9.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_64115e9471c9.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_exp_x_sin_y)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        exp_val = torch.exp(x_tile)
        mul_1_val = torch.mul(grad_out_tile, exp_val)
        sin_val = torch.sin(y_tile)
        mul_2_val = torch.mul(grad_out_tile, sin_val)
        cos_val = torch.cos(y_tile)
        mul_3_val = torch.mul(mul_1_val, cos_val)
        mul_4_val = torch.mul(mul_2_val, exp_val)
        grad_x[tile] = mul_4_val
        grad_y[tile] = mul_3_val
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_exp_x_sin_y)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_517f1e244946.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_517f1e244946.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_517f1e244946.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_517f1e244946.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_517f1e244946.py:N]: exp_val = torch.exp(x_tile)
    v_0 = libdevice.exp(x_tile)
    # src[helion_bwd_517f1e244946.py:N]: mul_1_val = torch.mul(grad_out_tile, exp_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_517f1e244946.py:N]: sin_val = torch.sin(y_tile)
    v_2 = tl_math.sin(y_tile)
    # src[helion_bwd_517f1e244946.py:N]: mul_2_val = torch.mul(grad_out_tile, sin_val)
    v_3 = grad_out_tile * v_2
    # src[helion_bwd_517f1e244946.py:N]: cos_val = torch.cos(y_tile)
    v_4 = tl_math.cos(y_tile)
    # src[helion_bwd_517f1e244946.py:N]: mul_3_val = torch.mul(mul_1_val, cos_val)
    v_5 = v_1 * v_4
    # src[helion_bwd_517f1e244946.py:N]: mul_4_val = torch.mul(mul_2_val, exp_val)
    v_6 = v_3 * v_0
    # src[helion_bwd_517f1e244946.py:N]: grad_x[tile] = mul_4_val
    tl.store(grad_x + indices_0 * 1, v_6, None)
    # src[helion_bwd_517f1e244946.py:N]: grad_y[tile] = mul_3_val
    tl.store(grad_y + indices_0 * 1, v_5, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_517f1e244946.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_517f1e244946.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_517f1e244946.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_517f1e244946.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_517f1e244946.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_517f1e244946.py:N]:     x_tile = x[tile]
    # src[helion_bwd_517f1e244946.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_517f1e244946.py:N]: return (grad_x, grad_y)
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_fma)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    grad_z = torch.empty_like(z)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        z_tile = z[tile]
        mul_1_val = torch.mul(grad_out_tile, x_tile)
        mul_2_val = torch.mul(grad_out_tile, y_tile)
        grad_x[tile] = mul_2_val
        grad_y[tile] = mul_1_val
        grad_z[tile] = grad_out_tile
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_fma)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, grad_x, grad_y, grad_z, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_3606c21e0beb.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_3606c21e0beb.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_3606c21e0beb.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_3606c21e0beb.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_3606c21e0beb.py:N]: mul_1_val = torch.mul(grad_out_tile, x_tile)
    v_0 = grad_out_tile * x_tile
    # src[helion_bwd_3606c21e0beb.py:N]: mul_2_val = torch.mul(grad_out_tile, y_tile)
    v_1 = grad_out_tile * y_tile
    # src[helion_bwd_3606c21e0beb.py:N]: grad_x[tile] = mul_2_val
    tl.store(grad_x + indices_0 * 1, v_1, None)
    # src[helion_bwd_3606c21e0beb.py:N]: grad_y[tile] = mul_1_val
    tl.store(grad_y + indices_0 * 1, v_0, None)
    # src[helion_bwd_3606c21e0beb.py:N]: grad_z[tile] = grad_out_tile
    tl.store(grad_z + indices_0 * 1, grad_out_tile, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_3606c21e0beb.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_3606c21e0beb.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_3606c21e0beb.py:N]: grad_z = torch.empty_like(z)
    grad_z = torch.empty_like(z)
    # src[helion_bwd_3606c21e0beb.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_3606c21e0beb.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_3606c21e0beb.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_3606c21e0beb.py:N]:     x_tile = x[tile]
    # src[helion_bwd_3606c21e0beb.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, grad_x, grad_y, grad_z, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_3606c21e0beb.py:N]: return (grad_x, grad_y, grad_z)
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_load_store_load_pattern)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        mul_val = torch.mul(x_tile, 2)
        cos_val = torch.cos(mul_val)
        mul_1_val = torch.mul(grad_out_tile, cos_val)
        mul_2_val = torch.mul(mul_1_val, 2)
        grad_x[tile] = mul_2_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_load_store_load_pattern)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_dccc593adaf9.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_dccc593adaf9.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_dccc593adaf9.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_dccc593adaf9.py:N]: mul_val = torch.mul(x_tile, 2)
    v_0 = 2.0
    v_1 = x_tile * v_0
    # src[helion_bwd_dccc593adaf9.py:N]: cos_val = torch.cos(mul_val)
    v_2 = tl_math.cos(v_1)
    # src[helion_bwd_dccc593adaf9.py:N]: mul_1_val = torch.mul(grad_out_tile, cos_val)
    v_3 = grad_out_tile * v_2
    # src[helion_bwd_dccc593adaf9.py:N]: mul_2_val = torch.mul(mul_1_val, 2)
    v_4 = 2.0
    v_5 = v_3 * v_4
    # src[helion_bwd_dccc593adaf9.py:N]: grad_x[tile] = mul_2_val
    tl.store(grad_x + indices_0 * 1, v_5, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_dccc593adaf9.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_dccc593adaf9.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_dccc593adaf9.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_dccc593adaf9.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_dccc593adaf9.py:N]:     x_tile = x[tile]
    # src[helion_bwd_dccc593adaf9.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_dccc593adaf9.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_log)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        div_val = torch.div(grad_out_tile, x_tile)
        grad_x[tile] = div_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_log)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_dad46ce1a0b8.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_dad46ce1a0b8.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_dad46ce1a0b8.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_dad46ce1a0b8.py:N]: div_val = torch.div(grad_out_tile, x_tile)
    v_0 = grad_out_tile / x_tile
    # src[helion_bwd_dad46ce1a0b8.py:N]: grad_x[tile] = div_val
    tl.store(grad_x + indices_0 * 1, v_0, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_dad46ce1a0b8.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_dad46ce1a0b8.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_dad46ce1a0b8.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_dad46ce1a0b8.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_dad46ce1a0b8.py:N]:     x_tile = x[tile]
    # src[helion_bwd_dad46ce1a0b8.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_dad46ce1a0b8.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_mul)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        mul_1_val = torch.mul(grad_out_tile, x_tile)
        mul_2_val = torch.mul(grad_out_tile, y_tile)
        grad_x[tile] = mul_2_val
        grad_y[tile] = mul_1_val
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_mul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_7fc556f8295d.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_7fc556f8295d.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_7fc556f8295d.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_7fc556f8295d.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_7fc556f8295d.py:N]: mul_1_val = torch.mul(grad_out_tile, x_tile)
    v_0 = grad_out_tile * x_tile
    # src[helion_bwd_7fc556f8295d.py:N]: mul_2_val = torch.mul(grad_out_tile, y_tile)
    v_1 = grad_out_tile * y_tile
    # src[helion_bwd_7fc556f8295d.py:N]: grad_x[tile] = mul_2_val
    tl.store(grad_x + indices_0 * 1, v_1, None)
    # src[helion_bwd_7fc556f8295d.py:N]: grad_y[tile] = mul_1_val
    tl.store(grad_y + indices_0 * 1, v_0, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_7fc556f8295d.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_7fc556f8295d.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_7fc556f8295d.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_7fc556f8295d.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_7fc556f8295d.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_7fc556f8295d.py:N]:     x_tile = x[tile]
    # src[helion_bwd_7fc556f8295d.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_7fc556f8295d.py:N]: return (grad_x, grad_y)
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_relu)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        relu_val = torch.relu(x_tile)
        le_val = torch.le(relu_val, 0)
        where_val = torch.where(le_val, 0, grad_out_tile)
        grad_x[tile] = where_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_relu)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_cfb9b7eac08f.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_cfb9b7eac08f.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_cfb9b7eac08f.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_cfb9b7eac08f.py:N]: relu_val = torch.relu(x_tile)
    v_0 = 0
    v_1 = triton_helpers.maximum(v_0, x_tile)
    # src[helion_bwd_cfb9b7eac08f.py:N]: le_val = torch.le(relu_val, 0)
    v_2 = 0.0
    v_3 = v_1 <= v_2
    # src[helion_bwd_cfb9b7eac08f.py:N]: where_val = torch.where(le_val, 0, grad_out_tile)
    v_4 = 0.0
    v_5 = tl.where(v_3, v_4, grad_out_tile)
    # src[helion_bwd_cfb9b7eac08f.py:N]: grad_x[tile] = where_val
    tl.store(grad_x + indices_0 * 1, v_5, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_cfb9b7eac08f.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_cfb9b7eac08f.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_cfb9b7eac08f.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_cfb9b7eac08f.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_cfb9b7eac08f.py:N]:     x_tile = x[tile]
    # src[helion_bwd_cfb9b7eac08f.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_cfb9b7eac08f.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sigmoid)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        sigmoid_val = torch.sigmoid(x_tile)
        sub_val = torch.sub(1, sigmoid_val)
        mul_val = torch.mul(sigmoid_val, sub_val)
        mul_1_val = torch.mul(grad_out_tile, mul_val)
        grad_x[tile] = mul_1_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sigmoid)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_b0edf9fb958a.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_b0edf9fb958a.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_b0edf9fb958a.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_b0edf9fb958a.py:N]: sigmoid_val = torch.sigmoid(x_tile)
    v_0 = tl.cast(x_tile, tl.float32)
    v_1 = tl.sigmoid(v_0)
    # src[helion_bwd_b0edf9fb958a.py:N]: sub_val = torch.sub(1, sigmoid_val)
    v_2 = 1.0
    v_3 = v_2 - v_1
    # src[helion_bwd_b0edf9fb958a.py:N]: mul_val = torch.mul(sigmoid_val, sub_val)
    v_4 = v_1 * v_3
    # src[helion_bwd_b0edf9fb958a.py:N]: mul_1_val = torch.mul(grad_out_tile, mul_val)
    v_5 = grad_out_tile * v_4
    # src[helion_bwd_b0edf9fb958a.py:N]: grad_x[tile] = mul_1_val
    tl.store(grad_x + indices_0 * 1, v_5, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_b0edf9fb958a.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_b0edf9fb958a.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_b0edf9fb958a.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_b0edf9fb958a.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_b0edf9fb958a.py:N]:     x_tile = x[tile]
    # src[helion_bwd_b0edf9fb958a.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_b0edf9fb958a.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        cos_val = torch.cos(x_tile)
        mul_val = torch.mul(grad_out_tile, cos_val)
        grad_x[tile] = mul_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_dea87ece8316.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_dea87ece8316.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_dea87ece8316.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_dea87ece8316.py:N]: cos_val = torch.cos(x_tile)
    v_0 = tl_math.cos(x_tile)
    # src[helion_bwd_dea87ece8316.py:N]: mul_val = torch.mul(grad_out_tile, cos_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_dea87ece8316.py:N]: grad_x[tile] = mul_val
    tl.store(grad_x + indices_0 * 1, v_1, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_dea87ece8316.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_dea87ece8316.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_dea87ece8316.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_dea87ece8316.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_dea87ece8316.py:N]:     x_tile = x[tile]
    # src[helion_bwd_dea87ece8316.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_dea87ece8316.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin_cos)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        sin_val = torch.sin(x_tile)
        mul_1_val = torch.mul(grad_out_tile, sin_val)
        cos_val = torch.cos(x_tile)
        mul_2_val = torch.mul(grad_out_tile, cos_val)
        neg_val = torch.neg(sin_val)
        mul_3_val = torch.mul(mul_1_val, neg_val)
        mul_4_val = torch.mul(mul_2_val, cos_val)
        add_val = torch.add(mul_3_val, mul_4_val)
        grad_x[tile] = add_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin_cos)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_974d4f6e8bcf.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_974d4f6e8bcf.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_974d4f6e8bcf.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_974d4f6e8bcf.py:N]: sin_val = torch.sin(x_tile)
    v_0 = tl_math.sin(x_tile)
    # src[helion_bwd_974d4f6e8bcf.py:N]: mul_1_val = torch.mul(grad_out_tile, sin_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_974d4f6e8bcf.py:N]: cos_val = torch.cos(x_tile)
    v_2 = tl_math.cos(x_tile)
    # src[helion_bwd_974d4f6e8bcf.py:N]: mul_2_val = torch.mul(grad_out_tile, cos_val)
    v_3 = grad_out_tile * v_2
    # src[helion_bwd_974d4f6e8bcf.py:N]: neg_val = torch.neg(sin_val)
    v_4 = -v_0
    # src[helion_bwd_974d4f6e8bcf.py:N]: mul_3_val = torch.mul(mul_1_val, neg_val)
    v_5 = v_1 * v_4
    # src[helion_bwd_974d4f6e8bcf.py:N]: mul_4_val = torch.mul(mul_2_val, cos_val)
    v_6 = v_3 * v_2
    # src[helion_bwd_974d4f6e8bcf.py:N]: add_val = torch.add(mul_3_val, mul_4_val)
    v_7 = v_5 + v_6
    # src[helion_bwd_974d4f6e8bcf.py:N]: grad_x[tile] = add_val
    tl.store(grad_x + indices_0 * 1, v_7, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_974d4f6e8bcf.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_974d4f6e8bcf.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_974d4f6e8bcf.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_974d4f6e8bcf.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_974d4f6e8bcf.py:N]:     x_tile = x[tile]
    # src[helion_bwd_974d4f6e8bcf.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_974d4f6e8bcf.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin_squared)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        sin_val = torch.sin(x_tile)
        mul_1_val = torch.mul(grad_out_tile, sin_val)
        add_val = torch.add(mul_1_val, mul_1_val)
        cos_val = torch.cos(x_tile)
        mul_3_val = torch.mul(add_val, cos_val)
        grad_x[tile] = mul_3_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin_squared)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_92e2e9f95ba1.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_92e2e9f95ba1.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_92e2e9f95ba1.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_92e2e9f95ba1.py:N]: sin_val = torch.sin(x_tile)
    v_0 = tl_math.sin(x_tile)
    # src[helion_bwd_92e2e9f95ba1.py:N]: mul_1_val = torch.mul(grad_out_tile, sin_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_92e2e9f95ba1.py:N]: add_val = torch.add(mul_1_val, mul_1_val)
    v_2 = v_1 + v_1
    # src[helion_bwd_92e2e9f95ba1.py:N]: cos_val = torch.cos(x_tile)
    v_3 = tl_math.cos(x_tile)
    # src[helion_bwd_92e2e9f95ba1.py:N]: mul_3_val = torch.mul(add_val, cos_val)
    v_4 = v_2 * v_3
    # src[helion_bwd_92e2e9f95ba1.py:N]: grad_x[tile] = mul_3_val
    tl.store(grad_x + indices_0 * 1, v_4, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_92e2e9f95ba1.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_92e2e9f95ba1.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_92e2e9f95ba1.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_92e2e9f95ba1.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_92e2e9f95ba1.py:N]:     x_tile = x[tile]
    # src[helion_bwd_92e2e9f95ba1.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_92e2e9f95ba1.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sin_x_cos_y)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        sin_1_val = torch.sin(y_tile)
        neg_val = torch.neg(sin_1_val)
        mul_val = torch.mul(grad_out_tile, neg_val)
        cos_1_val = torch.cos(x_tile)
        mul_1_val = torch.mul(grad_out_tile, cos_1_val)
        grad_x[tile] = mul_1_val
        grad_y[tile] = mul_val
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_sin_x_cos_y)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_128a52daef25.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_128a52daef25.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_128a52daef25.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_128a52daef25.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_128a52daef25.py:N]: sin_1_val = torch.sin(y_tile)
    v_0 = tl_math.sin(y_tile)
    # src[helion_bwd_128a52daef25.py:N]: neg_val = torch.neg(sin_1_val)
    v_1 = -v_0
    # src[helion_bwd_128a52daef25.py:N]: mul_val = torch.mul(grad_out_tile, neg_val)
    v_2 = grad_out_tile * v_1
    # src[helion_bwd_128a52daef25.py:N]: cos_1_val = torch.cos(x_tile)
    v_3 = tl_math.cos(x_tile)
    # src[helion_bwd_128a52daef25.py:N]: mul_1_val = torch.mul(grad_out_tile, cos_1_val)
    v_4 = grad_out_tile * v_3
    # src[helion_bwd_128a52daef25.py:N]: grad_x[tile] = mul_1_val
    tl.store(grad_x + indices_0 * 1, v_4, None)
    # src[helion_bwd_128a52daef25.py:N]: grad_y[tile] = mul_val
    tl.store(grad_y + indices_0 * 1, v_2, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_128a52daef25.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_128a52daef25.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_128a52daef25.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_128a52daef25.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_128a52daef25.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_128a52daef25.py:N]:     x_tile = x[tile]
    # src[helion_bwd_128a52daef25.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, grad_x, grad_y, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_128a52daef25.py:N]: return (grad_x, grad_y)
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_softplus)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        exp_val = torch.exp(x_tile)
        add_val = torch.add(exp_val, 1.0)
        div_val = torch.div(grad_out_tile, add_val)
        mul_val = torch.mul(div_val, exp_val)
        grad_x[tile] = mul_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_softplus)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_032e6373b5b1.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_032e6373b5b1.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_032e6373b5b1.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_032e6373b5b1.py:N]: exp_val = torch.exp(x_tile)
    v_0 = libdevice.exp(x_tile)
    # src[helion_bwd_032e6373b5b1.py:N]: add_val = torch.add(exp_val, 1.0)
    v_1 = 1.0
    v_2 = v_0 + v_1
    # src[helion_bwd_032e6373b5b1.py:N]: div_val = torch.div(grad_out_tile, add_val)
    v_3 = grad_out_tile / v_2
    # src[helion_bwd_032e6373b5b1.py:N]: mul_val = torch.mul(div_val, exp_val)
    v_4 = v_3 * v_0
    # src[helion_bwd_032e6373b5b1.py:N]: grad_x[tile] = mul_val
    tl.store(grad_x + indices_0 * 1, v_4, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_032e6373b5b1.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_032e6373b5b1.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_032e6373b5b1.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_032e6373b5b1.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_032e6373b5b1.py:N]:     x_tile = x[tile]
    # src[helion_bwd_032e6373b5b1.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_032e6373b5b1.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_sub)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        neg_val = torch.neg(grad_out_tile)
        grad_x[tile] = grad_out_tile
        grad_y[tile] = neg_val
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_sub)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, grad_x, grad_y, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_2af11fa45f85.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_2af11fa45f85.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_2af11fa45f85.py:N]: neg_val = torch.neg(grad_out_tile)
    v_0 = -grad_out_tile
    # src[helion_bwd_2af11fa45f85.py:N]: grad_x[tile] = grad_out_tile
    tl.store(grad_x + indices_0 * 1, grad_out_tile, None)
    # src[helion_bwd_2af11fa45f85.py:N]: grad_y[tile] = neg_val
    tl.store(grad_y + indices_0 * 1, v_0, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_2af11fa45f85.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_2af11fa45f85.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_2af11fa45f85.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_2af11fa45f85.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_2af11fa45f85.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_2af11fa45f85.py:N]:     x_tile = x[tile]
    # src[helion_bwd_2af11fa45f85.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, grad_x, grad_y, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_2af11fa45f85.py:N]: return (grad_x, grad_y)
    return (grad_x, grad_y)

--- assertExpectedJournal(TestAutodiff.test_sum_of_products)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    grad_z = torch.empty_like(z)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        z_tile = z[tile]
        mul_2_val = torch.mul(grad_out_tile, y_tile)
        mul_3_val = torch.mul(grad_out_tile, z_tile)
        mul_4_val = torch.mul(grad_out_tile, x_tile)
        mul_5_val = torch.mul(grad_out_tile, y_tile)
        add_1_val = torch.add(mul_3_val, mul_4_val)
        grad_x[tile] = mul_5_val
        grad_y[tile] = add_1_val
        grad_z[tile] = mul_2_val
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_sum_of_products)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, z, grad_x, grad_y, grad_z, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_8ec41d00243f.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_8ec41d00243f.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_8ec41d00243f.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_8ec41d00243f.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_8ec41d00243f.py:N]: z_tile = z[tile]
    z_tile = tl.load(z + indices_0 * 1, None)
    # src[helion_bwd_8ec41d00243f.py:N]: mul_2_val = torch.mul(grad_out_tile, y_tile)
    v_0 = grad_out_tile * y_tile
    # src[helion_bwd_8ec41d00243f.py:N]: mul_3_val = torch.mul(grad_out_tile, z_tile)
    v_1 = grad_out_tile * z_tile
    # src[helion_bwd_8ec41d00243f.py:N]: mul_4_val = torch.mul(grad_out_tile, x_tile)
    v_2 = grad_out_tile * x_tile
    # src[helion_bwd_8ec41d00243f.py:N]: mul_5_val = torch.mul(grad_out_tile, y_tile)
    v_3 = grad_out_tile * y_tile
    # src[helion_bwd_8ec41d00243f.py:N]: add_1_val = torch.add(mul_3_val, mul_4_val)
    v_4 = v_1 + v_2
    # src[helion_bwd_8ec41d00243f.py:N]: grad_x[tile] = mul_5_val
    tl.store(grad_x + indices_0 * 1, v_3, None)
    # src[helion_bwd_8ec41d00243f.py:N]: grad_y[tile] = add_1_val
    tl.store(grad_y + indices_0 * 1, v_4, None)
    # src[helion_bwd_8ec41d00243f.py:N]: grad_z[tile] = mul_2_val
    tl.store(grad_z + indices_0 * 1, v_0, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_8ec41d00243f.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_8ec41d00243f.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_8ec41d00243f.py:N]: grad_z = torch.empty_like(z)
    grad_z = torch.empty_like(z)
    # src[helion_bwd_8ec41d00243f.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_8ec41d00243f.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_8ec41d00243f.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_8ec41d00243f.py:N]:     x_tile = x[tile]
    # src[helion_bwd_8ec41d00243f.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, z, grad_x, grad_y, grad_z, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_8ec41d00243f.py:N]: return (grad_x, grad_y, grad_z)
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_tanh)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        tanh_val = torch.tanh(x_tile)
        mul_val = torch.mul(tanh_val, tanh_val)
        sub_val = torch.sub(1, mul_val)
        mul_1_val = torch.mul(grad_out_tile, sub_val)
        grad_x[tile] = mul_1_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_tanh)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_817d58d2593f.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_817d58d2593f.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_817d58d2593f.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_817d58d2593f.py:N]: tanh_val = torch.tanh(x_tile)
    v_0 = libdevice.tanh(x_tile)
    # src[helion_bwd_817d58d2593f.py:N]: mul_val = torch.mul(tanh_val, tanh_val)
    v_1 = v_0 * v_0
    # src[helion_bwd_817d58d2593f.py:N]: sub_val = torch.sub(1, mul_val)
    v_2 = 1.0
    v_3 = v_2 - v_1
    # src[helion_bwd_817d58d2593f.py:N]: mul_1_val = torch.mul(grad_out_tile, sub_val)
    v_4 = grad_out_tile * v_3
    # src[helion_bwd_817d58d2593f.py:N]: grad_x[tile] = mul_1_val
    tl.store(grad_x + indices_0 * 1, v_4, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_817d58d2593f.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_817d58d2593f.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_817d58d2593f.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_817d58d2593f.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_817d58d2593f.py:N]:     x_tile = x[tile]
    # src[helion_bwd_817d58d2593f.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_817d58d2593f.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_triple_mul)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> tuple[torch.Tensor, ...]:
    grad_x = torch.empty_like(x)
    grad_y = torch.empty_like(y)
    grad_z = torch.empty_like(z)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        y_tile = y[tile]
        z_tile = z[tile]
        mul_val = torch.mul(x_tile, y_tile)
        mul_2_val = torch.mul(grad_out_tile, mul_val)
        mul_3_val = torch.mul(grad_out_tile, z_tile)
        mul_4_val = torch.mul(mul_3_val, x_tile)
        mul_5_val = torch.mul(mul_3_val, y_tile)
        grad_x[tile] = mul_5_val
        grad_y[tile] = mul_4_val
        grad_z[tile] = mul_2_val
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_triple_mul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, y, z, grad_x, grad_y, grad_z, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_685114fbd109.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_685114fbd109.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_685114fbd109.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_685114fbd109.py:N]: y_tile = y[tile]
    y_tile = tl.load(y + indices_0 * 1, None)
    # src[helion_bwd_685114fbd109.py:N]: z_tile = z[tile]
    z_tile = tl.load(z + indices_0 * 1, None)
    # src[helion_bwd_685114fbd109.py:N]: mul_val = torch.mul(x_tile, y_tile)
    v_0 = x_tile * y_tile
    # src[helion_bwd_685114fbd109.py:N]: mul_2_val = torch.mul(grad_out_tile, mul_val)
    v_1 = grad_out_tile * v_0
    # src[helion_bwd_685114fbd109.py:N]: mul_3_val = torch.mul(grad_out_tile, z_tile)
    v_2 = grad_out_tile * z_tile
    # src[helion_bwd_685114fbd109.py:N]: mul_4_val = torch.mul(mul_3_val, x_tile)
    v_3 = v_2 * x_tile
    # src[helion_bwd_685114fbd109.py:N]: mul_5_val = torch.mul(mul_3_val, y_tile)
    v_4 = v_2 * y_tile
    # src[helion_bwd_685114fbd109.py:N]: grad_x[tile] = mul_5_val
    tl.store(grad_x + indices_0 * 1, v_4, None)
    # src[helion_bwd_685114fbd109.py:N]: grad_y[tile] = mul_4_val
    tl.store(grad_y + indices_0 * 1, v_3, None)
    # src[helion_bwd_685114fbd109.py:N]: grad_z[tile] = mul_2_val
    tl.store(grad_z + indices_0 * 1, v_1, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_685114fbd109.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_685114fbd109.py:N]: grad_y = torch.empty_like(y)
    grad_y = torch.empty_like(y)
    # src[helion_bwd_685114fbd109.py:N]: grad_z = torch.empty_like(z)
    grad_z = torch.empty_like(z)
    # src[helion_bwd_685114fbd109.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_685114fbd109.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_685114fbd109.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_685114fbd109.py:N]:     x_tile = x[tile]
    # src[helion_bwd_685114fbd109.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, y, z, grad_x, grad_y, grad_z, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_685114fbd109.py:N]: return (grad_x, grad_y, grad_z)
    return (grad_x, grad_y, grad_z)

--- assertExpectedJournal(TestAutodiff.test_x_squared)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        mul_1_val = torch.mul(grad_out_tile, x_tile)
        add_val = torch.add(mul_1_val, mul_1_val)
        grad_x[tile] = add_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_x_squared)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_5789a7e3e64f.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_5789a7e3e64f.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_5789a7e3e64f.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_5789a7e3e64f.py:N]: mul_1_val = torch.mul(grad_out_tile, x_tile)
    v_0 = grad_out_tile * x_tile
    # src[helion_bwd_5789a7e3e64f.py:N]: add_val = torch.add(mul_1_val, mul_1_val)
    v_1 = v_0 + v_0
    # src[helion_bwd_5789a7e3e64f.py:N]: grad_x[tile] = add_val
    tl.store(grad_x + indices_0 * 1, v_1, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_5789a7e3e64f.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_5789a7e3e64f.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_5789a7e3e64f.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_5789a7e3e64f.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_5789a7e3e64f.py:N]:     x_tile = x[tile]
    # src[helion_bwd_5789a7e3e64f.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_5789a7e3e64f.py:N]: return grad_x
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_x_times_sin)
"""
Auto-generated Helion backward kernel.
"""

import torch
import helion
from helion import language as hl

@helion.kernel()
def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    grad_x = torch.empty_like(x)
    for tile in hl.tile(grad_x.shape):
        grad_out_tile = grad_out[tile]
        x_tile = x[tile]
        mul_1_val = torch.mul(grad_out_tile, x_tile)
        sin_val = torch.sin(x_tile)
        mul_2_val = torch.mul(grad_out_tile, sin_val)
        cos_val = torch.cos(x_tile)
        mul_3_val = torch.mul(mul_1_val, cos_val)
        add_val = torch.add(mul_2_val, mul_3_val)
        grad_x[tile] = add_val
    return grad_x

--- assertExpectedJournal(TestAutodiff.test_x_times_sin)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_backward_kernel(grad_out, x, grad_x, _BLOCK_SIZE_0: tl.constexpr):
    # src[helion_bwd_bddf380a8245.py:N]: for tile in hl.tile(grad_x.shape):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[helion_bwd_bddf380a8245.py:N]: grad_out_tile = grad_out[tile]
    grad_out_tile = tl.load(grad_out + indices_0 * 1, None)
    # src[helion_bwd_bddf380a8245.py:N]: x_tile = x[tile]
    x_tile = tl.load(x + indices_0 * 1, None)
    # src[helion_bwd_bddf380a8245.py:N]: mul_1_val = torch.mul(grad_out_tile, x_tile)
    v_0 = grad_out_tile * x_tile
    # src[helion_bwd_bddf380a8245.py:N]: sin_val = torch.sin(x_tile)
    v_1 = tl_math.sin(x_tile)
    # src[helion_bwd_bddf380a8245.py:N]: mul_2_val = torch.mul(grad_out_tile, sin_val)
    v_2 = grad_out_tile * v_1
    # src[helion_bwd_bddf380a8245.py:N]: cos_val = torch.cos(x_tile)
    v_3 = tl_math.cos(x_tile)
    # src[helion_bwd_bddf380a8245.py:N]: mul_3_val = torch.mul(mul_1_val, cos_val)
    v_4 = v_0 * v_3
    # src[helion_bwd_bddf380a8245.py:N]: add_val = torch.add(mul_2_val, mul_3_val)
    v_5 = v_2 + v_4
    # src[helion_bwd_bddf380a8245.py:N]: grad_x[tile] = add_val
    tl.store(grad_x + indices_0 * 1, v_5, None)

def backward_kernel(grad_out: torch.Tensor, x: torch.Tensor, *, _launcher=_default_launcher):
    # src[helion_bwd_bddf380a8245.py:N]: grad_x = torch.empty_like(x)
    grad_x = torch.empty_like(x)
    # src[helion_bwd_bddf380a8245.py:N]: for tile in hl.tile(grad_x.shape):
    _BLOCK_SIZE_0 = 32
    # src[helion_bwd_bddf380a8245.py:N]: for tile in hl.tile(grad_x.shape):
    # src[helion_bwd_bddf380a8245.py:N]:     grad_out_tile = grad_out[tile]
    # src[helion_bwd_bddf380a8245.py:N]:     x_tile = x[tile]
    # src[helion_bwd_bddf380a8245.py:N-N]: ...
    _launcher(_helion_backward_kernel, (triton.cdiv(64, _BLOCK_SIZE_0),), grad_out, x, grad_x, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[helion_bwd_bddf380a8245.py:N]: return grad_x
    return grad_x
