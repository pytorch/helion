This file is automatically generated by assertExpectedJournal calls in test_shape_bucketing.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestShapeBucketing.test_nested_tile_none_mode_reuse)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_nested_tile_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, m, n, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    # src[test_shape_bucketing.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[test_shape_bucketing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + 1.0
    for offset_1 in tl.range(0, tl.cast(n, tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < n
        # src[test_shape_bucketing.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] + 1.0
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_0 = 1.0
        v_1 = load + v_0
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_0[:, None] & mask_1[None, :])

def nested_tile_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_shape_bucketing.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_shape_bucketing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_shape_bucketing.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    _BLOCK_SIZE_0 = 32
    # src[test_shape_bucketing.py:N]: for tile_n in hl.tile(n, block_size=block_size_n):
    # src[test_shape_bucketing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + 1.0
    _BLOCK_SIZE_1 = 32
    # src[test_shape_bucketing.py:N]: for tile_m in hl.tile(m, block_size=block_size_m):
    # src[test_shape_bucketing.py:N]:     for tile_n in hl.tile(n, block_size=block_size_n):
    # src[test_shape_bucketing.py:N]:         out[tile_m, tile_n] = x[tile_m, tile_n] + 1.0
    _launcher(_helion_nested_tile_kernel, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), m, n, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = x_size_0
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_1[None, :])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (x.size(0) * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1
    indices_1 = offset_1 + tl.zeros([1], tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None], other=0)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_0[:, None])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * x.size(1),), x, out, x.size(0), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_0, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, out_stride_0, out_stride_1, x_stride_0, x_stride_1):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = x_size_0
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1
    indices_1 = offset_1 + tl.zeros([1], tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), None)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (x.size(0) * x.size(1),), x, out, x.size(0), out.stride(0), out.stride(1), x.stride(0), x.stride(1), num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_1, out_stride_1, x_stride_1, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = 1
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + indices_1[None, :] * x_stride_1, mask_1[None, :], other=0), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(indices_1[None, :] * out_stride_1, [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), v_1, mask_1[None, :])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    _BLOCK_SIZE_0 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (1 * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, out, x.size(1), out.stride(1), x.stride(1), _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_0[:, None] & mask_1[None, :])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + indices_0[:, None] * x_stride_0, mask_0[:, None], other=0), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(indices_0[:, None] * out_stride_0, [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), v_1, mask_0[:, None])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    _BLOCK_SIZE_1 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * 1,), x, out, x.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_0[:, None] & mask_1[None, :])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 8
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = 1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + tl.zeros([_BLOCK_SIZE_0, _BLOCK_SIZE_1], tl.int32), None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(tl.zeros([_BLOCK_SIZE_0, _BLOCK_SIZE_1], tl.int32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), tl.reshape(v_1, []), None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    _BLOCK_SIZE_0 = 1
    _BLOCK_SIZE_1 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (1 * 1,), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_1, mask_0[:, None] & mask_1[None, :])

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 8
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = 1
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + indices_1[None, :] * 1, None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(indices_1[None, :] * 1, [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    _BLOCK_SIZE_0 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (1 * triton.cdiv(16, _BLOCK_SIZE_1),), x, out, _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(4, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * 16 + indices_1[None, :] * 1), None)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * 16 + indices_1[None, :] * 1), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(4, _BLOCK_SIZE_0) * triton.cdiv(16, _BLOCK_SIZE_1),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + indices_0[:, None] * 1, None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(indices_0[:, None] * 1, [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    _BLOCK_SIZE_1 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(16, _BLOCK_SIZE_0) * 1,), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * 8 + indices_1[None, :] * 1), None)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * 8 + indices_1[None, :] * 1), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 8
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(8, _BLOCK_SIZE_1),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = 1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.broadcast_to(tl.load(x + tl.zeros([_BLOCK_SIZE_0, _BLOCK_SIZE_1], tl.int32), None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + tl.broadcast_to(tl.zeros([_BLOCK_SIZE_0, _BLOCK_SIZE_1], tl.int32), [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), tl.reshape(v_1, []), None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    _BLOCK_SIZE_0 = 1
    _BLOCK_SIZE_1 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (1 * 1,), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_pointwise_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pointwise_add_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(4, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile] + 1.0
    load = tl.load(x + (indices_0[:, None] * 8 + indices_1[None, :] * 1), None)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None] * 8 + indices_1[None, :] * 1), v_1, None)

def pointwise_add_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    """Simple pointwise kernel: out = x + 1.0"""
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 8
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size()):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile] + 1.0
    _launcher(_helion_pointwise_add_kernel, (triton.cdiv(4, _BLOCK_SIZE_0) * triton.cdiv(8, _BLOCK_SIZE_1),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_1, out_stride_0, x_stride_0, x_stride_1, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (x.size(0),), x, out, x.size(1), out.stride(0), x.stride(0), x.stride(1), _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_1, x_stride_1, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.broadcast_to(tl.load(x + indices_1[None, :] * x_stride_1, mask_1[None, :], other=0), [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + tl.broadcast_to(tl.zeros([_BLOCK_SIZE_0], tl.int32), [_BLOCK_SIZE_0]), tl.reshape(sum_1, []), None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    _BLOCK_SIZE_0 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (1,), x, out, x.size(1), x.stride(1), _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.broadcast_to(tl.load(x + indices_1[None, :] * 1, None), [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + tl.broadcast_to(tl.zeros([_BLOCK_SIZE_0], tl.int32), [_BLOCK_SIZE_0]), tl.reshape(sum_1, []), None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _RDIM_SIZE_1 = 64
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    _BLOCK_SIZE_0 = 1
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (1,), x, out, _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 4
    _RDIM_SIZE_1 = 64
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(4, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 64
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(2, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_all_modes_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = 64
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(8, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_over_size1_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_over_size1_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + indices_0[:, None] * x_stride_0, mask_0[:, None], other=0)
    sum_1 = tl.reshape(load, [_BLOCK_SIZE_0])
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_over_size1_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + indices_0[:, None] * 1, None)
    sum_1 = tl.reshape(load, [_BLOCK_SIZE_0])
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + indices_0[:, None] * x_stride_0, mask_0[:, None], other=0)
    sum_1 = tl.reshape(load, [_BLOCK_SIZE_0])
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < x_size_1
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(x.size(1))
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 16 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 16
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + indices_0[:, None] * 1, None)
    sum_1 = tl.reshape(load, [_BLOCK_SIZE_0])
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out

--- assertExpectedJournal(TestShapeBucketing.test_reduction_varying_reduction_dim)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduction_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_shape_bucketing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """Reduction kernel: sum along last dimension."""
    # src[test_shape_bucketing.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[test_shape_bucketing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_shape_bucketing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_shape_bucketing.py:N]: return out
    return out
