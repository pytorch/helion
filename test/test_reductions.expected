This file is automatically generated by assertExpectedJournal calls in test_reductions.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestReductions.test_argmin_argmax)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

# src[test_reductions.py:N]: def reduce_kernel(
# src[test_reductions.py:N]:     x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32
# src[test_reductions.py:N]: ) -> torch.Tensor:
# src[test_reductions.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_reduce_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x_desc = tl.make_tensor_descriptor(x, [512, 512], [512, 1], [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    load = x_desc.load([offset_0, 0])
    argmax = tl.cast(triton_helpers.max_with_index(load, tl.broadcast_to(indices_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), 1)[1].to(tl.int64), tl.int64)
    tl.store(out + indices_0 * 1, argmax, None)

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = 512
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_argmin_argmax_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_reduce_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    argmax_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    argmax_acc_index = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 2147483647, tl.int32)
    for roffset_1 in tl.range(0, 512, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        load = tl.load(x + (indices_0[:, None] * 512 + rindex_1[None, :] * 1), None)
        argmax_acc, argmax_acc_index = triton_helpers.maximum_with_index(argmax_acc, argmax_acc_index, load, tl.broadcast_to(rindex_1[None, :], [_BLOCK_SIZE_0, _REDUCTION_BLOCK_1]))
    argmax = tl.cast(triton_helpers.max_with_index(argmax_acc, argmax_acc_index, 1)[1].to(tl.int64), tl.int64)
    tl.store(out + indices_0 * 1, argmax, None)

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    _BLOCK_SIZE_0 = 1
    _REDUCTION_BLOCK_1 = 16
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, (512,), x, out, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_broken_layernorm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    acc = tl.load(x + (indices_0[:, None] * 2 + indices_1[None, :] * 1), None)
    # src[test_reductions.py:N]: mean = hl.full([n], 0.0, acc.dtype)
    mean = tl.full([2], 0.0, tl.float32)
    # src[test_reductions.py:N]: count = hl.arange(0, acc.shape[1], 1)
    count = tl.arange(0, 2)
    # src[test_reductions.py:N]: delta = acc - mean
    v_0 = mean[None, :]
    v_1 = acc - v_0
    # src[test_reductions.py:N]: mean = delta / count[None, :]
    subscript = count[None, :]
    v_2 = tl.cast(subscript, tl.float32)
    v_3 = v_1 / v_2
    # src[test_reductions.py:N]: delta2 = acc - mean.sum(-1)[:, None]
    sum_1 = tl.cast(tl.sum(v_3, 1), tl.float32)
    subscript_1 = sum_1[:, None]
    v_4 = acc - subscript_1
    # src[test_reductions.py:N]: m2 = delta * delta2
    v_5 = v_1 * v_4
    # src[test_reductions.py:N]: var = m2 / n
    v_6 = 0.5
    v_7 = v_5 * v_6
    # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
    v_8 = acc - v_3
    v_9 = v_7 + eps
    v_10 = tl.rsqrt(v_9)
    v_11 = v_8 * v_10
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_12 = load_1[None, :]
    v_13 = v_11 * v_12
    # src[test_reductions.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, None)
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    # src[test_reductions.py:N]:     bias[:].to(torch.float32)
    # src[test_reductions.py:N]: )
    v_14 = load_2[None, :]
    v_15 = v_13 + v_14
    # src[test_reductions.py:N]: out[tile_m, :] = acc
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * 2 + indices_1[None, :] * 1), v_16, None)

def layer_norm_fwd(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 2
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     mean = hl.full([n], 0.0, acc.dtype)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(2, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_fp16_math_ops_fp32_fallback)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rsqrt_fp16_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: result[tile] = torch.rsqrt(x[tile])
    load = tl.load(x + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    v_1 = tl.rsqrt(v_0)
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(result + indices_0 * 1, v_2, None)

def rsqrt_fp16_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: result = torch.empty_like(x)
    result = torch.empty_like(x)
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_reductions.py:N]:     # This should now work via fp32 fallback
    # src[test_reductions.py:N]:     result[tile] = torch.rsqrt(x[tile])
    _launcher(_helion_rsqrt_fp16_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return result
    return result

--- assertExpectedJournal(TestReductions.test_fp16_math_ops_fp32_fallback)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_multi_math_ops_fp16_kernel(x, result, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: result[tile, 0] = torch.rsqrt(x[tile])
    load = tl.load(x + indices_0 * 1, None)
    v_0 = tl.cast(load, tl.float32)
    v_1 = tl.rsqrt(v_0)
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(result + (indices_0 * 8 + 0 * 1), v_2, None)
    # src[test_reductions.py:N]: result[tile, 1] = torch.sqrt(x[tile])
    load_1 = tl.load(x + indices_0 * 1, None)
    v_3 = tl.cast(load_1, tl.float32)
    v_4 = tl.sqrt_rn(v_3)
    v_5 = tl.cast(v_4, tl.float16)
    tl.store(result + (indices_0 * 8 + 1 * 1), v_5, None)
    # src[test_reductions.py:N]: result[tile, 2] = torch.sin(x[tile])
    load_2 = tl.load(x + indices_0 * 1, None)
    v_6 = tl.cast(load_2, tl.float32)
    v_7 = tl_math.sin(v_6)
    v_8 = tl.cast(v_7, tl.float16)
    tl.store(result + (indices_0 * 8 + 2 * 1), v_8, None)
    # src[test_reductions.py:N]: result[tile, 3] = torch.cos(x[tile])
    load_3 = tl.load(x + indices_0 * 1, None)
    v_9 = tl.cast(load_3, tl.float32)
    v_10 = tl_math.cos(v_9)
    v_11 = tl.cast(v_10, tl.float16)
    tl.store(result + (indices_0 * 8 + 3 * 1), v_11, None)
    # src[test_reductions.py:N]: result[tile, 4] = torch.log(x[tile])
    load_4 = tl.load(x + indices_0 * 1, None)
    v_12 = tl.cast(load_4, tl.float32)
    v_13 = tl_math.log(v_12)
    v_14 = tl.cast(v_13, tl.float16)
    tl.store(result + (indices_0 * 8 + 4 * 1), v_14, None)
    # src[test_reductions.py:N]: result[tile, 5] = torch.tanh(x[tile])
    load_5 = tl.load(x + indices_0 * 1, None)
    v_15 = tl.cast(load_5, tl.float32)
    v_16 = libdevice.tanh(v_15)
    v_17 = tl.cast(v_16, tl.float16)
    tl.store(result + (indices_0 * 8 + 5 * 1), v_17, None)
    # src[test_reductions.py:N]: result[tile, 6] = torch.log1p(x[tile])
    load_6 = tl.load(x + indices_0 * 1, None)
    v_18 = tl.cast(load_6, tl.float32)
    v_19 = libdevice.log1p(v_18)
    v_20 = tl.cast(v_19, tl.float16)
    tl.store(result + (indices_0 * 8 + 6 * 1), v_20, None)
    # src[test_reductions.py:N]: result[tile, 7] = torch.exp(x[tile])
    load_7 = tl.load(x + indices_0 * 1, None)
    v_21 = tl.cast(load_7, tl.float32)
    v_22 = libdevice.exp(v_21)
    v_23 = tl.cast(v_22, tl.float16)
    tl.store(result + (indices_0 * 8 + 7 * 1), v_23, None)

def multi_math_ops_fp16_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: result = torch.empty([x.size(0), 8], dtype=x.dtype, device=x.device)
    result = torch.empty([x.size(0), 8], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 16
    # src[test_reductions.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_reductions.py:N]:     # Test multiple operations that have confirmed fallbacks
    # src[test_reductions.py:N]:     result[tile, 0] = torch.rsqrt(x[tile])
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_multi_math_ops_fp16_kernel, (triton.cdiv(16, _BLOCK_SIZE_0),), x, result, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return result
    return result

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd_repro(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    x_part = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    v_0 = tl.cast(x_part, tl.float32)
    var_mean_extra = tl.cast(tl.reshape(tl.sum(v_0, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_1 = 64
    v_2 = var_mean_extra / v_1.to(tl.float32)
    v_3 = tl.cast(x_part, tl.float32)
    v_4 = v_3 - v_2
    v_5 = v_4 * v_4
    var_mean_extra_2 = tl.cast(tl.reshape(tl.sum(v_5, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_6 = 64
    v_7 = var_mean_extra_2 / v_6.to(tl.float32)
    v_8 = tl.cast(v_7, tl.float16)
    v_9 = tl.cast(v_2, tl.float16)
    # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
    v_10 = x_part - v_9
    v_11 = tl.cast(v_8, tl.float32)
    v_12 = v_11 + eps
    v_13 = tl.rsqrt(v_12)
    v_14 = tl.cast(v_10, tl.float32)
    v_15 = v_14 * v_13
    # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_16 = tl.cast(load_1, tl.float32)
    v_17 = v_16[None, :]
    v_18 = v_15 * v_17
    # src[test_reductions.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, None)
    v_19 = tl.cast(load_2, tl.float32)
    # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
    # src[test_reductions.py:N]:     bias[:].to(torch.float32)
    # src[test_reductions.py:N]: )
    v_20 = v_19[None, :]
    v_21 = v_18 + v_20
    v_22 = tl.cast(v_21, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_22, None)

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     x_part = x[tile_m, :]
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_repro, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd_repro(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    var_mean_extra_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        x_part = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_0 = tl.cast(x_part, tl.float32)
        v_1 = var_mean_extra_acc + v_0
        var_mean_extra_acc = v_1
    var_mean_extra = tl.cast(tl.reshape(tl.sum(var_mean_extra_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_2 = 64
    v_3 = var_mean_extra / v_2.to(tl.float32)
    var_mean_extra_2_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_3_copy = v_3
        x_part_1 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
        v_4 = tl.cast(x_part_1, tl.float32)
        v_5 = v_4 - v_3_copy
        v_6 = v_5 * v_5
        v_7 = var_mean_extra_2_acc + v_6
        var_mean_extra_2_acc = v_7
    var_mean_extra_2 = tl.cast(tl.reshape(tl.sum(var_mean_extra_2_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_8 = 64
    v_9 = var_mean_extra_2 / v_8.to(tl.float32)
    v_10 = tl.cast(v_9, tl.float16)
    v_11 = tl.cast(v_3, tl.float16)
    # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
    v_12 = tl.cast(v_10, tl.float32)
    v_13 = v_12 + eps
    v_14 = tl.rsqrt(v_13)
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_11_copy = v_11
        v_14_copy = v_14
        x_part_2 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        # src[test_reductions.py:N]: normalized = (x_part - mean) * torch.rsqrt(var.to(torch.float32) + eps)
        v_15 = x_part_2 - v_11_copy
        v_16 = tl.cast(v_15, tl.float32)
        v_17 = v_16 * v_14_copy
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        load_1 = tl.load(weight + rindex_1 * 1, None)
        v_18 = tl.cast(load_1, tl.float32)
        v_19 = v_18[None, :]
        v_20 = v_17 * v_19
        # src[test_reductions.py:N]: bias[:].to(torch.float32)
        load_2 = tl.load(bias + rindex_1 * 1, None)
        v_21 = tl.cast(load_2, tl.float32)
        # src[test_reductions.py:N]: out[tile_m, :] = normalized * (weight[:].to(torch.float32)) + (
        # src[test_reductions.py:N]:     bias[:].to(torch.float32)
        # src[test_reductions.py:N]: )
        v_22 = v_21[None, :]
        v_23 = v_20 + v_22
        v_24 = tl.cast(v_23, tl.float16)
        tl.store(out + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), v_24, None)

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: x_part = x[tile_m, :]
    _REDUCTION_BLOCK_1 = 8
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     x_part = x[tile_m, :]
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(x_part, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_repro, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_layer_norm_nonpow2_reduction)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd_nonpow2(x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 1536
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    load = tl.load(x + (indices_0[:, None] * 1536 + indices_1[None, :] * 1), mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    # src[test_reductions.py:N]: mean_val = torch.sum(acc, dim=-1) / n
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = 0.0006510416666666666
    v_2 = sum_1 * v_1
    # src[test_reductions.py:N]: centered = acc - mean_val[:, None]
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    # src[test_reductions.py:N]: var_val = torch.sum(centered * centered, dim=-1) / n
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
    v_5 = 0.0006510416666666666
    v_6 = sum_2 * v_5
    # src[test_reductions.py:N]: rstd_val = torch.rsqrt(var_val + eps)
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    # src[test_reductions.py:N]: normalized = centered * rstd_val[:, None]
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    load_1 = tl.load(weight + indices_1 * 1, mask_1, other=0)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    # src[test_reductions.py:N]: bias[:].to(torch.float32)
    load_2 = tl.load(bias + indices_1 * 1, mask_1, other=0)
    v_13 = tl.cast(load_2, tl.float32)
    # src[test_reductions.py:N]: acc = normalized * (weight[:].to(torch.float32)) + (
    # src[test_reductions.py:N]:     bias[:].to(torch.float32)
    # src[test_reductions.py:N]: )
    v_14 = v_13[None, :]
    v_15 = v_12 + v_14
    # src[test_reductions.py:N]: out[tile_m, :] = acc.to(x.dtype)
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * 1536 + indices_1[None, :] * 1), v_16, mask_1[None, :])
    # src[test_reductions.py:N]: mean[tile_m] = mean_val
    tl.store(mean + indices_0 * 1, v_2, None)
    # src[test_reductions.py:N]: rstd[tile_m] = rstd_val
    tl.store(rstd + indices_0 * 1, v_8, None)

def layer_norm_fwd_nonpow2(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: mean = torch.empty([m], dtype=torch.float32, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 2048
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     # Compute mean
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_fwd_nonpow2, (triton.cdiv(4096, _BLOCK_SIZE_0),), x, weight, bias, out, mean, rstd, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=4)
    # src[test_reductions.py:N]: return out, mean, rstd
    return (out, mean, rstd)

--- assertExpectedJournal(TestReductions.test_mean)
def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32):
    n, _m = 
    # Call: SequenceType((LiteralType(512), LiteralType(512))) SourceOrigin(location=<SourceLocation test_reductions.py:51>)
    # Attribute: TensorAttributeType AttributeOrigin(value=ArgumentOrigin(name='x'), key='size')
    # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x.size()
    out = 
    # Call: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:52>)
    # Attribute: CallableType(_VariableFunctionsClass.empty) AttributeOrigin(value=GlobalOrigin(name='torch'), key='empty')
    # Name: PythonModuleType(torch) GlobalOrigin(name='torch')
torch.empty(
    # List: SequenceType([LiteralType(512)]) SourceOrigin(location=<SourceLocation test_reductions.py:53>)
[
    # Name: LiteralType(512) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:51>), key=0)
n], dtype=
    # Name: LiteralType(torch.float32) ArgumentOrigin(name='out_dtype')
out_dtype, device=
    # Attribute: LiteralType(device=DEVICE) AttributeOrigin(value=ArgumentOrigin(name='x'), key='device')
    # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x.device)
    # For: loop_type=GRID

    for tile_n in 
    # Call: IterType(TileIndexType(0)) SourceOrigin(location=<SourceLocation test_reductions.py:57>)
    # Attribute: CallableType(tile) AttributeOrigin(value=GlobalOrigin(name='hl'), key='tile')
    # Name: PythonModuleType(helion.language) GlobalOrigin(name='hl')
hl.tile(
    # Name: LiteralType(512) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:51>), key=0)
n):
        
        # Subscript: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
        # Name: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:52>)
out[
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:57>)
tile_n] = 
        # Call: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
        # Name: CallableType(_VariableFunctionsClass.mean) ArgumentOrigin(name='fn')
fn(
        # Subscript: TensorType([block_size_0, rdim_1], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
        # Name: TensorType([512, 512], torch.float32) ArgumentOrigin(name='x')
x[
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:57>)
tile_n, 
        # Slice: SliceType(LiteralType(None):LiteralType(None):LiteralType(None)) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
:], dim=
        # UnaryOp: LiteralType(-1) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
-
        # Constant: LiteralType(1) DeviceOrigin(location=<SourceLocation test_reductions.py:58>)
1)
    return 
    # Name: TensorType([512], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:52>)
out

def root_graph_0():
    # File: .../test_reductions.py:58 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[512, 512]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None, None);  x = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [mean_extra]);  mean_extra = None
    out: "f32[512]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

def reduction_loop_1():
    # File: .../test_reductions.py:58 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[512, 512]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None, None);  x = block_size_0 = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    return [mean_extra]

def root_graph_2():
    # File: .../test_reductions.py:58 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    _get_symnode = helion_language__tracing_ops__get_symnode('rdim1')
    _for_loop = helion_language__tracing_ops__for_loop(1, [0], [_get_symnode], []);  _get_symnode = None
    getitem: "f32[u0]" = _for_loop[0];  _for_loop = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [getitem]);  getitem = None
    out: "f32[512]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

--- assertExpectedJournal(TestReductions.test_mean)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_reductions.py:N]: def reduce_kernel(
# src[test_reductions.py:N]:     x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32
# src[test_reductions.py:N]: ) -> torch.Tensor:
# src[test_reductions.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_reduce_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x_desc = tl.make_tensor_descriptor(x, [512, 512], [512, 1], [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: out[tile_n] = fn(x[tile_n, :], dim=-1)
    load = x_desc.load([offset_0, 0])
    mean_extra = tl.cast(tl.sum(load, 1), tl.float32)
    v_0 = 512
    v_1 = mean_extra / v_0.to(tl.float32)
    tl.store(out + indices_0 * 1, v_1, None)

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=out_dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = 512
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = fn(x[tile_n, :], dim=-1)
    _launcher(_helion_reduce_kernel, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_reduction_loops_integer_values)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_reduction(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
    var_mean_extra_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        load = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_0 = tl.cast(load, tl.float32)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
        v_1 = var_mean_extra_acc + v_0
        var_mean_extra_acc = v_1
    var_mean_extra = tl.cast(tl.reshape(tl.sum(var_mean_extra_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_2 = 64
    v_3 = var_mean_extra / v_2.to(tl.float32)
    var_mean_extra_2_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_3_copy = v_3
        load_1 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_4 = tl.cast(load_1, tl.float32)
        # src[test_reductions.py:N]: var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
        v_5 = v_4 - v_3_copy
        v_6 = v_5 * v_5
        v_7 = var_mean_extra_2_acc + v_6
        var_mean_extra_2_acc = v_7
    var_mean_extra_2 = tl.cast(tl.reshape(tl.sum(var_mean_extra_2_acc, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_8 = 64
    v_9 = var_mean_extra_2 / v_8.to(tl.float32)
    # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
    v_10 = v_9 + eps
    v_11 = tl.rsqrt(v_10)
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_3_copy_1 = v_3
        v_11_copy = v_11
        load_2 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_12 = tl.cast(load_2, tl.float32)
        # src[test_reductions.py:N]: normalized = (acc - mean) * torch.rsqrt(var + eps)
        v_13 = v_12 - v_3_copy_1
        v_14 = v_13 * v_11_copy
        # src[test_reductions.py:N]: result = normalized * (weight[:].to(torch.float32)) + (
        load_3 = tl.load(weight + rindex_1 * 1, None)
        v_15 = tl.cast(load_3, tl.float32)
        v_16 = v_15[None, :]
        v_17 = v_14 * v_16
        # src[test_reductions.py:N]: bias[:].to(torch.float32)
        load_4 = tl.load(bias + rindex_1 * 1, None)
        v_18 = tl.cast(load_4, tl.float32)
        # src[test_reductions.py:N]: result = normalized * (weight[:].to(torch.float32)) + (
        # src[test_reductions.py:N]:     bias[:].to(torch.float32)
        # src[test_reductions.py:N]: )
        v_19 = v_18[None, :]
        v_20 = v_17 + v_19
        # src[test_reductions.py:N]: out[tile_m, :] = result
        v_21 = tl.cast(v_20, tl.float16)
        tl.store(out + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), v_21, None)

def layer_norm_reduction(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: m, n = x.size()
    m, n = x.size()
    # src[test_reductions.py:N]: out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 32
    # src[test_reductions.py:N]: acc = x[tile_m, :].to(torch.float32)
    _REDUCTION_BLOCK_1 = 4
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     acc = x[tile_m, :].to(torch.float32)
    # src[test_reductions.py:N]:     var, mean = torch.var_mean(acc, dim=-1, keepdim=True, correction=0)
    # src[test_reductions.py:N-N]: ...
    _launcher(_helion_layer_norm_reduction, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_sum_kernel(x, out, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 512 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 512
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = x[tile_n, :].sum(-1)
    _launcher(_helion_sum_kernel, (512,), x, out, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum_keepdims)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_reductions.py:N]: def sum_kernel_keepdims(x: torch.Tensor) -> torch.Tensor:
# src[test_reductions.py:N]:     _n, m = x.size()
# src[test_reductions.py:N]:     out = torch.empty(
# src[test_reductions.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_sum_kernel_keepdims(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_reductions.py:N]: out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    x_desc = tl.make_tensor_descriptor(x, [512, 512], [512, 1], [_RDIM_SIZE_1, _BLOCK_SIZE_0])
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    load = x_desc.load([0, offset_0])
    sum_1 = tl.cast(tl.reshape(tl.sum(load, 0), [1, _BLOCK_SIZE_0]), tl.float32)
    tl.store(out + indices_0[None, :] * 1, sum_1, None)

def sum_kernel_keepdims(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: _n, m = x.size()
    _n, m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [1, m],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([1, m], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = 512
    # src[test_reductions.py:N]: for tile_m in hl.tile(m):
    # src[test_reductions.py:N]:     out[:, tile_m] = x[:, tile_m].sum(0, keepdim=True)
    _launcher(_helion_sum_kernel_keepdims, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out

--- assertExpectedJournal(TestReductions.test_sum_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_sum_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    sum_1_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, 512, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        load = tl.load(x + (indices_0[:, None] * 512 + rindex_1[None, :] * 1), None)
        v_0 = sum_1_acc + load
        sum_1_acc = v_0
    sum_1 = tl.cast(tl.sum(sum_1_acc, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_reductions.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_reductions.py:N]: out = torch.empty(
    # src[test_reductions.py:N]:     [n],
    # src[test_reductions.py:N]:     dtype=x.dtype,
    # src[test_reductions.py:N-N]: ...
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    _BLOCK_SIZE_0 = 2
    # src[test_reductions.py:N]: out[tile_n] = x[tile_n, :].sum(-1)
    _REDUCTION_BLOCK_1 = 64
    # src[test_reductions.py:N]: for tile_n in hl.tile(n):
    # src[test_reductions.py:N]:     out[tile_n] = x[tile_n, :].sum(-1)
    _launcher(_helion_sum_kernel, (triton.cdiv(512, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    # src[test_reductions.py:N]: return out
    return out
