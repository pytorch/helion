This file is automatically generated by assertExpectedJournal calls in test_int64_indexing.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestInt64Indexing.test_int32_block_ptr_still_works)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)

@triton.jit
def _helion_add_kernel_int32(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    load = tl.load(tl.make_block_ptr(x, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    load_1 = tl.load(tl.make_block_ptr(y, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    v_0 = load + load_1
    tl.store(tl.make_block_ptr(out, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def add_kernel_int32(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    # src[test_int64_indexing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    _launcher(_helion_add_kernel_int32, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_block_ptr_falls_back_to_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)

@triton.jit
def _helion_add_kernel_int64(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    load_1 = tl.load(y + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = load + load_1
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def add_kernel_int64(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    # src[test_int64_indexing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    _launcher(_helion_add_kernel_int64, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_block_ptr_matmul_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)
_BLOCK_SIZE_2 = tl.constexpr(16)

@triton.jit
def _helion_matmul_int64(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[test_int64_indexing.py:N]: for tile_k in hl.tile(k):
    # src[test_int64_indexing.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 64, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int64)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_int64_indexing.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 64 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 64 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = acc.to(out.dtype)
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def matmul_int64(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: m, k = x.size()
    m, k = x.size()
    # src[test_int64_indexing.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[test_int64_indexing.py:N]: assert k == k2
    assert k == k2
    # src[test_int64_indexing.py:N]: out = torch.empty(
    # src[test_int64_indexing.py:N]:     [m, n],
    # src[test_int64_indexing.py:N]:     dtype=torch.promote_types(x.dtype, y.dtype),
    # src[test_int64_indexing.py:N-N]: ...
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_int64_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[test_int64_indexing.py:N]:     for tile_k in hl.tile(k):
    # src[test_int64_indexing.py:N-N]: ...
    _launcher(_helion_matmul_int64, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_block_ptr_with_explicit_cast)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)

@triton.jit
def _helion_kernel_with_cast(x, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[idx_m, idx_n] = x[tile_m, tile_n] * 2.0
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = 2.0
    v_1 = load * v_0
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_1, None)

def kernel_with_cast(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    # src[test_int64_indexing.py:N]:     # Use explicit cast
    # src[test_int64_indexing.py:N]:     idx_m = tile_m.index.to(torch.int64)
    # src[test_int64_indexing.py:N-N]: ...
    _launcher(_helion_kernel_with_cast, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_block_ptr_with_reduction_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(8)

@triton.jit
def _helion_reduction_sum_int64(x, out, _RDIM_SIZE_1: tl.constexpr):
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0).to(tl.int64)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 128 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_int64(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: m, _ = x.size()
    m, _ = x.size()
    # src[test_int64_indexing.py:N]: out = torch.empty([m], device=x.device, dtype=x.dtype)
    out = torch.empty([m], device=x.device, dtype=x.dtype)
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = 128
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_int64_indexing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_int64, (triton.cdiv(64, _BLOCK_SIZE_0),), x, out, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_block_ptr_with_tile_index_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(32)

@triton.jit
def _helion_pairwise_add_int64(x, out):
    # src[test_int64_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0).to(tl.int64)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    mask_0 = indices_0 < 499
    # src[test_int64_indexing.py:N]: out[tile] = x[tile] + x[tile.index + 1]
    load = tl.load(x + indices_0 * 1, mask_0, other=0)
    v_0 = 1
    v_1 = indices_0 + v_0
    load_1 = tl.load(x + (indices_0 + 1) * 1, mask_0, other=0)
    v_2 = load + load_1
    tl.store(out + indices_0 * 1, v_2, mask_0)

def pairwise_add_int64(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = x.new_empty([x.size(0) - 1])
    out = x.new_empty([x.size(0) - 1])
    # src[test_int64_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_int64_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_int64_indexing.py:N]:     out[tile] = x[tile] + x[tile.index + 1]
    _launcher(_helion_pairwise_add_int64, (triton.cdiv(499, _BLOCK_SIZE_0),), x, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_pointer_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)

@triton.jit
def _helion_add_kernel_int64(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    load_1 = tl.load(y + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = load + load_1
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def add_kernel_int64(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    # src[test_int64_indexing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    _launcher(_helion_add_kernel_int64, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_tensor_descriptor_falls_back_to_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)

@triton.jit
def _helion_add_kernel_int64(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    load = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    load_1 = tl.load(y + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = load + load_1
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def add_kernel_int64(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile(x.size()):
    # src[test_int64_indexing.py:N]:     out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m, tile_n]
    _launcher(_helion_add_kernel_int64, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_tensor_descriptor_matmul_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(16)
_BLOCK_SIZE_1 = tl.constexpr(16)
_BLOCK_SIZE_2 = tl.constexpr(16)

@triton.jit
def _helion_matmul_int64(x, y, out):
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[test_int64_indexing.py:N]: for tile_k in hl.tile(k):
    # src[test_int64_indexing.py:N]:     acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
    for offset_2 in tl.range(0, 64, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int64)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_int64_indexing.py:N]: acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])
        load = tl.load(x + (indices_0[:, None] * 64 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 64 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    # src[test_int64_indexing.py:N]: out[tile_m, tile_n] = acc.to(out.dtype)
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def matmul_int64(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: m, k = x.size()
    m, k = x.size()
    # src[test_int64_indexing.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[test_int64_indexing.py:N]: assert k == k2
    assert k == k2
    # src[test_int64_indexing.py:N]: out = torch.empty(
    # src[test_int64_indexing.py:N]:     [m, n],
    # src[test_int64_indexing.py:N]:     dtype=torch.promote_types(x.dtype, y.dtype),
    # src[test_int64_indexing.py:N-N]: ...
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_int64_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_int64_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[test_int64_indexing.py:N]:     for tile_k in hl.tile(k):
    # src[test_int64_indexing.py:N-N]: ...
    _launcher(_helion_matmul_int64, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), x, y, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_tensor_descriptor_with_reduction_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(8)

@triton.jit
def _helion_reduction_sum_int64(x, out, _RDIM_SIZE_1: tl.constexpr):
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0).to(tl.int64)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    # src[test_int64_indexing.py:N]: out[tile] = x[tile, :].sum(-1)
    load = tl.load(x + (indices_0[:, None] * 128 + indices_1[None, :] * 1), None)
    sum_1 = tl.cast(tl.sum(load, 1), tl.float32)
    tl.store(out + indices_0 * 1, sum_1, None)

def reduction_sum_int64(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: m, _ = x.size()
    m, _ = x.size()
    # src[test_int64_indexing.py:N]: out = torch.empty([m], device=x.device, dtype=x.dtype)
    out = torch.empty([m], device=x.device, dtype=x.dtype)
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = 128
    # src[test_int64_indexing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_int64_indexing.py:N]:     out[tile] = x[tile, :].sum(-1)
    _launcher(_helion_reduction_sum_int64, (triton.cdiv(64, _BLOCK_SIZE_0),), x, out, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_tensor_descriptor_with_tile_index_falls_back)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(32)

@triton.jit
def _helion_pairwise_add_2d_int64(x, out, _RDIM_SIZE_1: tl.constexpr):
    # src[test_int64_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0).to(tl.int64)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    mask_0 = indices_0 < 118
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int64)
    # src[test_int64_indexing.py:N]: tile_offset = tile_m + 10
    v_0 = 10
    v_1 = indices_0 + v_0
    # src[test_int64_indexing.py:N]: out[tile_m, :] = x[tile_offset, :]
    load = tl.load(x + ((indices_0 + 10)[:, None] * 64 + indices_1[None, :] * 1), mask_0[:, None], other=0)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), load, mask_0[:, None])

def pairwise_add_2d_int64(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: M, N = x.size()
    M, N = x.size()
    # src[test_int64_indexing.py:N]: out = x.new_empty(M - 10, N)
    out = x.new_empty(M - 10, N)
    # src[test_int64_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[test_int64_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    # src[test_int64_indexing.py:N]:     # Use tile + offset pattern
    # src[test_int64_indexing.py:N]:     tile_offset = tile_m + 10
    # src[test_int64_indexing.py:N-N]: ...
    _launcher(_helion_pairwise_add_2d_int64, (triton.cdiv(118, _BLOCK_SIZE_0),), x, out, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestInt64Indexing.test_int64_with_loaded_indices)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

_BLOCK_SIZE_0 = tl.constexpr(8)
_BLOCK_SIZE_1 = tl.constexpr(8)

@triton.jit
def _helion_gather_kernel_int64(index_tensor, input_tensor, out):
    # src[test_int64_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    num_blocks_0 = tl.cdiv(32, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0).to(tl.int64) % num_blocks_0
    pid_1 = tl.program_id(0).to(tl.int64) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int64)
    # src[test_int64_indexing.py:N]: indices = index_tensor[tile_n, tile_k]
    indices = tl.load(index_tensor + (indices_0[:, None] * 16 + indices_1[None, :] * 1), None)
    # src[test_int64_indexing.py:N]: out[tile_n, tile_k] = input_tensor[tile_n.index[:, None], indices]
    load_1 = indices_0[:, None]
    load_2 = tl.load(input_tensor + (load_1 * 64 + indices * 1), None)
    tl.store(out + (indices_0[:, None] * 16 + indices_1[None, :] * 1), load_2, None)

def gather_kernel_int64(input_tensor: torch.Tensor, index_tensor: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_int64_indexing.py:N]: N, M = input_tensor.size()
    N, M = input_tensor.size()
    # src[test_int64_indexing.py:N]: K = index_tensor.size(1)
    K = index_tensor.size(1)
    # src[test_int64_indexing.py:N]: out = torch.empty(
    # src[test_int64_indexing.py:N]:     N, K, dtype=input_tensor.dtype, device=input_tensor.device
    # src[test_int64_indexing.py:N]: )
    out = torch.empty(N, K, dtype=input_tensor.dtype, device=input_tensor.device)
    # src[test_int64_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    # src[test_int64_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    # src[test_int64_indexing.py:N]:     # indices loaded as int64
    # src[test_int64_indexing.py:N]:     indices = index_tensor[tile_n, tile_k]
    # src[test_int64_indexing.py:N-N]: ...
    _RDIM_SIZE_2 = triton.next_power_of_2(_BLOCK_SIZE_0)
    _launcher(_helion_gather_kernel_int64, (triton.cdiv(32, _BLOCK_SIZE_0) * triton.cdiv(16, _BLOCK_SIZE_1),), index_tensor, input_tensor, out, num_warps=4, num_stages=1)
    # src[test_int64_indexing.py:N]: return out
    return out
