name: Benchmark

on:
  workflow_call:
    inputs:
      runner:
        required: true
        type: string
      python-version:
        required: true
        type: string
      image:
        required: true
        type: string
      runtime-version:
        required: true
        type: string
      container-options:
        required: true
        type: string
      alias:
        required: true
        type: string
      kernels:
        required: true
        type: string
      env-vars:
        required: false
        type: string
        default: ""
      custom-args:
        required: false
        type: string
        default: ""

jobs:
  benchmark:
    name: benchmark-${{ inputs.runtime-version }}-${{ inputs.kernels }}-py${{ inputs.python-version }}-${{ inputs.alias }}

    container:
      image: ${{ inputs.image }}
      options: ${{ inputs.container-options }}

    runs-on: ${{ inputs.runner }}
    permissions:
      id-token: write
      contents: read

    defaults:
      run:
        shell: bash -l {0}

    outputs:
      benchmark-metadata: ${{ steps.gather-benchmark-metadata.outputs.benchmark-metadata }}
      runners-info: ${{ steps.gather-runners-info.outputs.runners-info }}
      dependencies: ${{ steps.gather-dependencies.outputs.dependencies }}

    steps:
      - name: Run NVIDIA command
        if: startsWith(inputs.image, 'nvidia')
        run: |
          echo "Detected NVIDIA image"
          nvidia-smi || echo "nvidia-smi not found"

      - name: Run ROCm command
        if: startsWith(inputs.image, 'rocm')
        run: |
          echo "Detected ROCm image"
          rocminfo || echo "rocminfo not found"

      - name: Setup toolchain
        run: |
          scripts/ci/run_benchmarks.sh "Setup toolchain"

      - name: Check out code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Create virtual environment
        env:
          PYTHON_VERSION: ${{ inputs.python-version }}
        run: |
          scripts/ci/run_benchmarks.sh "Create virtual environment"

      - name: Install PyTorch
        env:
          RUNTIME_VERSION: ${{ inputs.runtime-version }}
        run: |
          scripts/ci/run_benchmarks.sh "Install PyTorch"

      - name: Install Helion
        run: |
          scripts/ci/run_benchmarks.sh "Install Helion"

      - name: Install Benchmark Requirements
        run: |
          scripts/ci/run_benchmarks.sh "Install Benchmark Requirements"

      - name: Run Benchmark
        env:
          KERNELS: ${{ inputs.kernels }}
          ENV_VARS: ${{ inputs.env-vars }}
          CUSTOM_ARGS: ${{ inputs.custom-args }}
        run: |
          scripts/ci/run_benchmarks.sh "Run Benchmark"

      - name: Gather benchmark metadata
        id: gather-benchmark-metadata
        uses: pytorch/test-infra/.github/actions/gather-benchmark-metadata@main
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          venv: .venv/bin/activate

      - name: Gather runners info
        id: gather-runners-info
        uses: pytorch/test-infra/.github/actions/gather-runners-info@main
        with:
          venv: .venv/bin/activate

      - name: Gather dependencies
        id: gather-dependencies
        uses: pytorch/test-infra/.github/actions/gather-dependencies@main
        with:
          venv: .venv/bin/activate

      - name: Upload the benchmark results to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ inputs.alias }}-${{ inputs.kernels }}
          path: test/test-reports

  upload-benchmark-results:
    needs: benchmark
    uses: pytorch/test-infra/.github/workflows/upload_benchmark_results.yml@main
    permissions:
      id-token: write
      contents: read
    with:
      benchmark-artifact: benchmark-results-${{ inputs.alias }}-${{ inputs.kernels }}
      benchmark-metadata: ${{ needs.benchmark.outputs.benchmark-metadata }}
      runners-info: ${{ needs.benchmark.outputs.runners-info }}
      dependencies: ${{ needs.benchmark.outputs.dependencies }}
      schema-version: v3
      dry-run: false
