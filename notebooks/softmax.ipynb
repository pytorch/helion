{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"torch==2.9.*\" --index-url https://download.pytorch.org/whl/cu126\n",
    "%pip install helion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helion Softmax Kernel Examples\n",
    "==============================\n",
    "This example demonstrates multiple Helion kernel implementations of the softmax function,\n",
    "including a simple wrapper around PyTorch's softmax, and a numerically optimized two-pass version.\n",
    "The example also includes a check function to compare these kernels against PyTorch's\n",
    "built-in softmax for correctness.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import helion\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl\n",
    "\n",
    "\n",
    "# %%\n",
    "@helion.kernel(autotune_effort=\"quick\")\n",
    "def softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simple Helion kernel wrapping PyTorch's softmax function.\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape [n, m].\n",
    "    Returns:\n",
    "        torch.Tensor: Softmax output tensor of the same shape.\n",
    "    \"\"\"\n",
    "    n, _m = x.size()\n",
    "    out = torch.empty_like(x)\n",
    "    for tile_n in hl.tile(n):\n",
    "        out[tile_n, :] = torch.nn.functional.softmax(x[tile_n, :], dim=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Runs correctness checks comparing Helion softmax kernels against PyTorch's softmax.\n",
    "    Args:\n",
    "        m (int): Number of rows in input tensor.\n",
    "        n (int): Number of columns in input tensor.\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=\"cuda\", dtype=torch.float16)\n",
    "    run_example(softmax, lambda x: torch.nn.functional.softmax(x, dim=1), (x,))\n",
    "\n",
    "\n",
    "# %%\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the softmax kernel correctness check with example input size.\n",
    "    \"\"\"\n",
    "    check(4096, 2560)\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helion Softmax Kernel Examples\n",
    "==============================\n",
    "This example demonstrates multiple Helion kernel implementations of the softmax function,\n",
    "including a simple wrapper around PyTorch's softmax, and a numerically optimized two-pass version.\n",
    "The example also includes a check function to compare these kernels against PyTorch's\n",
    "built-in softmax for correctness.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import helion\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl\n",
    "\n",
    "\n",
    "# %%\n",
    "@helion.kernel(autotune_effort=\"quick\")\n",
    "def softmax_two_pass(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically optimized Helion kernel performing softmax in two passes.\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape [m, n].\n",
    "    Returns:\n",
    "        torch.Tensor: Softmax output tensor of the same shape.\n",
    "    \"\"\"\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)\n",
    "    block_size_m = hl.register_block_size(m)\n",
    "    block_size_n = hl.register_block_size(n)\n",
    "    for tile_m in hl.tile(m, block_size=block_size_m):\n",
    "        mi = hl.full([tile_m], float(\"-inf\"), dtype=torch.float32)\n",
    "        di = hl.zeros([tile_m], dtype=torch.float32)\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            local_amax = torch.amax(values, dim=1)\n",
    "            mi_next = torch.maximum(mi, local_amax)\n",
    "            di = di * torch.exp(mi - mi_next) + torch.exp(\n",
    "                values - mi_next[:, None]\n",
    "            ).sum(dim=1)\n",
    "            mi = mi_next\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]\n",
    "    return out\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Runs correctness checks comparing Helion softmax kernels against PyTorch's softmax.\n",
    "    Args:\n",
    "        m (int): Number of rows in input tensor.\n",
    "        n (int): Number of columns in input tensor.\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=\"cuda\", dtype=torch.float16)\n",
    "    run_example(softmax_two_pass, lambda x: torch.nn.functional.softmax(x, dim=1), (x,))\n",
    "\n",
    "\n",
    "# %%\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the softmax kernel correctness check with example input size.\n",
    "    \"\"\"\n",
    "    check(4096, 2560)\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
